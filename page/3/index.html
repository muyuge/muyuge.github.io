<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
<meta name="keywords" content="Gridea静态个人博客">
<meta name="description" content="温故而知新">
<meta name="theme-color" content="#000">
<title>muyuge</title>
<link rel="shortcut icon" href="/favicon.ico?v=1694485912664">
<link rel="stylesheet" href="/media/css/gemini.css">
<link rel="stylesheet" href="/media/fonts/font-awesome.css">
<link
  href="//fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Rosario:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext"
  rel="stylesheet" type="text/css">

<link href="/media/hljs/styles/monokai.css"
  rel="stylesheet">

<link rel="stylesheet" href="/styles/main.css">
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script src="/media/js/jquery.js"></script>
<script src="/media/hljs/highlight.js"></script>
<script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.0/velocity.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.0/velocity.ui.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
  integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
  integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
  integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>






</head>

<body>
  <div class="head-top-line"></div>
  <div class="header-box">
    
<div class="gemini">
  <header class="header  ">
    <div class="blog-header box-shadow-wrapper bg-color " id="header">
      <div class="nav-toggle" id="nav_toggle">
        <div class="toggle-box">
          <div class="line line-top"></div>
          <div class="line line-center"></div>
          <div class="line line-bottom"></div>
        </div>
      </div>
      <div class="site-meta">       
        <div class="site-title">
          
            <a href="/" class="brand">
              <span>muyuge</span>
            </a>  
          
        </div>
        
          <p class="subtitle">记录，沉淀</p>
        
      </div>
      <nav class="site-nav" id="site_nav">
        <ul id="nav_ul">
          
            
            
              
            
            <li class="nav-item nav-item-active">
              
              
                <a href="/" target="_self">
                  <i class="fa fa-globe"></i> 首页
                </a>
              
            </li>
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/archives" target="_self">
                  <i class="fa fa-globe"></i> 归档
                </a>
              
            </li>
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/tags" target="_self">
                  <i class="fa fa-globe"></i> 标签
                </a>
              
            </li>
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/post/about" target="_self">
                  <i class="fa fa-globe"></i> 关于
                </a>
              
            </li>
          
          
            
              <li class="nav-item ">
                <a href="/friends/" target="_self">
                  
                    <i class="fa fa-address-book"></i> 友链
                  
                </a>
              </li>
            
          
          
            <li id="fa_search" class="nav-item">
              <a href="javascript:void(0);">
                <i class="fa fa-search"></i> <span class="language" data-lan="search">搜索</span>
              </a>
            </li>
          
        </ul>
      </nav>
    </div>
  </header>
</div>

<script type="text/javascript"> 
 
  let showNav = true;

  let navToggle = document.querySelector('#nav_toggle'),
  siteNav = document.querySelector('#site_nav');
  
  function navClick() {
    let sideBar = document.querySelector('.sidebar');
    let navUl = document.querySelector('#nav_ul');
    navToggle.classList.toggle('nav-toggle-active');
    siteNav.classList.toggle('nav-menu-active');
    if (siteNav.classList.contains('nav-menu-active')) {
      siteNav.style = "height: " + (navUl.children.length * 42) +"px !important";
    } else {
      siteNav.style = "";
    }
  }

  navToggle.addEventListener('click',navClick);  
</script>
  </div>
  <div class="main-continer">
    
    <div
      class="section-layout gemini ">
      <div class="section-layout-wrapper">
        <div id="sidebarMeta" class="sidebar">
    
<div class="sidebar-wrapper box-shadow-wrapper bg-color">
  <div class="sidebar-item">
    <img class="site-author-image right-motion" src="/images/avatar.png"/>
    <p class="site-author-name">muyuge</p>
    
  </div>
  <div class="sidebar-item side-item-stat right-motion">
    <div class="sidebar-item-box">
      <a href="/archives/">
        
        <span class="site-item-stat-count">30</span>
        <span class="site-item-stat-name language" data-lan="article">文章</span>
      </a>
    </div>
    <div class="sidebar-item-box">
      <a href="/tags/">
        <span class="site-item-stat-count">1</span>
        <span class="site-item-stat-name language" data-lan="tag">标签</span>
      </a>
    </div>
  </div>
  
    
      <div class="sidebar-item">
        <span class="site-item-rss">
            <i class="fa fa-rss"></i>
            <a href="https://muyuge.github.io/atom.xml" target="_blank">RSS</a>
        </span>
      </div>
    
  
  



</div>
</div>
<script>
  let sidebarMeta = document.querySelector('#sidebarMeta');
  let scheme = 'gemini';
  let sidebarWrapper = document.querySelector('.sidebar-wrapper');
  if (sidebarMeta && (scheme === 'pisces' || scheme === 'gemini')) {
    document.addEventListener('scroll', function(e) {
      if (document.scrollingElement.scrollTop > parseInt(sidebarMeta.style.marginTop) + 10) {
        sidebarWrapper.classList.add('home-sidebar-fixed')
      } else {
        sidebarWrapper.classList.remove('home-sidebar-fixed')
      }
    });
  }
  </script>
        <div class="section-box gemini">
          <section class="section  posts-expand slide-down-in">
            
  <article class="post-list-box  post box-shadow-wrapper">
    <div class="article-wrapper bg-color">
      <section class="post-header">
  <h1 class="post-title">
    <a class="post-title-link" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-nei-cun-guan-li/"> Go实现原理-内存管理 </a>
  </h1>
  <div class="post-meta">
    
    <span class="meta-item pc-show">
      <i class="fa fa-calendar-o"></i>
      <span class="language" data-lan="publish">发布于</span>
      <span class="publish-time" data-t="2021-11-13 17:15:26">2021-11-13</span>
      <span class="post-meta-divider pc-show">|</span>
    </span>
    
    <span class="meta-item">
      <i class="fa fa-clock-o"></i>
      <span
        >28<span class="language" data-lan="minute"
          >分钟</span
        ></span
      >
    </span>
    <span class="meta-item">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-file-word-o"></i>
      <span
        >7683<span class="pc-show language" data-lan="words"
          >字数</span
        ></span
      >
    </span>
    
  </div>
</section>

      <div class="post-body">
        
        
          
            <!-- 没有手动摘要切开启了自动摘要,则根据配置筛除摘要内容 -->
            
            
              
              
              
            
              
              
              
            
              
              
              
            
            
              <p><p>这篇文章<strong>主要介绍Go内存分配和Go内存管理</strong>，会轻微涉及内存申请和释放，以及Go垃圾回收。</p>
<p>从非常宏观的角度讲，Go的内存管理就是下图这个样子，我们今天主要关注其中标红的部分。</p>
<figure data-type="image" tabindex="1"><img src="https://s2.loli.net/2023/08/23/gykn8JAcmMLYG92.jpg" alt="" loading="lazy"></figure>

                
              <p>
            
          
        
        
        <div class="post-button text-center">
          <a class="btn language" data-lan="read-more" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-nei-cun-guan-li/" rel="contents">
            阅读全文 »
          </a>
        </div>
        
      </div>
      
    </div>
  </article>

  <article class="post-list-box  post box-shadow-wrapper">
    <div class="article-wrapper bg-color">
      <section class="post-header">
  <h1 class="post-title">
    <a class="post-title-link" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-gc/"> Go实现原理-GC </a>
  </h1>
  <div class="post-meta">
    
    <span class="meta-item pc-show">
      <i class="fa fa-calendar-o"></i>
      <span class="language" data-lan="publish">发布于</span>
      <span class="publish-time" data-t="2021-10-23 10:54:24">2021-10-23</span>
      <span class="post-meta-divider pc-show">|</span>
    </span>
    
    <span class="meta-item">
      <i class="fa fa-clock-o"></i>
      <span
        >9<span class="language" data-lan="minute"
          >分钟</span
        ></span
      >
    </span>
    <span class="meta-item">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-file-word-o"></i>
      <span
        >2424<span class="pc-show language" data-lan="words"
          >字数</span
        ></span
      >
    </span>
    
  </div>
</section>

      <div class="post-body">
        
        
          
            <!-- 没有手动摘要切开启了自动摘要,则根据配置筛除摘要内容 -->
            
            
              
              
              
            
              
              
              
            
              
              
              
            
            
              <p><hr>
<h3 id="什么是gc">什么是GC？</h3>
<p><strong>垃圾回收（Garbage Collection，缩写为GC）</strong>，是一种自动内存管理机制。</p>

                
              <p>
            
          
        
        
        <div class="post-button text-center">
          <a class="btn language" data-lan="read-more" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-gc/" rel="contents">
            阅读全文 »
          </a>
        </div>
        
      </div>
      
    </div>
  </article>

  <article class="post-list-box  post box-shadow-wrapper">
    <div class="article-wrapper bg-color">
      <section class="post-header">
  <h1 class="post-title">
    <a class="post-title-link" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-channel/"> Go实现原理-Channel </a>
  </h1>
  <div class="post-meta">
    
    <span class="meta-item pc-show">
      <i class="fa fa-calendar-o"></i>
      <span class="language" data-lan="publish">发布于</span>
      <span class="publish-time" data-t="2021-10-09 21:22:14">2021-10-09</span>
      <span class="post-meta-divider pc-show">|</span>
    </span>
    
    <span class="meta-item">
      <i class="fa fa-clock-o"></i>
      <span
        >9<span class="language" data-lan="minute"
          >分钟</span
        ></span
      >
    </span>
    <span class="meta-item">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-file-word-o"></i>
      <span
        >2200<span class="pc-show language" data-lan="words"
          >字数</span
        ></span
      >
    </span>
    
  </div>
</section>

      <div class="post-body">
        
        
          
            <!-- 没有手动摘要切开启了自动摘要,则根据配置筛除摘要内容 -->
            
            
              
              
              
            
              
              
              
            
              
              
              
            
            
              <p><h2 id="channel的底层数据结构">channel的底层数据结构</h2>
<p>channel是golang中用来实现多个goroutine通信的管道，它的底层是一个叫做hchan的结构体。在go的runtime包下。</p>
<h3 id="数据结构">数据结构</h3>

                
              <p>
            
          
        
        
        <div class="post-button text-center">
          <a class="btn language" data-lan="read-more" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-channel/" rel="contents">
            阅读全文 »
          </a>
        </div>
        
      </div>
      
    </div>
  </article>

  <article class="post-list-box  post box-shadow-wrapper">
    <div class="article-wrapper bg-color">
      <section class="post-header">
  <h1 class="post-title">
    <a class="post-title-link" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-mutex/"> Go实现原理-Mutex </a>
  </h1>
  <div class="post-meta">
    
    <span class="meta-item pc-show">
      <i class="fa fa-calendar-o"></i>
      <span class="language" data-lan="publish">发布于</span>
      <span class="publish-time" data-t="2021-10-02 17:48:53">2021-10-02</span>
      <span class="post-meta-divider pc-show">|</span>
    </span>
    
    <span class="meta-item">
      <i class="fa fa-clock-o"></i>
      <span
        >5<span class="language" data-lan="minute"
          >分钟</span
        ></span
      >
    </span>
    <span class="meta-item">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-file-word-o"></i>
      <span
        >1122<span class="pc-show language" data-lan="words"
          >字数</span
        ></span
      >
    </span>
    
  </div>
</section>

      <div class="post-body">
        
        
          
            <!-- 没有手动摘要切开启了自动摘要,则根据配置筛除摘要内容 -->
            
            
              
              
              
            
              
              
              
            
              
              
              
            
            
              <p><p>在Go中，主要实现了两种锁：sync.Mutex(互斥锁) 以及 sync.RWMutex(读写锁)。</p>
<p>本篇主要介绍sync.Mutex的使用和实现原理。</p>
<h2 id="为什么需要锁">为什么需要锁</h2>

                
              <p>
            
          
        
        
        <div class="post-button text-center">
          <a class="btn language" data-lan="read-more" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-mutex/" rel="contents">
            阅读全文 »
          </a>
        </div>
        
      </div>
      
    </div>
  </article>

  <article class="post-list-box  post box-shadow-wrapper">
    <div class="article-wrapper bg-color">
      <section class="post-header">
  <h1 class="post-title">
    <a class="post-title-link" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-gmp-mo-xing/"> Go实现原理-GMP模型 </a>
  </h1>
  <div class="post-meta">
    
    <span class="meta-item pc-show">
      <i class="fa fa-calendar-o"></i>
      <span class="language" data-lan="publish">发布于</span>
      <span class="publish-time" data-t="2021-09-30 11:42:58">2021-09-30</span>
      <span class="post-meta-divider pc-show">|</span>
    </span>
    
    <span class="meta-item">
      <i class="fa fa-clock-o"></i>
      <span
        >5<span class="language" data-lan="minute"
          >分钟</span
        ></span
      >
    </span>
    <span class="meta-item">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-file-word-o"></i>
      <span
        >1239<span class="pc-show language" data-lan="words"
          >字数</span
        ></span
      >
    </span>
    
  </div>
</section>

      <div class="post-body">
        
        
          
            <!-- 没有手动摘要切开启了自动摘要,则根据配置筛除摘要内容 -->
            
            
              
              
              
            
              
              
              
            
              
              
              
            
            
              <p><ul>
<li>
<p>Go 调度器模型我们通常叫做<strong>G-P-M 模型</strong></p>

                
              <p>
            
          
        
        
        <div class="post-button text-center">
          <a class="btn language" data-lan="read-more" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-gmp-mo-xing/" rel="contents">
            阅读全文 »
          </a>
        </div>
        
      </div>
      
    </div>
  </article>

  <article class="post-list-box  post box-shadow-wrapper">
    <div class="article-wrapper bg-color">
      <section class="post-header">
  <h1 class="post-title">
    <a class="post-title-link" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-map/"> Go实现原理-Map </a>
  </h1>
  <div class="post-meta">
    
    <span class="meta-item pc-show">
      <i class="fa fa-calendar-o"></i>
      <span class="language" data-lan="publish">发布于</span>
      <span class="publish-time" data-t="2021-09-04 17:22:59">2021-09-04</span>
      <span class="post-meta-divider pc-show">|</span>
    </span>
    
    <span class="meta-item">
      <i class="fa fa-clock-o"></i>
      <span
        >8<span class="language" data-lan="minute"
          >分钟</span
        ></span
      >
    </span>
    <span class="meta-item">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-file-word-o"></i>
      <span
        >1939<span class="pc-show language" data-lan="words"
          >字数</span
        ></span
      >
    </span>
    
  </div>
</section>

      <div class="post-body">
        
        
          
            <!-- 没有手动摘要切开启了自动摘要,则根据配置筛除摘要内容 -->
            
            
              
              
              
            
              
              
              
            
              
              
              
            
            
              <p><h2 id="map的数据结构">map的数据结构</h2>
<p>首先先列出源码结构关键字段，实现在 src/runtime/map.go：</p>
<pre><code class="language-go">type hmap struct {

                
                  <!-- 自动摘要出来的内容有<code则补上结尾,防止格式错误 -->
                  </code></pre>
                
              <p>
            
          
        
        
        <div class="post-button text-center">
          <a class="btn language" data-lan="read-more" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-map/" rel="contents">
            阅读全文 »
          </a>
        </div>
        
      </div>
      
    </div>
  </article>

  <article class="post-list-box  post box-shadow-wrapper">
    <div class="article-wrapper bg-color">
      <section class="post-header">
  <h1 class="post-title">
    <a class="post-title-link" href="https://muyuge.github.io/post/cpu-huan-cun-yi-zhi-xing/"> CPU 缓存一致性 </a>
  </h1>
  <div class="post-meta">
    
    <span class="meta-item pc-show">
      <i class="fa fa-calendar-o"></i>
      <span class="language" data-lan="publish">发布于</span>
      <span class="publish-time" data-t="2021-03-12 19:46:09">2021-03-12</span>
      <span class="post-meta-divider pc-show">|</span>
    </span>
    
    <span class="meta-item">
      <i class="fa fa-clock-o"></i>
      <span
        >15<span class="language" data-lan="minute"
          >分钟</span
        ></span
      >
    </span>
    <span class="meta-item">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-file-word-o"></i>
      <span
        >4181<span class="pc-show language" data-lan="words"
          >字数</span
        ></span
      >
    </span>
    
  </div>
</section>

      <div class="post-body">
        
        
          
            <!-- 没有手动摘要切开启了自动摘要,则根据配置筛除摘要内容 -->
            
            
              
              
              
            
              
              
              
            
              
              
              
            
            
              <p><hr>
<h2 id="前言">前言</h2>
<figure data-type="image" tabindex="1"><img src="https://gitee.com/fxiang930/muyuge/raw/master/20220312193703.png" alt="" loading="lazy"></figure>

                
              <p>
            
          
        
        
        <div class="post-button text-center">
          <a class="btn language" data-lan="read-more" href="https://muyuge.github.io/post/cpu-huan-cun-yi-zhi-xing/" rel="contents">
            阅读全文 »
          </a>
        </div>
        
      </div>
      
    </div>
  </article>

  <article class="post-list-box  post box-shadow-wrapper">
    <div class="article-wrapper bg-color">
      <section class="post-header">
  <h1 class="post-title">
    <a class="post-title-link" href="https://muyuge.github.io/post/go-gc/"> Go GC </a>
  </h1>
  <div class="post-meta">
    
    <span class="meta-item pc-show">
      <i class="fa fa-calendar-o"></i>
      <span class="language" data-lan="publish">发布于</span>
      <span class="publish-time" data-t="2021-03-10 14:52:23">2021-03-10</span>
      <span class="post-meta-divider pc-show">|</span>
    </span>
    
    <span class="meta-item">
      <i class="fa fa-clock-o"></i>
      <span
        >12<span class="language" data-lan="minute"
          >分钟</span
        ></span
      >
    </span>
    <span class="meta-item">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-file-word-o"></i>
      <span
        >2841<span class="pc-show language" data-lan="words"
          >字数</span
        ></span
      >
    </span>
    
  </div>
</section>

      <div class="post-body">
        
        
          
            <!-- 没有手动摘要切开启了自动摘要,则根据配置筛除摘要内容 -->
            
            
              
              
              
            
              
              
              
            
              
              
              
            
            
              <p><h3 id="一-go-gc-要点">一. Go GC 要点</h3>
<p>先来回顾一下GC的几个重要的阶段:</p>
<h4 id="mark-prepare-stw">Mark Prepare - STW</h4>

                
              <p>
            
          
        
        
        <div class="post-button text-center">
          <a class="btn language" data-lan="read-more" href="https://muyuge.github.io/post/go-gc/" rel="contents">
            阅读全文 »
          </a>
        </div>
        
      </div>
      
    </div>
  </article>

  <article class="post-list-box  post box-shadow-wrapper">
    <div class="article-wrapper bg-color">
      <section class="post-header">
  <h1 class="post-title">
    <a class="post-title-link" href="https://muyuge.github.io/post/redis-yu-mysq-yi-zhi-xing/"> Redis与MySQ一致性 </a>
  </h1>
  <div class="post-meta">
    
    <span class="meta-item pc-show">
      <i class="fa fa-calendar-o"></i>
      <span class="language" data-lan="publish">发布于</span>
      <span class="publish-time" data-t="2021-02-20 12:26:03">2021-02-20</span>
      <span class="post-meta-divider pc-show">|</span>
    </span>
    
    <span class="meta-item">
      <i class="fa fa-clock-o"></i>
      <span
        >7<span class="language" data-lan="minute"
          >分钟</span
        ></span
      >
    </span>
    <span class="meta-item">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-file-word-o"></i>
      <span
        >1821<span class="pc-show language" data-lan="words"
          >字数</span
        ></span
      >
    </span>
    
  </div>
</section>

      <div class="post-body">
        
        
          
            <!-- 没有手动摘要切开启了自动摘要,则根据配置筛除摘要内容 -->
            
            
              
              
              
            
              
              
              
            
              
              
              
            
            
              <p><h2 id="谈谈一致性">谈谈一致性</h2>
<figure data-type="image" tabindex="1"><img src="https://s2.loli.net/2023/08/23/EHw3d4NjKetZmak.webp" alt="" loading="lazy"></figure>
<p>一致性就是数据保持一致，在分布式系统中，可以理解为多个节点中数据的值是一致的。</p>

                
              <p>
            
          
        
        
        <div class="post-button text-center">
          <a class="btn language" data-lan="read-more" href="https://muyuge.github.io/post/redis-yu-mysq-yi-zhi-xing/" rel="contents">
            阅读全文 »
          </a>
        </div>
        
      </div>
      
    </div>
  </article>

  <article class="post-list-box  post box-shadow-wrapper">
    <div class="article-wrapper bg-color">
      <section class="post-header">
  <h1 class="post-title">
    <a class="post-title-link" href="https://muyuge.github.io/post/mysql-zhong-jie-1/"> MySQL 总结 </a>
  </h1>
  <div class="post-meta">
    
    <span class="meta-item pc-show">
      <i class="fa fa-calendar-o"></i>
      <span class="language" data-lan="publish">发布于</span>
      <span class="publish-time" data-t="2021-01-19 10:38:16">2021-01-19</span>
      <span class="post-meta-divider pc-show">|</span>
    </span>
    
    <span class="meta-item">
      <i class="fa fa-folder-o"></i>
      <span class="pc-show language" data-lan="category-in">标签:</span>
       
      <a href="https://muyuge.github.io/tag/QIwoCBgM_/">
        <span>mysql</span>
      </a>
       
    </span>
    <span class="post-meta-divider">|</span>
    
    <span class="meta-item">
      <i class="fa fa-clock-o"></i>
      <span
        >22<span class="language" data-lan="minute"
          >分钟</span
        ></span
      >
    </span>
    <span class="meta-item">
      <span class="post-meta-divider">|</span>
      <i class="fa fa-file-word-o"></i>
      <span
        >6044<span class="pc-show language" data-lan="words"
          >字数</span
        ></span
      >
    </span>
    
  </div>
</section>

      <div class="post-body">
        
        
          
            <!-- 没有手动摘要切开启了自动摘要,则根据配置筛除摘要内容 -->
            
            
              
              
              
            
              
              
              
            
              
              
              
            
            
              <p><h1 id="结构一览">结构一览</h1>
<hr>
<p>参考资料 <a href="http://c.biancheng.net/view/7939.html">MySQL体系结构详解</a><br>

                
              <p>
            
          
        
        
        <div class="post-button text-center">
          <a class="btn language" data-lan="read-more" href="https://muyuge.github.io/post/mysql-zhong-jie-1/" rel="contents">
            阅读全文 »
          </a>
        </div>
        
      </div>
      
    </div>
  </article>
  
            
            
<div class="page bg-color">
  <ul class="pagination-ul">
    
      <li class="pagination-dir">
        <a href="https://muyuge.github.io/page/2/">
          <i class="fa fa-angle-left"></i>
        </a>
      </li>
    
    
      
        <li class="pagination-li ">
            <a href="/page/../">
              1
            </a>
        </li>
      
        <li class="pagination-li ">
            <a href="/page/2">
              2
            </a>
        </li>
      
        <li class="pagination-li pagination-active">
            <a href="/page/3">
              3
            </a>
        </li>
      
    
    
  </ul>
</div>
          </section>
        </div>
      </div>
    </div>
    <div class="footer-box">
  <footer class="footer">
    <center id="runTimeBox">
      已运行:<span id="run_time"></span>
    </center>
    <span id="busuanzi_container_site_pv">浏览数:<span id="busuanzi_value_site_pv"></span> 次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">访客数:<span id="busuanzi_value_site_uv"></span> 人</span>

    <script>
      BirthDay = new Date('');
      if (BirthDay.getTime()) {
        function runTime() {
          str = "";
          today = new Date();
          timeold = today.getTime() - BirthDay.getTime();
          msPerDay = 24 * 60 * 60 * 1000;
          e_daysold = timeold / msPerDay;
          daysold = Math.floor(e_daysold);
          str += daysold + "天";
          return str;
        }
        setInterval(function () {
          $("#run_time").html(runTime());
        }, 1000);
      } else {
        document.querySelector('.footer').removeChild(document.querySelector('#runTimeBox'));
      }
    </script>
    <div class="poweredby">
      
    </div>
  </footer>
  
    
        <div class="gemini back-to-top" id="back_to_top">
          <i class="fa fa-arrow-up"></i>
          
            <span class="scrollpercent"> <span id="back_to_top_text">0</span>% </span>
            
        </div>
        
                  
                        
</div>
<script>
  let sideBarOpen = "sidebar-open";
  let body = document.body;
  let back2Top = document.querySelector("#back_to_top"),
    back2TopText = document.querySelector("#back_to_top_text"),
    drawerBox = document.querySelector("#drawer_box"),
    rightSideBar = document.querySelector(".sidebar"),
    viewport = document.querySelector("body");

  function scrollAnimation(currentY, targetY) {
    let needScrollTop = targetY - currentY;
    let _currentY = currentY;
    setTimeout(() => {
      const dist = Math.ceil(needScrollTop / 10);
      _currentY += dist;
      window.scrollTo(_currentY, currentY);
      if (needScrollTop > 10 || needScrollTop < -10) {
        scrollAnimation(_currentY, targetY);
      } else {
        window.scrollTo(_currentY, targetY);
      }
    }, 1);
  }

  back2Top.addEventListener("click", function (e) {
    scrollAnimation(document.scrollingElement.scrollTop, 0);
    e.stopPropagation();
    return false;
  });

  window.addEventListener("scroll", function (e) {
    let percent =
      (document.scrollingElement.scrollTop /
        (document.scrollingElement.scrollHeight -
          document.scrollingElement.clientHeight)) *
      100;
    if (percent > 1 && !back2Top.classList.contains("back-top-active")) {
      back2Top.classList.add("back-top-active");
    }
    if (percent == 0) {
      back2Top.classList.remove("back-top-active");
    }
    if (back2TopText) {
      back2TopText.textContent = Math.floor(percent);
    }
  });

  let hasCacu = false;
  window.addEventListener("resize", function (e) {
    calcuHeight();
  });

  function calcuHeight() {
    // 动态调整站点概览高度
    if (
      (!hasCacu && back2Top.classList.contains("pisces")) ||
      back2Top.classList.contains("gemini")
    ) {
      let sideBar = document.querySelector(".sidebar");
      let navUl = document.querySelector("#site_nav");
      sideBar.style =
        "margin-top:" + (navUl.offsetHeight + navUl.offsetTop + 15) + "px;";
      hasCacu = true;
    }
  }
  calcuHeight();

  let open = false,
    MOTION_TIME = 300,
    RIGHT_MOVE_DIS = "320px";

  if (drawerBox) {
    let rightMotions = document.querySelectorAll(".right-motion");
    let right = drawerBox.classList.contains("right");

    let transitionDir = right
      ? "transition.slideRightIn"
      : "transition.slideLeftIn";

    let openProp, closeProp;
    if (right) {
      openProp = {
        paddingRight: RIGHT_MOVE_DIS,
      };
      closeProp = {
        paddingRight: "0px",
      };
    } else {
      openProp = {
        paddingLeft: RIGHT_MOVE_DIS,
      };
      closeProp = {
        paddingLeft: "0px",
      };
    }

    drawerBox.onclick = function () {
      open = !open;
      jQuery.Velocity(rightSideBar, "stop");
      jQuery.Velocity(viewport, "stop");
      jQuery.Velocity(rightMotions, "stop");
      if (open) {
        jQuery.Velocity(
          rightSideBar,
          {
            width: RIGHT_MOVE_DIS,
          },
          {
            duration: MOTION_TIME,
            begin: function () {
              jQuery.Velocity(rightMotions, transitionDir, {});
            },
          }
        );
        jQuery.Velocity(viewport, openProp, {
          duration: MOTION_TIME,
        });
      } else {
        jQuery.Velocity(
          rightSideBar,
          {
            width: "0px",
          },
          {
            duration: MOTION_TIME,
            begin: function () {
              jQuery.Velocity(rightMotions, {
                opacity: 0,
              });
            },
          }
        );
        jQuery.Velocity(viewport, closeProp, {
          duration: MOTION_TIME,
        });
      }
      for (let i = 0; i < drawerBox.children.length; i++) {
        drawerBox.children[i].classList.toggle("muse-line");
      }
      drawerBox.classList.toggle(sideBarOpen);
    };
  }

  // 链接跳转
  let newWindow = "true";
  if (newWindow === "true") {
    let links = document.querySelectorAll(".post-body a");
    links.forEach((item) => {
      if (!item.classList.contains("btn")) {
        item.setAttribute("target", "_blank");
      }
    });
  }

  let faSearch = document.querySelector("#fa_search");
  faSearch &&
    faSearch.addEventListener("click", function () {
      document.querySelector("#search_mask").style = "";
    });

  // 代码高亮
  hljs.initHighlightingOnLoad();

  // 离开当前页title变化
  var leaveTitle = "";
  var normal_title = document.title;
  if (leaveTitle) {
    document.addEventListener("visibilitychange", function () {
      if (document.visibilityState == "hidden") {
        normal_title = document.title;
        document.title = leaveTitle;
      } else {
        document.title = normal_title;
      }
    });
  }
</script>

<link rel="stylesheet" href="/media/css/jquery.fancybox.css" />
<script src="/media/js/jquery.fancybox.js"></script>

<script>
  let images = document.querySelectorAll(".section img");
  images.forEach((image) => {
    var parent = image.parentElement;
    var next = image.nextElementSibling;
    parent.removeChild(image);
    var aelem = document.createElement("a");
    aelem.href = image.src;
    aelem.dataset["fancybox"] = "images";
    aelem.dataset["rel"] = "fancybox-button";
    aelem.classList.add("fancybox");
    aelem.appendChild(image);
    parent.insertBefore(aelem, next);
  });
</script>
  </div>
</body>

  <div class="search-mask" id="search_mask" style="display: none;">
  <div class="search-box">
    <div class="search-title">
      <i class="fa fa-search"></i>
      <div class="input-box">
        <input id="search" type="text" class="language" data-lan="search" placeholder="搜索">
      </div>
      <i id="close" class="fa fa-times-circle"></i>
    </div>
    <div class="stat-box">
      <span id="stat_count">0</span><span class="language" data-lan="stat">条相关条目，使用了</span><span id="stat_times">0</span><span class="language" data-lan="stat-time">毫秒</span>
      <hr>
    </div>
    <div class="result" id="result">
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/ddia-notes-chapter-11-stream-processing/"" data-c="
          &lt;p&gt;Chapter 11 provides an overview of stream processing, its use cases, key characteristics, and popular stream processing frameworks. It also discusses challenges related to event time, state management, and exactly-once processing semantics.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stream Processing&lt;/strong&gt;: Stream processing is a data processing paradigm focused on handling data in real-time as it flows through a system. It&#39;s used for a wide range of applications, including real-time analytics, monitoring, fraud detection, and more.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Characteristics&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stream processing systems ingest data continuously, often from various sources.&lt;/li&gt;
&lt;li&gt;They process data incrementally as it arrives, enabling low-latency processing.&lt;/li&gt;
&lt;li&gt;Stream processors typically offer support for event time and processing time semantics.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Use Cases for Stream Processing&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Real-time analytics: Analyzing data as it arrives to gain insights and make decisions.&lt;/li&gt;
&lt;li&gt;Monitoring and alerting: Detecting anomalies or critical events in real time.&lt;/li&gt;
&lt;li&gt;Fraud detection: Identifying fraudulent activities as they occur.&lt;/li&gt;
&lt;li&gt;Recommendations: Generating personalized recommendations based on user behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Streams&lt;/strong&gt;: Data streams represent a continuous flow of data records. They can be unbounded (infinite) or bounded (finite). Examples include logs, sensor data, social media updates, and more.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Event Time vs. Processing Time&lt;/strong&gt;: Event time represents the time when an event occurred in the real world, while processing time represents the time when the system processes the event. Stream processing systems often need to reconcile these two notions of time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stateful Processing&lt;/strong&gt;: Stream processing systems can maintain state, such as aggregations, for a defined time window to perform operations like counting, summing, or averaging.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stream Processing Frameworks&lt;/strong&gt;: Popular stream processing frameworks include Apache Kafka Streams, Apache Flink, and Apache Storm.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apache Kafka Streams&lt;/strong&gt;: Kafka Streams is an embedded stream processing library for building applications that process data from Kafka topics. It provides exactly-once processing semantics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apache Flink&lt;/strong&gt;: Flink is a stream processing framework that also supports batch processing. It offers strong event-time support, low-latency processing, and stateful operations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apache Storm&lt;/strong&gt;: Storm is an older stream processing framework known for its low-latency capabilities. It processes data in small units called &amp;quot;tuples&amp;quot; and supports various processing topologies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Lambda Architecture&lt;/strong&gt;: The Lambda Architecture is a design pattern that combines batch and stream processing to provide both real-time and batch processing capabilities. It aims to provide the best of both worlds while handling late-arriving data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Complex Event Processing (CEP)&lt;/strong&gt;: CEP is a specialized field of stream processing that focuses on detecting complex patterns and sequences of events in real-time data streams.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Time Windowing&lt;/strong&gt;: Time-based windows are often used in stream processing to group events within a specified time interval for aggregation or analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Watermarks&lt;/strong&gt;: Watermarks are used to track progress in processing event-time data. They help systems deal with out-of-order events and ensure correctness in windowed operations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exactly-Once Processing&lt;/strong&gt;: Ensuring exactly-once processing semantics is a complex challenge in stream processing and often involves coordination and state management.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Takeaway&lt;/strong&gt;: Stream processing is a powerful paradigm for handling real-time data, enabling low-latency insights and actions. It&#39;s essential for applications where timely data processing is critical.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
">DDIA notes - Chapter 11: Stream Processing </a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/ddia-notes-chapter-10-batch-processing/"" data-c="
          &lt;p&gt;Chapter 10 provides insights into batch processing, its use cases, and the challenges associated with processing large volumes of data in a batch-oriented manner. It also introduces key batch processing frameworks and their characteristics.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Batch Processing&lt;/strong&gt;: Batch processing is a common data processing paradigm used to handle large volumes of data efficiently. It involves processing data in chunks or batches rather than in real-time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Use Cases for Batch Processing&lt;/strong&gt;: Batch processing is well-suited for various use cases, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data ETL (Extract, Transform, Load) for data warehousing.&lt;/li&gt;
&lt;li&gt;Generating reports and analytics.&lt;/li&gt;
&lt;li&gt;Large-scale data analysis and transformations.&lt;/li&gt;
&lt;li&gt;Data quality checks and validation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Locality&lt;/strong&gt;: Batch processing often leverages data locality, where data is processed where it resides to minimize data transfer costs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Batch Data Processing Frameworks&lt;/strong&gt;: Popular batch processing frameworks include Apache Hadoop (with MapReduce), Apache Spark, and Apache Flink. These frameworks provide abstractions for parallel processing and fault tolerance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MapReduce&lt;/strong&gt;: MapReduce is a programming model and processing framework designed for batch processing. It involves two main phases: a Map phase for data transformation and a Reduce phase for aggregation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Shuffling&lt;/strong&gt;: Shuffling is the process of redistributing data between nodes in a cluster during a batch processing job. It can be a performance bottleneck.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apache Hadoop&lt;/strong&gt;: Hadoop is one of the earliest batch processing frameworks, known for its use of Hadoop Distributed File System (HDFS) and MapReduce. It is suitable for batch processing of large-scale data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apache Spark&lt;/strong&gt;: Spark is a more recent batch processing framework that offers in-memory processing, which can significantly speed up batch jobs compared to MapReduce.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apache Flink&lt;/strong&gt;: Flink is another batch processing framework designed for low-latency, high-throughput, and exactly-once processing semantics. It&#39;s suitable for both batch and stream processing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Batch Processing Challenges&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scalability: Ensuring that batch processing jobs can scale horizontally to handle large datasets.&lt;/li&gt;
&lt;li&gt;Fault Tolerance: Handling failures and ensuring that jobs can resume from where they left off.&lt;/li&gt;
&lt;li&gt;Data Skew: Dealing with uneven data distribution, which can lead to performance bottlenecks.&lt;/li&gt;
&lt;li&gt;Resource Management: Efficiently managing CPU, memory, and disk resources across a cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Pipeline Orchestration&lt;/strong&gt;: Tools like Apache Airflow are used for orchestrating complex batch processing workflows, including scheduling, monitoring, and dependency management.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Serialization&lt;/strong&gt;: Batch processing often requires serialization and deserialization of data between processing stages, which can introduce overhead.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State Management&lt;/strong&gt;: Batch processing jobs may need to maintain state, such as aggregations or counters, between processing steps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Takeaway&lt;/strong&gt;: Batch processing is a valuable paradigm for handling large-scale data processing tasks efficiently. Modern batch processing frameworks like Spark and Flink offer enhanced performance and ease of use compared to traditional MapReduce.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
">DDIA notes - Chapter 10: Batch Processing</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/ddia-notes-chapter-9-consistency-and-consensus/"" data-c="
          &lt;p&gt;Chapter 9 explores the concepts of consistency and consensus in distributed systems, highlighting the challenges and trade-offs involved in achieving strong consistency across distributed nodes. It also introduces some key consensus algorithms used in practice.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consistency in Distributed Systems&lt;/strong&gt;: Achieving consistency in distributed systems involves ensuring that all nodes in the system agree on the order of operations or values. It&#39;s a fundamental challenge in maintaining data integrity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Linearizability&lt;/strong&gt;: Linearizability is a strong form of consistency that ensures that writes appear to be instantaneous, and all nodes in the system see the same order of operations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sequential Consistency&lt;/strong&gt;: Sequential consistency is a weaker form of consistency that ensures that all operations appear to be executed in the same order, but not necessarily instantaneously.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Causal Consistency&lt;/strong&gt;: Causal consistency ensures that operations are ordered based on their causality, meaning that if one operation causally precedes another, all nodes will observe the same causal order.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CAP Theorem&lt;/strong&gt;: The CAP theorem states that in the presence of a network partition, a distributed system must choose between providing Consistency (C) or Availability (A). Partition tolerance (P) is always assumed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consensus&lt;/strong&gt;: Consensus is the process by which a distributed system agrees on a single value or sequence of values. It is a fundamental building block for achieving strong consistency in distributed systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Two-Phase Commit (2PC)&lt;/strong&gt;: 2PC is a classic consensus protocol used to coordinate distributed transactions. It ensures that all participants agree on whether to commit or abort a transaction.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Two-Phase Commit Limitations&lt;/strong&gt;: 2PC can suffer from blocking and is not suitable for systems with high availability requirements or in the presence of network partitions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Three-Phase Commit (3PC)&lt;/strong&gt;: 3PC is a variation of 2PC that mitigates some of its limitations, but it still does not guarantee progress in the presence of network partitions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Paxos&lt;/strong&gt;: Paxos is a consensus algorithm that can tolerate network partitions and has been widely used in distributed systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Raft&lt;/strong&gt;: Raft is another consensus algorithm designed for ease of understanding and implementation. It provides strong consistency and can tolerate network partitions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ZooKeeper&lt;/strong&gt;: Apache ZooKeeper is a distributed coordination service that uses a variation of the Zab protocol, which is similar to Paxos, to achieve consensus.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Chubby&lt;/strong&gt;: Chubby is a distributed lock service developed by Google that uses Paxos for achieving consensus.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Takeaway&lt;/strong&gt;: Achieving consensus in distributed systems is crucial for maintaining strong consistency and data integrity. Various consensus algorithms like Paxos and Raft offer different trade-offs in terms of ease of use and performance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
">DDIA notes - Chapter 9: Consistency and Consensus</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/ddia-notes-chapter-8-the-trouble-with-distributed-systems/"" data-c="
          &lt;p&gt;Chapter 8 explores the complexities and challenges of distributed systems, emphasizing the need to design for failure, understand network partitions, and make informed trade-offs between consistency and availability. It also highlights the importance of testing and monitoring in ensuring the reliability of distributed systems.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distributed Systems Complexity&lt;/strong&gt;: Distributed systems are complex due to the challenges of dealing with network partitions, failures, and the need for coordination.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fallacies of Distributed Computing&lt;/strong&gt;: The Fallacies of Distributed Computing are a set of common misconceptions about distributed systems, including assumptions about network reliability, latency, and topology.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Partitions&lt;/strong&gt;: Network partitions occur when a network connection between nodes in a distributed system is lost or severely delayed. Dealing with partitions is one of the most challenging aspects of distributed systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CAP Theorem Revisited&lt;/strong&gt;: The CAP theorem states that in the presence of a network partition (P), distributed systems must choose between providing Consistency (C) or Availability (A). There&#39;s no one-size-fits-all answer; the choice depends on the specific requirements of the system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt;: Network latency can vary widely and is a critical factor in the performance of distributed systems. Reducing latency often requires trade-offs, such as increased complexity or the need for distributed caching.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consensus and Coordination&lt;/strong&gt;: Distributed systems often rely on consensus algorithms like Paxos or Raft to coordinate and agree on values in the presence of failures.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Impossibility of Consensus&lt;/strong&gt;: The FLP (Fischer-Lynch-Paterson) theorem shows that in an asynchronous network, it is impossible to achieve consensus if a single node can fail in an arbitrary way. This theorem highlights the challenges of achieving fault tolerance in distributed systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Failures Are the Norm&lt;/strong&gt;: In distributed systems, it&#39;s essential to design for failure and assume that failures will occur. This approach, known as &amp;quot;chaos engineering,&amp;quot; involves testing systems for resilience under various failure scenarios.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: Distributed systems often rely on distributed clocks or timestamps to order events. However, synchronizing clocks across nodes is challenging, and clock skew can lead to problems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monotonic Reads and Writes&lt;/strong&gt;: Ensuring monotonic reads and writes in a distributed system can be difficult, as operations may be executed out of order.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consistency Models&lt;/strong&gt;: Different consistency models, such as strong consistency, eventual consistency, and causal consistency, offer different trade-offs between performance and data consistency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ACID Transactions in Distributed Systems&lt;/strong&gt;: Implementing ACID transactions across distributed systems is complex due to the need for coordination and the risk of performance bottlenecks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Complexity and Testing&lt;/strong&gt;: Distributed systems often exhibit emergent behaviors that are difficult to predict or test in isolation. Comprehensive testing and monitoring are crucial.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Operational Complexity&lt;/strong&gt;: Operating and maintaining distributed systems can be challenging, as they often involve multiple nodes, configurations, and dependencies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Takeaway&lt;/strong&gt;: Building and operating distributed systems is inherently complex, and developers must be aware of the challenges and trade-offs involved in achieving reliability, performance, and consistency.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
">DDIA notes - Chapter 8: The Trouble with Distributed Systems</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/ddia-notes-chapter-7-transactions/"" data-c="
          &lt;p&gt;Chapter 7 dives deep into the topic of transactions, exploring different isolation levels, concurrency control mechanisms, and distributed transaction strategies. It highlights the trade-offs between strong consistency and performance in distributed systems.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transactions&lt;/strong&gt;: Transactions are a fundamental concept in databases and distributed systems. They provide a way to ensure data consistency and integrity, even in the presence of failures.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ACID Properties&lt;/strong&gt;: ACID stands for Atomicity, Consistency, Isolation, and Durability. These properties define the guarantees provided by a transaction:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Atomicity&lt;/strong&gt;: A transaction is atomic; it&#39;s either fully completed or fully rolled back.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: A transaction brings the database from one consistent state to another.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Isolation&lt;/strong&gt;: Transactions are isolated from each other, and their effects appear as if they occurred serially.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Durability&lt;/strong&gt;: Once a transaction is committed, its effects are permanent.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Weak Isolation Levels&lt;/strong&gt;: Many databases default to weaker isolation levels than full serializability to improve performance. These levels include Read Uncommitted, Read Committed, and Repeatable Read.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Serializable Transactions&lt;/strong&gt;: Serializable transactions provide the strongest isolation guarantees but may result in performance trade-offs, such as reduced concurrency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Two-Phase Locking&lt;/strong&gt;: Two-phase locking is a common concurrency control mechanism used in databases to ensure serializability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Serializable Snapshot Isolation&lt;/strong&gt;: Serializable snapshot isolation is an alternative to two-phase locking, providing serializability with better concurrency in some cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Linearizability&lt;/strong&gt;: Linearizability is a stronger guarantee than serializability, ensuring that operations appear to occur instantly at a single point in time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Causal Consistency&lt;/strong&gt;: Causal consistency ensures that operations appear in a causal order.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Implementing Serializable Transactions&lt;/strong&gt;: Implementing serializable transactions in distributed databases is challenging due to network delays and the need for coordination.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Global Serializability&lt;/strong&gt;: Global serializability ensures that transactions appear to be executed in a single, globally agreed-upon order.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distributed Transactions&lt;/strong&gt;: Distributed transactions involve multiple databases or services. Implementing distributed transactions often requires two-phase commit (2PC) or distributed commit protocols.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sagas&lt;/strong&gt;: Sagas are a pattern for orchestrating distributed transactions across multiple services, often using a sequence of local transactions and compensating actions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Concurrency Control without Two-Phase Commit&lt;/strong&gt;: Some systems use multi-version concurrency control (MVCC) to achieve strong isolation without the need for distributed transactions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distributed Consensus&lt;/strong&gt;: Distributed consensus protocols like Paxos and Raft provide a way for distributed systems to agree on a single value.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distributed Locks&lt;/strong&gt;: Distributed locks can be used to coordinate access to shared resources in a distributed system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Takeaway&lt;/strong&gt;: Transactions are essential for maintaining data consistency and integrity in data-intensive applications. Choosing the right isolation level and concurrency control mechanism depends on the application&#39;s requirements and trade-offs between consistency and performance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
">DDIA notes - Chapter 7: Transactions</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/ddia-notes-chapter-6-partitioning/"" data-c="
          &lt;p&gt;Chapter 6 provides a detailed exploration of partitioning strategies and considerations in data-intensive applications. It highlights the importance of choosing the right partitioning method and key to achieve a balanced and efficient data distribution.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Partitioning&lt;/strong&gt;: Partitioning involves dividing a dataset into smaller, more manageable pieces called partitions or shards. It is a fundamental technique for distributing data and workload in data-intensive applications.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Motivation for Partitioning&lt;/strong&gt;: Partitioning is used to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Distribute data across multiple servers or nodes for scalability.&lt;/li&gt;
&lt;li&gt;Improve data locality for performance.&lt;/li&gt;
&lt;li&gt;Isolate different parts of the dataset for fault tolerance and availability.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Partitioning Methods&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hash Partitioning&lt;/strong&gt;: Data is assigned to partitions based on a hash of the partition key.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Range Partitioning&lt;/strong&gt;: Data is divided into partitions based on a predefined range of values.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;List Partitioning&lt;/strong&gt;: Data is assigned to partitions based on a list of values associated with each partition.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Round Robin Partitioning&lt;/strong&gt;: Data is distributed evenly among partitions in a circular fashion.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Composite Partitioning&lt;/strong&gt;: A combination of multiple partitioning methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Partitioning and Data Distribution&lt;/strong&gt;: Effective partitioning requires distributing data evenly across partitions to avoid hotspots and ensure balanced load.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Partitioning and Query Performance&lt;/strong&gt;: The choice of partitioning key can significantly impact query performance. Queries that involve multiple partitions may require distributed queries and introduce latency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Joins and Denormalization&lt;/strong&gt;: In a partitioned system, joining data from different partitions can be challenging. Denormalization can help by duplicating data in multiple partitions, reducing the need for joins.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rebalancing Partitions&lt;/strong&gt;: Over time, the distribution of data may become uneven due to changes in data volume or access patterns. Systems need mechanisms for rebalancing partitions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Leader-Based vs. Key-Based Partitioning&lt;/strong&gt;: In leader-based partitioning, one node (leader) handles all requests for a particular partition. In key-based partitioning, each partition can have its leader.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Zookeeper&lt;/strong&gt;: Apache ZooKeeper is a coordination service often used to keep track of which node is the leader for each partition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Partitioning and Secondary Indexes&lt;/strong&gt;: Secondary indexes can be challenging in partitioned systems. They may require additional indexes or specialized techniques for distributed querying.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Partitioning in Practice&lt;/strong&gt;: Real-world systems often use a combination of partitioning methods and strategies to meet their specific requirements.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Takeaway&lt;/strong&gt;: Partitioning is a crucial technique for distributing data in data-intensive applications to achieve scalability, performance, and fault tolerance. However, it introduces challenges related to data distribution, query performance, and rebalancing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
">DDIA notes - Chapter 6: Partitioning</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/ddia-notes-chapter-5-replication/"" data-c="
          &lt;p&gt;Chapter 5 provides a comprehensive overview of replication strategies and their impact on the reliability and performance of data-intensive applications. It covers various aspects of replication, including its use for redundancy and scaling, consistency models, and conflict resolution techniques.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Replication&lt;/strong&gt;: Replication involves maintaining multiple copies of data in different locations. It&#39;s a fundamental technique for improving the availability, fault tolerance, and performance of data-intensive systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Replication for Redundancy&lt;/strong&gt;: One primary reason for replication is to ensure that data remains available even if some nodes in a system fail. Redundancy is achieved by keeping multiple copies of the same data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Replication for Scaling&lt;/strong&gt;: Replication can also be used to distribute read traffic across multiple nodes, improving read performance and scalability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Synchronous vs. Asynchronous Replication&lt;/strong&gt;: In synchronous replication, a write is considered complete only when all replicas have acknowledged it. In asynchronous replication, writes are acknowledged as soon as they are received, without waiting for all replicas to be updated.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Single-Leader vs. Multi-Leader Replication&lt;/strong&gt;: In single-leader replication, one node is responsible for handling all writes, while in multi-leader replication, multiple nodes can accept writes independently.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Leader-Based Replication&lt;/strong&gt;: In leader-based replication, one node (the leader) handles all writes and then replicates the changes to followers. This approach is often used in databases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Quorums&lt;/strong&gt;: Quorum-based systems require a minimum number of nodes to agree on a write or read operation to ensure consistency and fault tolerance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consistency and Availability Trade-offs&lt;/strong&gt;: The CAP theorem applies to replicated systems, and trade-offs between consistency and availability must be made during network partitions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Conflict Resolution&lt;/strong&gt;: When multiple replicas receive conflicting updates, conflict resolution strategies must be employed. Timestamps or vector clocks are often used to determine the order of updates.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Replication Lag&lt;/strong&gt;: Replication lag refers to the delay between a write being made to the leader and it being replicated to followers. High replication lag can affect read consistency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Read Your Writes&lt;/strong&gt;: Systems can be configured to ensure that after a write is acknowledged, a subsequent read by the same client will return the updated data. This is known as &amp;quot;read your writes&amp;quot; consistency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Center Locality&lt;/strong&gt;: Replicas are often placed in different data centers for fault tolerance and low-latency access. However, cross-data center communication can introduce latency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consistency Models&lt;/strong&gt;: Systems may offer different consistency models, such as strong consistency, eventual consistency, or causal consistency. The choice of model depends on the application&#39;s requirements.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gossip Protocols&lt;/strong&gt;: Gossip protocols are used for disseminating information about the state of nodes in a cluster. They are often employed in decentralized systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Anti-Entropy and Merkle Trees&lt;/strong&gt;: To ensure consistency between replicas, anti-entropy processes compare data in replicas and repair inconsistencies. Merkle trees are a data structure that helps efficiently detect and correct differences.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Takeaway&lt;/strong&gt;: Replication is a critical technique for improving availability, fault tolerance, and performance in data-intensive systems. However, it introduces challenges related to consistency, conflict resolution, and latency.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
">DDIA notes - Chapter 5: Replication</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/ddia-notes-chapter-4-encoding-and-evolution/"" data-c="
          &lt;p&gt;Chapter 4 explores the challenges of data encoding and schema evolution in data-intensive applications. It emphasizes the importance of handling schema changes while maintaining compatibility and provides insights into various techniques and serialization frameworks to achieve this goal.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Serialization&lt;/strong&gt;: Data serialization is the process of converting data structures or objects into a format that can be easily stored, transmitted, and reconstructed. Common serialization formats include JSON, XML, and binary formats.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Schema&lt;/strong&gt;: A schema defines the structure and types of data in a format. Schemas can be implicit (unstructured data like JSON) or explicit (XML Schema, Protocol Buffers, Avro schema).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Schema Evolution&lt;/strong&gt;: Over time, the structure of data often changes. Schema evolution deals with how to handle these changes in a way that does not break compatibility with existing data and clients.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: A new version of a schema is backward compatible if it can be read by an old version of the software. This ensures that old clients can still read data produced by newer clients.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Forward Compatibility&lt;/strong&gt;: A new version of a schema is forward compatible if it can read data produced by an old version of the software. This ensures that new clients can read data produced by older clients.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Compatibility Techniques&lt;/strong&gt;: Techniques like adding optional fields, providing default values, and using a version number can help achieve schema compatibility.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Avro&lt;/strong&gt;: Apache Avro is a popular data serialization framework that provides schema evolution support. It allows schemas to evolve and supports both backward and forward compatibility.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Thrift and Protocol Buffers&lt;/strong&gt;: Apache Thrift and Google Protocol Buffers are other serialization frameworks that provide schema evolution support and are widely used in distributed systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Message-Passing Data&lt;/strong&gt;: In distributed systems, messages are often used to communicate between different components or services. Serializing messages correctly is critical for interoperability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dataflow&lt;/strong&gt;: Dataflow systems, like Apache Beam, use a different approach to data serialization, focusing on the transformation of data records rather than fixed schemas.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Event Sourcing&lt;/strong&gt;: Event sourcing is a pattern where all changes to application state are captured as a sequence of immutable events. It requires careful handling of schema evolution to ensure historical data remains readable.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Schema Registry&lt;/strong&gt;: A schema registry is a centralized service that stores and manages schemas. It helps ensure consistency and compatibility between producers and consumers of data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Encoding&lt;/strong&gt;: Data encoding refers to the representation of data in a specific format for efficient storage and transmission. Common encodings include binary, text-based (UTF-8), and various compression formats.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Types&lt;/strong&gt;: Different programming languages and systems have different data types, and encoding needs to handle these types correctly during serialization and deserialization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Code Generation&lt;/strong&gt;: Some serialization frameworks generate code from schemas, allowing for efficient serialization and deserialization in different programming languages.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Takeaway&lt;/strong&gt;: Effective data encoding and schema evolution strategies are crucial for long-term data compatibility and system maintainability in data-intensive applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
">DDIA notes - Chapter 4: Encoding and Evolution</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/ddia-notes-chapter-3-storage-and-retrieval/"" data-c="
          &lt;p&gt;Chapter 3 dive into the fundamentals of data storage and retrieval, covering various storage subsystems, data structures, and access patterns. It provides insights into how different storage choices can impact system performance and scalability.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Storage&lt;/strong&gt;: In data-intensive applications, data storage is a critical component. Efficient storage and retrieval are essential for system performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Storage Subsystems&lt;/strong&gt;: There are various storage subsystems, each optimized for specific access patterns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;File Systems&lt;/strong&gt;: Suitable for storing large files, often used for multimedia data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relational Databases&lt;/strong&gt;: Provide structured storage and powerful querying capabilities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NoSQL Databases&lt;/strong&gt;: Offer various data models and scalability options.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Search Indexes&lt;/strong&gt;: Specialized for full-text search and text retrieval.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Column-Family Stores&lt;/strong&gt;: Optimized for write-heavy workloads.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Graph Databases&lt;/strong&gt;: Designed for traversing and querying relationships in data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Access Patterns&lt;/strong&gt;: The choice of storage subsystem depends on the access patterns of the application. Considerations include read-heavy vs. write-heavy workloads and the complexity of queries.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Structures&lt;/strong&gt;: Efficient data structures, such as B-trees and LSM-trees, are crucial for storage systems to support operations like insertion, deletion, and range queries.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Indexes&lt;/strong&gt;: Indexes are used to accelerate data retrieval by providing a fast path to locate data. They can be implemented using various data structures, including B-trees, hash indexes, and bitmap indexes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Log-Structured Storage&lt;/strong&gt;: Many storage systems use a log-structured approach to write data sequentially, improving write performance and ensuring durability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Compaction&lt;/strong&gt;: Over time, data storage systems may accumulate obsolete data and need to perform compaction to reclaim space and improve performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Warehousing&lt;/strong&gt;: Data warehousing systems are optimized for analytical queries rather than transactional workloads. They often use columnar storage and query optimization techniques.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Caching&lt;/strong&gt;: Caching is an important technique to reduce latency by storing frequently accessed data in memory. Caches can be implemented at various levels, from application-level caching to distributed caching.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bloom Filters&lt;/strong&gt;: Bloom filters are probabilistic data structures used to test whether an element is a member of a set. They are useful for reducing expensive disk reads when querying data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key-Value Stores&lt;/strong&gt;: Key-value stores are simple and highly performant storage systems that are often used for caching and as building blocks for more complex data storage solutions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distributed Storage&lt;/strong&gt;: Distributed storage systems replicate data across multiple nodes to ensure fault tolerance and availability. Techniques like sharding and consistent hashing are used to distribute data effectively.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transactions&lt;/strong&gt;: Maintaining data consistency in distributed systems often requires implementing distributed transactions, which can be complex to ensure correctness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Durability&lt;/strong&gt;: Ensuring data durability involves mechanisms like write-ahead logs, replication, and synchronization between replicas.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Compression&lt;/strong&gt;: Data compression techniques can significantly reduce storage costs and improve retrieval speed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Takeaway&lt;/strong&gt;: The choice of storage subsystem and data structures significantly impacts the performance, scalability, and reliability of a data-intensive application. Understanding the trade-offs between different storage options is crucial for designing effective systems.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
">DDIA notes - Chapter 3: Storage and Retrieval</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/ddia-notes-chapter-2-data-models-and-query-languages/"" data-c="
          &lt;p&gt;Chapter 2 provides a foundation for understanding the different data models and query languages used in data-intensive applications. It emphasizes that the choice of data model should align with the specific requirements and characteristics of the application and its data.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Models&lt;/strong&gt;: Data models are high-level descriptions of the structure and organization of data. They play a crucial role in how data is stored, retrieved, and processed in a system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational Model&lt;/strong&gt;: The relational data model, based on tables with rows and columns, is one of the most widely used models in databases. It supports powerful querying and is known for its simplicity and rigor.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Schema&lt;/strong&gt;: A schema defines the structure of data, including data types, constraints, and relationships between data elements.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NoSQL Data Models&lt;/strong&gt;: NoSQL databases offer alternatives to the relational model, including document stores, key-value stores, wide-column stores, and graph databases. These models are often more flexible but may require application-level logic for data consistency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Schemaless Models&lt;/strong&gt;: Some NoSQL databases are schemaless, allowing data to have different structures within the same database.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Query Languages&lt;/strong&gt;: The choice of query language is closely tied to the data model. SQL is the standard query language for relational databases, while NoSQL databases often have their query languages or APIs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ACID and BASE&lt;/strong&gt;: ACID (Atomicity, Consistency, Isolation, Durability) and BASE (Basically Available, Soft state, Eventually consistent) are two sets of properties that describe the trade-offs between consistency and availability in distributed systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CAP Theorem&lt;/strong&gt;: The CAP theorem states that it is impossible for a distributed system to simultaneously provide all three of the following guarantees: Consistency, Availability, and Partition tolerance. Systems must make trade-offs in the face of network partitions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consistency Models&lt;/strong&gt;: Different databases offer different consistency models, such as strong consistency, eventual consistency, and causal consistency. The choice of model depends on the application&#39;s requirements.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Immutability&lt;/strong&gt;: Some systems embrace immutability, where data is never updated but instead new versions are created. This simplifies some aspects of data management but may require new strategies for handling updates.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Query Languages for NoSQL&lt;/strong&gt;: NoSQL databases often have query languages or APIs tailored to their data models. For example, MongoDB uses a JSON-like query language, while Cassandra uses CQL (Cassandra Query Language).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Polyglot Persistence&lt;/strong&gt;: It&#39;s common to use multiple data storage technologies within a single application to leverage the strengths of each for different use cases. This is known as polyglot persistence.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Takeaway&lt;/strong&gt;: The choice of data model and query language significantly impacts how data is stored, retrieved, and processed in a system. Understanding the trade-offs between different models and languages is essential for designing effective data-intensive applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
">DDIA notes - Chapter 2: Data Models and Query Languages</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/ddia-notes-chapter-1-reliable-scalable-and-maintainable-applications/"" data-c="
          &lt;p&gt;This chapter sets the stage for the rest of the book by introducing the core principles and challenges associated with designing data-intensive applications. It highlights the importance of balancing reliability, scalability, and maintainability while making design choices.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Data-Intensive Applications: These are applications where data is a central part of the problem being solved. Examples include social networks, e-commerce platforms, and financial systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Three Key Properties:&lt;br&gt;
Reliability: The system should continue to work correctly, even in the face of hardware or software faults.&lt;br&gt;
Scalability: The system should handle an increase in load by adding resources, making it performant as the load grows.&lt;br&gt;
Maintainability: Developers should be able to understand, update, and extend the system with ease.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Trade-offs: There are often trade-offs between these properties. For example, achieving high availability (reliability) might require sacrificing some degree of performance (scalability).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fundamental Questions:&lt;br&gt;
How do you store data reliably?&lt;br&gt;
How do you ensure data is available for reading and writing?&lt;br&gt;
How do you make the system scalable as the data volume and user load increase?&lt;br&gt;
How do you maintain the system over time without it becoming a nightmare to work on?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reliability: Achieving reliability often involves replication, redundancy, and fault tolerance. The system should gracefully handle failures without user-visible issues.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scalability: Scalability can be achieved through techniques like load balancing, partitioning, and caching. It&#39;s crucial to understand the difference between vertical and horizontal scaling.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maintainability: A well-maintained system is essential for long-term success. Good documentation, monitoring, and automation can greatly aid maintainability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Complexity: Data-intensive systems can become complex due to various factors, including distributed architectures, concurrency, and evolving requirements. Managing this complexity is a significant challenge.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Abstraction Layers: Data systems often have multiple layers of abstractions, from high-level programming languages to distributed storage systems. Understanding these layers helps in making informed decisions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Key Takeaway: The goal of this book is to equip you with the knowledge and tools to make informed decisions when designing, building, and maintaining data-intensive applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
">DDIA notes - Chapter 1: Reliable, Scalable, and Maintainable Applications</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/time_waitclose_wait-jie-jue-fang-an/"" data-c="
          &lt;p&gt;&lt;strong&gt;服务器大量处于TIME_WAIT状态，可能是什么原因，会造成什么影响，怎么解决？CLOSE_WAIT状态呢？&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-time_wait与close_wait产生原因&#34;&gt;1. TIME_WAIT与CLOSE_WAIT产生原因&lt;/h2&gt;
&lt;p&gt;TIME_WAIT与CLOSE_WAIT 发生于客户端与服务器断开连接时四次挥手协议。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/9Nl2rxO6iqThZ1A.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;表示 TCP 断开连接的时候,需要客户端和服务端总共发送 4 个包以确认连接的断开；客户端或服务器均可主动发起挥手动作(因为 TCP 是一个全双工协议)，在 socket 编程中，任何一方执行 close() 操作即可产生挥手操作。&lt;/p&gt;
&lt;p&gt;上面的一次socket关闭操作，我们可以得出以下几点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;主动关闭连接的一方，也就是主动调用socket的close操作的一方，最终会进入TIME_WAIT状态&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;被动关闭连接的一方，有一个中间状态，即CLOSE_WAIT，因为协议层在等待上层的应用程序，主动调close操作后才主动关闭这条连接&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TIME_WAIT会默认等待2MSL时间后，才最终进入CLOSED状态。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在一个连接没有进入CLOSED状态之前，这个连接是不能被重用的！&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以，TIME_WAIT并不可怕，CLOSE_WAIT才可怕，因为CLOSE_WAIT很多，表示说要么是你的应用程序写的有问题，没有合适的关闭socket；要么是说，你的服务器CPU处理不过来（CPU太忙）或者你的应用程序一直睡眠到其它地方(锁，或者文件I/O等等)，你的应用程序获得不到合适的调度时间，造成你的程序没法真正的执行close操作。&lt;/p&gt;
&lt;h2 id=&#34;2-time_wait-状态存在的理由&#34;&gt;2. TIME_WAIT 状态存在的理由&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;可靠地实现TCP全双工连接的终止&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在进行关闭连接四次挥手协议时，最后的ACK是由主动关闭端发出的，如果这个最终的ACK丢失，服务器将重发最终的FIN，因此客户端必须维护状态信息允许它重发最终的ACK。如果不维持这个状态信息，那么客户端将响应RST分节，服务器将此分节解释成一个错误（在java中会抛出connection reset的SocketException)。&lt;/p&gt;
&lt;p&gt;因而，要实现TCP全双工连接的正常终止，必须处理终止序列四个分节中任何一个分节的丢失情况，主动关闭的客户端必须维持状态信息进入TIME_WAIT状态。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;允许老的重复分节在网络中消逝&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TCP分节可能由于路由器异常而“迷途”，在迷途期间，TCP发送端可能因确认超时而重发这个分节，迷途的分节在路由器修复后也会被送到最终目的地，这个原来的迷途分节就称为lost duplicate。&lt;/p&gt;
&lt;p&gt;在关闭一个TCP连接后，马上又重新建立起一个相同的IP地址和端口之间的TCP连接，后一个连接被称为前一个连接的化身（incarnation)，那么有可能出现这种情况，前一个连接的迷途重复分组在前一个连接终止后出现，从而被误解成从属于新的化身。&lt;/p&gt;
&lt;p&gt;为了避免这个情况，TCP不允许处于TIME_WAIT状态的连接启动一个新的化身，因为TIME_WAIT状态持续2MSL，就可以保证当成功建立一个TCP连接的时候，来自连接先前化身的重复分组已经在网络中消逝。&lt;/p&gt;
&lt;h2 id=&#34;3-time_wait与close_wait解决办法&#34;&gt;3. TIME_WAIT与CLOSE_WAIT解决办法&lt;/h2&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/aw9vHMV1QJl7hjO.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h3 id=&#34;31-查询tcp连接数&#34;&gt;3.1 查询TCP连接数&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;netstat -ant|awk &#39;/^tcp/ {++state[$NF]} END {for(key in state) print (key,state[key])}&#39; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结果显示如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;LISTEN 70
CLOSE_WAIT 1
ESTABLISHED 283
FIN_WAIT2 1
TIME_WAIT 170 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;状态描述：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CLOSED：无连接是活动的或正在进行
LISTEN：服务器在等待进入呼叫
SYN_RECV：一个连接请求已经到达，等待确认
SYN_SENT：应用已经开始，打开一个连接
ESTABLISHED：正常数据传输状态
FIN_WAIT1：应用说它已经完成
FIN_WAIT2：另一边已同意释放
ITMED_WAIT：等待所有分组死掉
CLOSING：两边同时尝试关闭
TIME_WAIT：另一边已初始化一个释放
LAST_ACK：等待所有分组死掉 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;32-time_wait解决方案&#34;&gt;3.2 TIME_WAIT解决方案&lt;/h3&gt;
&lt;p&gt;编辑内核文件/etc/sysctl.conf，打开系统的TIMEWAIT重用和快速回收，加入以下内容：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;net.ipv4.tcp_syncookies = 1 表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；
net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；
net.ipv4.tcp_tw_recycle = 1 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。
net.ipv4.tcp_fin_timeout 修改系默认的 TIMEOUT 时间 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后执行 &lt;code&gt;/sbin/sysctl -p&lt;/code&gt; 让参数生效.&lt;/p&gt;
&lt;p&gt;如果性能不太理想，加入如下配置尝试：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;net.ipv4.tcp_keepalive_time = 1200 
#表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。
net.ipv4.ip_local_port_range = 1024 65000 
#表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为1024到65000。
net.ipv4.tcp_max_syn_backlog = 8192 
#表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。
net.ipv4.tcp_max_tw_buckets = 5000 
#表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。
默认为180000，改为5000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量，但是对于 Squid，效果却不大。此项参数可以控制TIME_WAIT套接字的最大数量，避免Squid服务器被大量的TIME_WAIT套接字拖死。 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;33-close_wait解决方案&#34;&gt;3.3 CLOSE_WAIT解决方案&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;检查代码！！！&lt;/strong&gt;&lt;/p&gt;
">TIME_WAIT、CLOSE_WAIT 解决方案</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/da-liang-close_wait-yuan-yin-fen-xi/"" data-c="
          &lt;p&gt;这一次重启真的无法解决问题了：一次 &lt;strong&gt;MySQL&lt;/strong&gt; 主动关闭，导致服务出现大量 &lt;strong&gt;CLOSE_WAIT&lt;/strong&gt; 的全流程排查过程。&lt;/p&gt;
&lt;p&gt;近日遇到一个线上服务 &lt;strong&gt;socket&lt;/strong&gt; 资源被不断打满的情况。通过各种工具分析线上问题,定位到问题代码。这里对该问题发现、修复过程进行一下复盘总结。&lt;/p&gt;
&lt;p&gt;先看两张图。一张图是服务正常时监控到的 &lt;strong&gt;socket&lt;/strong&gt; 状态，另一张当然就是异常啦！&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/soq4LBdIXCpgEzJ.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;图一：正常时监控&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/DxYypolmBR63Swq.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;图二：异常时监控&lt;/p&gt;
&lt;p&gt;从图中的表现情况来看，就是从 &lt;strong&gt;04:00&lt;/strong&gt; 开始，socket 资源不断上涨，每个谷底时重启后恢复到正常值，然后继续不断上涨不释放，而且每次达到峰值的间隔时间越来越短。&lt;/p&gt;
&lt;p&gt;重启后，排查了日志，没有看到 &lt;strong&gt;panic&lt;/strong&gt; ，此时也就没有进一步检查，真的以为重启大法好。&lt;/p&gt;
&lt;h2 id=&#34;情况说明&#34;&gt;情况说明&lt;/h2&gt;
&lt;p&gt;该服务使用Golang开发，已经上线正常运行将近一年，提供给其它服务调用，主要底层资源有DB/Redis/MQ。&lt;/p&gt;
&lt;p&gt;为了后续说明的方便，将服务的架构图进行一下说明。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;3&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/Hia2YeRWUcdunz3.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;图三：服务架构&lt;/p&gt;
&lt;p&gt;架构是非常简单。&lt;/p&gt;
&lt;p&gt;问题出现在早上 &lt;strong&gt;08:20&lt;/strong&gt; 左右开始的，报警收到该服务出现 &lt;strong&gt;504&lt;/strong&gt;，此时第一反应是该服务长时间没有重启（快两个月了），可能存在一些内存泄漏，没有多想直接进行了重启。也就是在图二第一个谷底的时候，经过重启服务恢复到正常水平（重启真好用，开心）。&lt;/p&gt;
&lt;p&gt;将近 &lt;strong&gt;14:00&lt;/strong&gt; 的时候，再次被告警出现了 &lt;strong&gt;504&lt;/strong&gt; ，当时心中略感不对劲，但由于当天恰好有一场大型促销活动，因此先立马再次重启服务。直到后续大概过了1小时后又开始告警，连续几次重启后，发现需要重启的时间间隔越来越短。此时发现问题绝不简单。&lt;strong&gt;这一次重启真的解决不了问题老&lt;/strong&gt;，因此立马申请机器权限、开始排查问题。下面的截图全部来源我的重现demo，与线上无关。&lt;/p&gt;
&lt;h2 id=&#34;发现问题&#34;&gt;发现问题&lt;/h2&gt;
&lt;p&gt;出现问题后，首先要进行分析推断、然后验证、最后定位修改。根据当时的表现是分别进行了以下猜想。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ps：后续截图全部来源自己本地复现时的截图&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;推断一&#34;&gt;推断一&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;socket 资源被不断打满，并且之前从未出现过，今日突然出现，&lt;strong&gt;怀疑是不是请求量太大压垮服务&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;经过查看实时 &lt;strong&gt;qps&lt;/strong&gt; 后，放弃该想法，虽然量有增加，但依然在服务器承受范围（远远未达到压测的基准值）。&lt;/p&gt;
&lt;h3 id=&#34;推断二&#34;&gt;推断二&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;两台机器故障是同时发生，重启一台，另外一台也会得到缓解，作为独立部署在两个集群的服务非常诡异&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有了上面的的依据，推出的结果是肯定是该服务依赖的底层资源除了问题，要不然不可能独立集群的服务同时出问题。&lt;/p&gt;
&lt;p&gt;由于监控显示是 &lt;strong&gt;socket&lt;/strong&gt; 问题，因此通过 &lt;strong&gt;netstat&lt;/strong&gt; 命令查看了当前tcp链接的情况（本地测试，线上实际值大的多）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/go/src/hello # netstat -na | awk &#39;/^tcp/ {++S\[$NF\]} END {for(a in S) print a, S\[a\]}&#39;
LISTEN 2
CLOSE_WAIT 23 # 非常异常
TIME_WAIT 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;发现绝大部份的链接处于 &lt;strong&gt;CLOSE_WAIT&lt;/strong&gt; 状态，这是非常不可思议情况。然后用 &lt;code&gt;netstat -an&lt;/code&gt; 命令进行了检查。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;4&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/MxQCynX9gw6vcPh.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;图四：大量的CLOSE_WAIT&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;CLOSED 表示socket连接没被使用。&lt;br&gt;
LISTENING 表示正在监听进入的连接。&lt;br&gt;
SYN_SENT 表示正在试着建立连接。&lt;br&gt;
SYN_RECEIVED 进行连接初始同步。&lt;br&gt;
ESTABLISHED 表示连接已被建立。&lt;br&gt;
CLOSE_WAIT 表示远程计算器关闭连接，正在等待socket连接的关闭。&lt;br&gt;
FIN_WAIT_1 表示socket连接关闭，正在关闭连接。&lt;br&gt;
CLOSING 先关闭本地socket连接，然后关闭远程socket连接，最后等待确认信息。&lt;br&gt;
LAST_ACK 远程计算器关闭后，等待确认信号。&lt;br&gt;
FIN_WAIT_2 socket连接关闭后，等待来自远程计算器的关闭信号。&lt;br&gt;
TIME_WAIT 连接关闭后，等待远程计算器关闭重发。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;然后开始重点思考为什么会出现大量的mysql连接是 &lt;strong&gt;CLOSE_WAIT&lt;/strong&gt; 呢？为了说清楚，我们来插播一点TCP的四次挥手知识。&lt;/p&gt;
&lt;h4 id=&#34;tcp四次挥手&#34;&gt;TCP四次挥手&lt;/h4&gt;
&lt;p&gt;我们来看看 &lt;strong&gt;TCP&lt;/strong&gt; 的四次挥手是怎么样的流程：&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;5&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/WTGMcmFRDYxBXus.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;图五：TCP四次挥手&lt;/p&gt;
&lt;p&gt;用中文来描述下这个过程：&lt;/p&gt;
&lt;p&gt;Client: &lt;code&gt;服务端大哥，我事情都干完了，准备撤了&lt;/code&gt;，这里对应的就是客户端发了一个&lt;strong&gt;FIN&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Server：&lt;code&gt;知道了，但是你等等我，我还要收收尾&lt;/code&gt;，这里对应的就是服务端收到 &lt;strong&gt;FIN&lt;/strong&gt; 后回应的 &lt;strong&gt;ACK&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;经过上面两步之后，服务端就会处于 &lt;strong&gt;CLOSE_WAIT&lt;/strong&gt; 状态。过了一段时间 &lt;strong&gt;Server&lt;/strong&gt; 收尾完了&lt;/p&gt;
&lt;p&gt;Server：&lt;code&gt;小弟，哥哥我做完了，撤吧&lt;/code&gt;，服务端发送了&lt;strong&gt;FIN&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Client：&lt;code&gt;大哥，再见啊&lt;/code&gt;，这里是客户端对服务端的一个 &lt;strong&gt;ACK&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;到此服务端就可以跑路了，但是客户端还不行。为什么呢？客户端还必须等待 &lt;strong&gt;2MSL&lt;/strong&gt; 个时间，这里为什么客户端还不能直接跑路呢？主要是为了防止发送出去的 &lt;strong&gt;ACK&lt;/strong&gt; 服务端没有收到，服务端重发 &lt;strong&gt;FIN&lt;/strong&gt; 再次来询问，如果客户端发完就跑路了，那么服务端重发的时候就没人理他了。这个等待的时间长度也很讲究。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Maximum Segment Lifetime&lt;/strong&gt; 报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这里一定不要被图里的 &lt;strong&gt;client／server&lt;/strong&gt; 和项目里的客户端服务器端混淆，你只要记住：主动关闭的一方发出 &lt;strong&gt;FIN&lt;/strong&gt; 包（Client），被动关闭（Server）的一方响应 &lt;strong&gt;ACK&lt;/strong&gt; 包，此时，被动关闭的一方就进入了 &lt;strong&gt;CLOSE_WAIT&lt;/strong&gt; 状态。如果一切正常，稍后被动关闭的一方也会发出 &lt;strong&gt;FIN&lt;/strong&gt; 包，然后迁移到 &lt;strong&gt;LAST_ACK&lt;/strong&gt; 状态。&lt;/p&gt;
&lt;p&gt;既然是这样， &lt;strong&gt;TCP&lt;/strong&gt; 抓包分析下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/go # tcpdump -n port 3306
\# 发生了 3次握手
11:38:15.679863 IP 172.18.0.5.38822 &amp;gt; 172.18.0.3.3306: Flags \[S\], seq 4065722321, win 29200, options \[mss 1460,sackOK,TS val 2997352 ecr 0,nop,wscale 7\], length 0
11:38:15.679923 IP 172.18.0.3.3306 &amp;gt; 172.18.0.5.38822: Flags \[S.\], seq 780487619, ack 4065722322, win 28960, options \[mss 1460,sackOK,TS val 2997352 ecr 2997352,nop,wscale 7\], length 0
11:38:15.679936 IP 172.18.0.5.38822 &amp;gt; 172.18.0.3.3306: Flags \[.\], ack 1, win 229, options \[nop,nop,TS val 2997352 ecr 2997352\], length 0
 \# mysql 主动断开链接
11:38:45.693382 IP 172.18.0.3.3306 &amp;gt; 172.18.0.5.38822: Flags \[F.\], seq 123, ack 144, win 227, options \[nop,nop,TS val 3000355 ecr 2997359\], length 0 # MySQL负载均衡器发送fin包给我
11:38:45.740958 IP 172.18.0.5.38822 &amp;gt; 172.18.0.3.3306: Flags \[.\], ack 124, win 229, options \[nop,nop,TS val 3000360 ecr 3000355\], length 0 # 我回复ack给它

... ... # 本来还需要我发送fin给他，但是我没有发，所以出现了close_wait。那这是什么缘故呢？
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;src &amp;gt; dst: flags data-seqno ack window urgent options&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;src &amp;gt; dst 表明从源地址到目的地址&lt;br&gt;
flags 是TCP包中的标志信息,S 是SYN标志, F(FIN), P(PUSH) , R(RST) &amp;quot;.&amp;quot;(没有标记)&lt;br&gt;
data-seqno 是数据包中的数据的顺序号&lt;br&gt;
ack 是下次期望的顺序号&lt;br&gt;
window 是接收缓存的窗口大小&lt;br&gt;
urgent 表明数据包中是否有紧急指针&lt;br&gt;
options 是选项&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;结合上面的信息，我用文字说明下：&lt;strong&gt;MySQL负载均衡器&lt;/strong&gt; 给我的服务发送 &lt;strong&gt;FIN&lt;/strong&gt; 包，我进行了响应，此时我进入了 &lt;strong&gt;CLOSE_WAIR&lt;/strong&gt; 状态，但是后续作为被动关闭方的我，并没有发送 &lt;strong&gt;FIN&lt;/strong&gt;，导致我服务端一直处于 &lt;strong&gt;CLOSE_WAIR&lt;/strong&gt; 状态，无法最终进入 &lt;strong&gt;CLOSED&lt;/strong&gt; 状态。&lt;/p&gt;
&lt;p&gt;那么我推断出现这种情况可能的原因有以下几种：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;负载均衡器&lt;/strong&gt; 异常退出了，&lt;/p&gt;
&lt;p&gt;&lt;code&gt;这基本是不可能的，他出现问题绝对是大面积的服务报警，而不仅仅是我一个服务&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MySQL负载均衡器&lt;/strong&gt; 的超时设置的太短了，导致业务代码还没有处理完，&lt;strong&gt;MySQL负载均衡器&lt;/strong&gt; 就关闭tcp连接了&lt;/p&gt;
&lt;p&gt;&lt;code&gt;这也不太可能，因为这个服务并没有什么耗时操作，当然还是去检查了负载均衡器的配置，设置的是60s。&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;代码问题，&lt;strong&gt;MySQL&lt;/strong&gt; 连接无法释放&lt;/p&gt;
&lt;p&gt;&lt;code&gt;目前看起来应该是代码质量问题，加之本次数据有异常，触发到了以前某个没有测试到的点，目前看起来很有可能是这个原因&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;查找错误原因&#34;&gt;查找错误原因&lt;/h2&gt;
&lt;p&gt;由于代码的业务逻辑并不是我写的，我担心一时半会看不出来问题，所以直接使用 &lt;code&gt;perf&lt;/code&gt; 把所有的调用关系使用火焰图给绘制出来。既然上面我们推断代码中没有释放mysql连接。无非就是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;确实没有调用close&lt;/li&gt;
&lt;li&gt;有耗时操作（火焰图可以非常明显看到），导致超时了&lt;/li&gt;
&lt;li&gt;mysql的事务没有正确处理，例如：rollback 或者 commit&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由于火焰图包含的内容太多，为了让大家看清楚，我把一些不必要的信息进行了折叠。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;6&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/Pl4xt1CSQ5su6Lh.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;图六：有问题的火焰图&lt;/p&gt;
&lt;p&gt;火焰图很明显看到了开启了事务，但是在余下的部分，并没有看到 &lt;strong&gt;Commit&lt;/strong&gt; 或者是&lt;strong&gt;Rollback&lt;/strong&gt; 操作。这肯定会操作问题。然后也清楚看到出现问题的是：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MainController.update&lt;/strong&gt; 方法内部，话不多说，直接到 update 方法中去检查。发现了如下代码：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;func (c *MainController) update() (flag bool) {
    o := orm.NewOrm()
    o.Using(&amp;quot;default&amp;quot;)
    
    o.Begin()
    nilMap := getMapNil()
    if nilMap == nil {
        return false
    }

    nilMap\[10\] = 1
    nilMap\[20\] = 2
    if nilMap == nil &amp;amp;&amp;amp; len(nilMap) == 0 {
        o.Rollback()
        return false
    }

    sql := &amp;quot;update tb_user set name=%s where id=%d&amp;quot;
    res, err := o.Raw(sql, &amp;quot;Bug&amp;quot;, 2).Exec()
    if err == nil {
        num, _ := res.RowsAffected()
        fmt.Println(&amp;quot;mysql row affected nums: &amp;quot;, num)
        o.Commit()
        return true
    }

    o.Rollback()
    return false
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;至此，全部分析结束。经过查看 &lt;strong&gt;getMapNil&lt;/strong&gt; 返回了nil，但是下面的判断条件没有进行回滚。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if nilMap == nil {
    o.Rollback()
    return false
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;整个分析过程还是废了不少时间。最主要的是主观意识太强，觉得运行了一年没有出问题的为什么会突然出问题？因此一开始是质疑 SRE、DBA、各种基础设施出了问题（人总是先怀疑别人）。导致在这上面费了不少时间。&lt;/p&gt;
&lt;p&gt;理一下正确的分析思路：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;出现问题后，立马应该检查日志，确实日志没有发现问题；&lt;/li&gt;
&lt;li&gt;监控明确显示了socket不断增长，很明确立马应该使用 &lt;code&gt;netstat&lt;/code&gt; 检查情况看看是哪个进程的锅；&lt;/li&gt;
&lt;li&gt;根据 &lt;code&gt;netstat&lt;/code&gt; 的检查，使用 &lt;code&gt;tcpdump&lt;/code&gt; 抓包分析一下为什么连接会&lt;strong&gt;被动断开&lt;/strong&gt;（TCP知识非常重要）；&lt;/li&gt;
&lt;li&gt;如果熟悉代码应该直接去检查业务代码，如果不熟悉则可以使用 &lt;code&gt;perf&lt;/code&gt; 把代码的调用链路打印出来；&lt;/li&gt;
&lt;li&gt;不论是分析代码还是火焰图，到此应该能够很快定位到问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;那么本次到底是为什么会出现 &lt;strong&gt;CLOSE_WAIR&lt;/strong&gt; 呢？大部分同学应该已经明白了，我这里再简单说明一下：&lt;/p&gt;
&lt;p&gt;由于那一行代码没有对事务进行回滚，导致服务端没有主动发起close。因此 &lt;strong&gt;MySQL负载均衡器&lt;/strong&gt; 在达到 60s 的时候主动触发了close操作，但是通过tcp抓包发现，服务端并没有进行回应，这是因为代码中的事务没有处理，因此从而导致大量的端口、连接资源被占用。在贴一下挥手时的抓包数据：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;\# mysql 主动断开链接
11:38:45.693382 IP 172.18.0.3.3306 &amp;gt; 172.18.0.5.38822: Flags \[F.\], seq 123, ack 144, win 227, options \[nop,nop,TS val 3000355 ecr 2997359\], length 0 # MySQL负载均衡器发送fin包给我
11:38:45.740958 IP 172.18.0.5.38822 &amp;gt; 172.18.0.3.3306: Flags \[.\], ack 124, win 229, options \[nop,nop,TS val 3000360 ecr 3000355\], length 0 # 我回复ack给它
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我参考的一篇文章对这种情况提出了两个思考题，我觉得非常有意义，大家自己思考下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为什么一台机器几百个 &lt;strong&gt;CLOSE_WAIR&lt;/strong&gt; 就导致不可继续访问？我们不是经常说一台机器有 &lt;strong&gt;65535&lt;/strong&gt; 个文件描述符可用吗？&lt;/li&gt;
&lt;li&gt;为什么我有负载均衡，而两台部署服务的机器确几乎同时出了 &lt;strong&gt;CLOSE_WAIR&lt;/strong&gt; ?&lt;/li&gt;
&lt;/ol&gt;
">CLOSE_WAIT 原因分析</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/go-zhong-dui-epoll-de-feng-zhuang/"" data-c="
          &lt;h2 id=&#34;一-golang-net的使用方式&#34;&gt;&lt;strong&gt;一、Golang net的使用方式&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;开头我先把一个基于官方 net 包的 golang 服务的简单使用代码给大家列出来。为了方便理解，只保留骨干代码。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;func main() {
 //构造一个listener
 listener, _ := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;127.0.0.1:9008&amp;quot;)
 for {
  //接收请求
  conn, err := listener.Accept()
  //启动一个协程来处理
  go process(conn)
 }
}

func process(conn net.Conn) {
 //结束时关闭连接
 defer conn.Close()
 //读取连接上的数据
 var buf [1024]byte
 len, err := conn.Read(buf[:])
 //发送数据
 _, err = conn.Write([]byte(&amp;quot;I am server!&amp;quot;))

 ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在这个示例服务程序中，先是使用 net.Listen 来监听了本地的 9008 这个端口。然后调用 Accept 进行接收连接处理。如果接收到了连接请求，通过 go process() 来启动一个协程进行处理。在连接的处理中我展示了读写操作（Read 和 Write）。&lt;/p&gt;
&lt;p&gt;整个服务程序看起来，妥妥的就是一个同步模型，包括 Accept、Read 和 Write 都会将当前协程给“阻塞”掉。比如 Read 函数这里，如果服务器调用时客户端数据还没有到达，那么 Read 是不带返回的，会将当前的协程 park 住。直到有了数据 Read 才会返回，处理协程继续执行。&lt;/p&gt;
&lt;p&gt;你如果在其它语言，例如 C 和 Java 中写出这样类似的服务器代码，估计会被打死的。因为每一次同步的 Accept、Read、Write 都会导致你当前的线程被阻塞掉，会浪费大量的 CPU 进行线程上下文的切换。&lt;/p&gt;
&lt;p&gt;但是在 golang 中这样的代码运行性能却是非常的不错，为啥呢？我们继续看本文接下来的内容。&lt;/p&gt;
&lt;h2 id=&#34;二-listen-底层过程&#34;&gt;&lt;strong&gt;二、Listen 底层过程&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;在传统的 C、Java 等传统语言中，listen 所做的事情就是直接调用内核的 listen 系统调用。但是如果你也这么同等地理解 golang net 包里的 Listen， 那可就大错特错了。&lt;/p&gt;
&lt;p&gt;和其它语言不同，在 golang net 的 listen 中，会完成如下几件事：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;创建 socket 并设置非阻塞，&lt;/li&gt;
&lt;li&gt;bind 绑定并监听本地的一个端口&lt;/li&gt;
&lt;li&gt;调用 listen 开始监听&lt;/li&gt;
&lt;li&gt;epoll_create 创建一个 epoll 对象&lt;/li&gt;
&lt;li&gt;epoll_etl 将 listen 的 socket 添加到 epoll 中等待连接到来&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一次 Golang 的 Listen 调用，相当于在 C 语言中的 socket、bind、listen、epoll_create、epoll_etl 等多次函数调用的效果。封装度非常的高，更大程度地对程序员屏蔽了底层的实现细节。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;插一句题外话：现在的各种开发工具的封装程度越来越高，真不知道对码农来说是好事还是坏事。好处是开发效率更高了，缺点是将来的程序员想了解底层也越来越难了，越来越像传统企业里流水线上的工人。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;内部源码瞅一瞅。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/HXiMq7nRAO1LSge.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;Listen 的入口在 golang 源码的 net/dial.go 文件中，让我们展开来看更细节的逻辑。&lt;/p&gt;
&lt;h3 id=&#34;21-listen-入口执行流程&#34;&gt;&lt;strong&gt;2.1 Listen 入口执行流程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;源码不用细看，看懂大概流程就可以。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:go1.14.4/src/net/dial.go
func Listen(network, address string) (Listener, error) {
 var lc ListenConfig
 return lc.Listen(context.Background(), network, address)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可见，这个 Listen 只是一个入口。接下来会进入到 ListenConfig 下的 Listen 方法中。在 ListenConfig 的 Listen 中判断这是一个 TCP 类型的话，会进入到 sysListener 下的 listenTCP 方法里（src/net/tcpsock_posix.go）。然后再经过两三次的函数调用跳转，会进入到 net/sock_posix.go 文件下的 socket 函数中。我们直接看它。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:go1.14.4/src/net/sock_posix.go
func socket(ctx context.Context, net string, family, ...) (fd *netFD, err error) {
 //创建 socket，见 2.2 小节 
 s, err := sysSocket(family, sotype, proto)

 ...

 //TCP 绑定和监听，见 2.3 小节
 //epoll对象的创建以及文件描述符的添加 见 2.4 小节
 if laddr != nil &amp;amp;&amp;amp; raddr == nil {
  switch sotype {
  case syscall.SOCK_STREAM, syscall.SOCK_SEQPACKET:
   fd.listenStream(laddr, listenerBacklog(), ctrlFn);
  ......
 }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;接下来我们分别在 2.2 和 2.3 小节来介绍 sysSocket 和 listenStream 这两个函数。&lt;/p&gt;
&lt;h3 id=&#34;22-创建-socket&#34;&gt;&lt;strong&gt;2.2 创建 socket&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;sysSocket 这个函数和其它语言中的 socket 函数有很大的不同。在这个一个函数内就完成了三件事，创建 socket、bind 和 listen 监听。我们来看 sysSocket的具体代码。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:net/sys_cloexec.go
func sysSocket(family, sotype, proto int) (int, error) {
 //创建 socket
 s, err := socketFunc(family, sotype, proto)

 //设置为非阻塞模式
 syscall.SetNonblock(s, true)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 sysSocket 中，调用的 socketFunc 其实就是 socket 系统调用。见如下代码。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:net/hook_unix.go
var (
 // Placeholders for socket system calls.
 socketFunc        func(int, int, int) (int, error)  = syscall.Socket
 connectFunc       func(int, syscall.Sockaddr) error = syscall.Connect
 listenFunc        func(int, int) error              = syscall.Listen
 getsockoptIntFunc func(int, int, int) (int, error)  = syscall.GetsockoptInt
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建完 socket 之后，再调用 syscall.SetNonblock 将其设置为非阻塞模式。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:syscall/exec_unix.go
func SetNonblock(fd int, nonblocking bool) (err error) {
 ...
 if nonblocking {
  flag |= O_NONBLOCK
 } 
 fcntl(fd, F_SETFL, flag)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;23-绑定和监听&#34;&gt;&lt;strong&gt;2.3 绑定和监听&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;我们接着再来看 listenStream。这个函数一进来就调用了系统调用 bind 和 listen 来完成了绑定和监听。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:net/sock_posix.go
func (fd *netFD) listenStream(laddr sockaddr,...) error 
{
 ...

 //等同于 c 语言中的： bind(listenfd, ...)
 syscall.Bind(fd.pfd.Sysfd, lsa);

 //等同于 c 语言中的：listen(listenfd, ...)
 listenFunc(fd.pfd.Sysfd, backlog);

 //这里非常关键：初始化socket与异步IO相关的内容
 if err = fd.init(); err != nil {
  return err
 }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 listenFunc 是一个宏，指向的就是 syscall.Listen 系统调用&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:go1.14.4/src/net/hook_unix.go
import &amp;quot;syscall&amp;quot;
var (
 // Placeholders for socket system calls.
 socketFunc        func(int, int, int) (int, error)  = syscall.Socket
 connectFunc       func(int, syscall.Sockaddr) error = syscall.Connect
 listenFunc        func(int, int) error              = syscall.Listen
 getsockoptIntFunc func(int, int, int) (int, error)  = syscall.GetsockoptInt
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;24-epoll创建和初始化&#34;&gt;&lt;strong&gt;2.4 epoll创建和初始化&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;接下来在 fd.init 这一行，经过多次的函数调用展开以后会执行到 epoll 对象的创建，并还把在 listen 状态的 socket 句柄添加到了 epoll 对象中来管理其网络事件。&lt;/p&gt;
&lt;p&gt;我们来看它是如何完成的。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:go1.14.4/src/internal/poll/fd_poll_runtime.go
func (pd *pollDesc) init(fd *FD) error {

 serverInit.Do(runtime_pollServerInit)
 ctx, errno := runtime_pollOpen(uintptr(fd.Sysfd))
 ...
 return nil
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;serverInit.Do 这个是用来保证参数内的函数只执行一次的。不过多展开介绍。其参数 runtime_pollServerInit 是对 runtime 包的函数 poll_runtime_pollServerInit 的调用，其源码位于 runtime/netpoll.go 下。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:runtime/netpoll.go
//go:linkname poll_runtime_pollServerInit internal/poll.runtime_pollServerInit
func poll_runtime_pollServerInit() {
 netpollGenericInit()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;该函数会执行到 netpollGenericInit， epoll 就是在它的内部创建的。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:netpoll_epoll.go
func netpollinit() {
 // epoll 对象的创建
 epfd = epollcreate1(_EPOLL_CLOEXEC)
 ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再来看 runtime_pollOpen。它的参数就是前面 listen 好了的 socket 的文件描述符。在这个函数里，它将被放到 epoll 对象中。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:runtime/netpoll_epoll.go
//go:linkname poll_runtime_pollOpen internal/poll.runtime_pollOpen
func poll_runtime_pollOpen(fd uintptr) (*pollDesc, int) {
 ...
 errno = netpollopen(fd, pd)
 return pd, int(errno)
}

//file:runtime/netpoll_epoll.go
func netpollopen(fd uintptr, pd *pollDesc) int32 {
 var ev epollevent
 ev.events = _EPOLLIN | _EPOLLOUT | _EPOLLRDHUP | _EPOLLET
 *(**pollDesc)(unsafe.Pointer(&amp;amp;ev.data)) = pd

 // listen 状态的 socket 被添加到了 epoll 中。
 return -epollctl(epfd, _EPOLL_CTL_ADD, int32(fd), &amp;amp;ev)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;三-accept-过程&#34;&gt;&lt;strong&gt;三、Accept 过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;服务端在 Listen 完了之后，就是对 Accept 的调用了。该函数主要做了三件事&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;调用 accept 系统调用接收一个连接&lt;/li&gt;
&lt;li&gt;如果没有连接到达，把当前协程阻塞掉&lt;/li&gt;
&lt;li&gt;新连接到来的话，将其添加到 epoll 中管理，然后返回&lt;/li&gt;
&lt;/ul&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/97Pdqp5jzYrv2Gf.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;通过 Golang 里的单步调试可以看到它进入到了 TCPListener 下的 Accept 里了。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file: net/tcpsock.go
func (l *TCPListener) Accept() (Conn, error) {
 c, err := l.accept()
 ...
}
func (ln *TCPListener) accept() (*TCPConn, error) {
 //以 netFD 的形式返回一个新连接
 fd, err := ln.fd.accept()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们上面说的三步都是在 netFD 的 accept 函数里处理的。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:net/fd_unix.go
func (fd *netFD) accept() (netfd *netFD, err error) {
 //3.1 接收一个连接
 //3.2 如果连接没有到达阻塞当前协程
 d, rsa, errcall, err := fd.pfd.Accept()

 //3.2 将新到的连接也添加到 epoll 中进行管理
 netfd, err = newFD(d, fd.family, fd.sotype, fd.net);
 netfd.init();

 ...
 return netfd, nil
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;接下来我们详细看每一步的细节。&lt;/p&gt;
&lt;h3 id=&#34;31-接收一个连接&#34;&gt;&lt;strong&gt;3.1 接收一个连接&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;经过单步跟踪后发现 Accept 进入到了 FD 对象的 Accept 方法下。在这里将调用操作系统的 accept 系统调用。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:internal/poll/fd_unix.go
// Accept wraps the accept network call.
func (fd *FD) Accept() (int, syscall.Sockaddr, string, error) {

 for {
  //调用 accept 系统调用接收一个连接
  s, rsa, errcall, err := accept(fd.Sysfd)

  //接收到了连接就返回它
  if err == nil {
   return s, rsa, &amp;quot;&amp;quot;, err
  }

  switch err {
  case syscall.EAGAIN:
   //如果没有获取到，那就把协程给阻塞起来
   if fd.pd.pollable() {
    if err = fd.pd.waitRead(fd.isFile); err == nil {
     continue
    }
   }
  ... 
 }
 ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 accept 方法内部会触发 linux 操作系统的 accept 系统调用，我们就不过度展开了。 调用 accept 目的是获取一个来自客户端的连接。如果接收到了，就把他返回回去。&lt;/p&gt;
&lt;h3 id=&#34;32-阻塞当前协程&#34;&gt;&lt;strong&gt;3.2 阻塞当前协程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;我们来说说如果没 accept 调用的时候，客户端的连接请求还一个都没有过来怎么办。&lt;/p&gt;
&lt;p&gt;这时候，accept 系统调用会返回 syscall.EAGAIN。Golang 在对这个状态的处理中，会把当前协程给阻塞起来。关键代码在这里&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file: internal/poll/fd_poll_runtime.go
func (pd *pollDesc) waitRead(isFile bool) error {
 return pd.wait(&#39;r&#39;, isFile)
}
func (pd *pollDesc) wait(mode int, isFile bool) error {
 if pd.runtimeCtx == 0 {
  return errors.New(&amp;quot;waiting for unsupported file type&amp;quot;)
 }
 res := runtime_pollWait(pd.runtimeCtx, mode)
 return convertErr(res, isFile)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;runtime_pollWait 的源码在 runtime/netpoll.go 下。gopark（协程的阻塞）就是在这里完成的。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:runtime/netpoll.go
//go:linkname poll_runtime_pollWait internal/poll.runtime_pollWait
func poll_runtime_pollWait(pd *pollDesc, mode int) int {
 ...
 for !netpollblock(pd, int32(mode), false) {
 }
}

func netpollblock(pd *pollDesc, mode int32, waitio bool) bool {
 ...
 if waitio || netpollcheckerr(pd, mode) == 0 {
  gopark(netpollblockcommit, unsafe.Pointer(gpp), waitReasonIOWait, traceEvGoBlockNet, 5)
 }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;gopark 这个函数就是 golang 内部阻塞协程的入口。&lt;/p&gt;
&lt;h3 id=&#34;33-将新连接添加到-epoll-中&#34;&gt;&lt;strong&gt;3.3 将新连接添加到 epoll 中。&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;我们再来说说假如客户端连接已经到来了的情况。这时 fd.pfd.Accept 会返回新建的连接。然后会将该新连接也一并加入到 epoll 中进行高效的事件管理。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:net/fd_unix.go
func (fd *netFD) accept() (netfd *netFD, err error) {
 //3.1 接收一个连接
 //3.2 如果连接没有到达阻塞当前协程
 d, rsa, errcall, err := fd.pfd.Accept()

 //3.2 将新到的连接也添加到 epoll 中进行管理
 netfd, err = newFD(d, fd.family, fd.sotype, fd.net);
 netfd.init();

 ...
 return netfd, nil
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们来看 netfd.init&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:internal/poll/fd_poll_runtime.go
func (pd *pollDesc) init(fd *FD) error {
 ...
 ctx, errno := runtime_pollOpen(uintptr(fd.Sysfd))
 ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;runtime_pollOpen 这个runtime 函数我们在上面的 2.4 节介绍过了，就是把文件句柄添加到 epoll 对象中。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:runtime/netpoll_epoll.go
//go:linkname poll_runtime_pollOpen internal/poll.runtime_pollOpen
func poll_runtime_pollOpen(fd uintptr) (*pollDesc, int) {
 ...
 errno = netpollopen(fd, pd)
 return pd, int(errno)
}

func netpollopen(fd uintptr, pd *pollDesc) int32 {
 var ev epollevent
 ev.events = _EPOLLIN | _EPOLLOUT | _EPOLLRDHUP | _EPOLLET
 *(**pollDesc)(unsafe.Pointer(&amp;amp;ev.data)) = pd

 //新连接的 socket 也被添加到了 epoll 中。
 return -epollctl(epfd, _EPOLL_CTL_ADD, int32(fd), &amp;amp;ev)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;四-read-和-write-内部过程&#34;&gt;&lt;strong&gt;四、Read 和 Write 内部过程&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;当连接接收完成后，剩下的就是在连接上的读写了。&lt;/p&gt;
&lt;h3 id=&#34;41-read-内部过程&#34;&gt;&lt;strong&gt;4.1 Read 内部过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;我们先来看 Read。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;3&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/elEzR1XMYhNBDks.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;来看详细的代码。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:/Users/zhangyanfei/sdk/go1.14.4/src/net/net.go
func (c *conn) Read(b []byte) (int, error) {
 ...
 n, err := c.fd.Read(b)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Read 函数会进入到 FD 的 Read 中。在这个函数内部调用 Read 系统调用来读取数据。如果数据还尚未到达则也是把自己阻塞起来。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:internal/poll/fd_unix.go
func (fd *FD) Read(p []byte) (int, error) {
 for {
  //调用 Read 系统调用
  n, err := syscall.Read(fd.Sysfd, p)
  if err != nil {
   n = 0

   //将自己添加到 epoll 中等待事件，然后阻塞掉。
   if err == syscall.EAGAIN &amp;amp;&amp;amp; fd.pd.pollable() {
    if err = fd.pd.waitRead(fd.isFile); err == nil {
     continue
    }
   }
  ...... 
 }  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 waitRead 是如何将当前协程阻塞掉的，这个和我们前面 3.2 节介绍的是一样的，就不过多展开叙述了。&lt;/p&gt;
&lt;h3 id=&#34;42-write-内部过程&#34;&gt;&lt;strong&gt;4.2 Write 内部过程&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Write 的大体过程和 Read 是类似的。 先是调用 Write 系统调用发送数据，如果内核发送缓存区不足的时候，就把自己先阻塞起来，然后等可写时间发生的时候再继续发送。其源码入口位于 net/net.go。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:net/net.go
func (c *conn) Write(b []byte) (int, error) {
 ...
 n, err := c.fd.Write(b)
}

//file:internal/poll/fd_unix.go
func (fd *FD) Write(p []byte) (int, error) {
 for {
  n, err := syscall.Write(fd.Sysfd, p[nn:max])
  if err == syscall.EAGAIN &amp;amp;&amp;amp; fd.pd.pollable() {
   if err = fd.pd.waitWrite(fd.isFile); err == nil {
    continue
   }
  }
 }
}

//file:internal/poll/fd_poll_runtime.go
func (pd *pollDesc) waitWrite(isFile bool) error {
 return pd.wait(&#39;w&#39;, isFile)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;pd.wait 之后的事情就又和 3.2 节介绍的过程一样了。调用 runtime_pollWait 来讲当前协程阻塞掉。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;func (pd *pollDesc) wait(mode int, isFile bool) error {
 ...
 res := runtime_pollWait(pd.runtimeCtx, mode)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;五-golang-唤醒&#34;&gt;&lt;strong&gt;五、Golang 唤醒&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;前面我们讨论的很多步骤里都涉及到协程的阻塞。例如 Accept 时如果新连接还尚未到达。再比如像 Read 数据的时候对方还没有发送，当前协程都不会占着 cpu 不放，而是会阻塞起来。&lt;/p&gt;
&lt;p&gt;那么当要等待的事件就绪的时候，被阻塞掉的协程又是如何被重新调度的呢？ 相信大家一定会好奇这个问题。&lt;/p&gt;
&lt;p&gt;Go 语言的运行时会在调度或者系统监控中调用 sysmon，它会调用 netpoll，来不断地调用 epoll_wait 来查看 epoll 对象所管理的文件描述符中哪一个有事件就绪需要被处理了。如果有，就唤醒对应的协程来进行执行。&lt;/p&gt;
&lt;p&gt;其实除此之外还有几个地方会唤醒协程，如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;startTheWorldWithSema&lt;/li&gt;
&lt;li&gt;findrunnable 在 schedule 中调用 有top 和 stop 之分。 其中 stop 中会导致阻塞。&lt;/li&gt;
&lt;li&gt;pollWork&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不过为了简便起见，我们只选择 sysmon 来作为一个切入口。sysmon 是一个周期性的监控协程，来看源码。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:src/runtime/proc.go
func sysmon() {
 ...
 list := netpoll(0) 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;它会不断触发对 netpoll 的调用，在 netpoll 会调用 epollwait 看查看是否有网络事件发生。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:runtime/netpoll_epoll.go
func netpoll(delay int64) gList {
 ...
retry:
 n := epollwait(epfd, &amp;amp;events[0], int32(len(events)), waitms)
 if n &amp;lt; 0 {
  //没有网络事件
  goto retry
 }

 for i := int32(0); i &amp;lt; n; i++ {

  //查看是读事件还是写事件发生
  var mode int32
  if ev.events&amp;amp;(_EPOLLIN|_EPOLLRDHUP|_EPOLLHUP|_EPOLLERR) != 0 {
   mode += &#39;r&#39;
  }
  if ev.events&amp;amp;(_EPOLLOUT|_EPOLLHUP|_EPOLLERR) != 0 {
   mode += &#39;w&#39;
  }

  if mode != 0 {

   pd := *(**pollDesc)(unsafe.Pointer(&amp;amp;ev.data))
   pd.everr = false
   if ev.events == _EPOLLERR {
    pd.everr = true
   }
   netpollready(&amp;amp;toRun, pd, mode)
  }
 }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 epoll 返回的时候，ev.data 中是就绪的网络 socket 的文件描述符。根据网络就绪 fd 拿到 pollDesc。在 netpollready 中，将对应的协程推入可运行队列等待调度执行。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;//file:runtime/netpoll.go
func netpollready(toRun *gList, pd *pollDesc, mode int32) {
 var rg, wg *g
 if mode == &#39;r&#39; || mode == &#39;r&#39;+&#39;w&#39; {
  rg = netpollunblock(pd, &#39;r&#39;, true)
 }
 if mode == &#39;w&#39; || mode == &#39;r&#39;+&#39;w&#39; {
  wg = netpollunblock(pd, &#39;w&#39;, true)
 }
 if rg != nil {
  toRun.push(rg)
 }
 if wg != nil {
  toRun.push(wg)
 }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;本文总结&#34;&gt;&lt;strong&gt;本文总结&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;同步编码方式的优点是符合人的直线思维。在这种模式下的代码很容易写，写出来也容易理解，但是缺点就是性能奇差。因为会导致频繁的线程上下文切换。&lt;/p&gt;
&lt;p&gt;所以现在 epoll 是 Linux 下网络程序工作的最主要的模式。现在各种语言下的流行的网络框架模型都是基于 epoll 来工作的。区别就是各自对 epoll 的使用方式上存在一些差别。主流各种基于 epoll 的异步非阻塞的模型虽然提高了性能，但是基于回调函数的编程方式却非常不符合人的的直线思维模式。开发出来的代码的也不那么容易被人理解。&lt;/p&gt;
&lt;p&gt;Golang开辟了一种新的网络编程模型。这种模型在应用层看来仍然是同步的方式。但是在底层确实通过协程和 epoll 的配合避免了线程切换的性能高损耗，因此并不会阻塞用户线程。代替的是切换开销更小的协程。协程的切换开销大约只有线程切换的三十分之一&lt;/p&gt;
&lt;p&gt;我个人一直觉得，Golang 封装的网络编程模型非常之精妙，是世界级的代码。它非常值得你好好学习一下。学完了觉得好的话，转发给你的朋友们一起来了解了解吧！&lt;/p&gt;
">Go中对epoll的封装</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/io-duo-lu-fu-yong/"" data-c="
          &lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/VRjbkphoYsgacDF.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h2 id=&#34;&#34;&gt;&lt;a href=&#34;#%E6%9C%80%E5%9F%BA%E6%9C%AC%E7%9A%84-socket-%E6%A8%A1%E5%9E%8B&#34;&gt;#&lt;/a&gt; 最基本的 Socket 模型&lt;/h2&gt;
&lt;p&gt;要使客户端和服务器能够在网络上进行通信，Socket编程是必不可少的。这是一种特殊的进程间通信方式，尤其以其能够在不同主机之间通信的能力而闻名。&lt;/p&gt;
&lt;p&gt;在中文中，Socket被称为“插口”，这乍一看可能有些令人困惑。实际上，为了实现两方之间的有效网络通信，每一方都需要创建一个Socket。可以将其想象成客户端和服务器各自为自己打开了一个“门户”。当它们想要交换数据，无论是读取还是发送，都是通过这个“门户”进行的。从这个角度来看，这不是很像布置了一根虚拟的网络电缆吗？一端插在客户端，另一端插在服务器，从而实现通信的流动。&lt;/p&gt;
&lt;p&gt;在创建Socket时，你可以指定在网络层使用IPv4还是IPv6，以及在传输层使用TCP还是UDP。&lt;/p&gt;
&lt;p&gt;使用UDP的Socket编程相对较简单。然而，在这里我们只会深入探讨基于TCP的Socket编程。&lt;/p&gt;
&lt;p&gt;要使服务器运行起来，首先需要启动它。然后，它会等待来自客户端的连接和数据。让我们首先看一下服务器端的Socket编程过程是如何展开的。&lt;/p&gt;
&lt;p&gt;服务端首先调用 &lt;code&gt;socket()&lt;/code&gt; 函数，创建网络协议为 IPv4，以及传输协议为 TCP 的 Socket ，接着调用 &lt;code&gt;bind()&lt;/code&gt; 函数，给这个 Socket 绑定一个 &lt;strong&gt;IP 地址和端口&lt;/strong&gt;，绑定这两个的目的是什么？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;绑定端口的目的：当内核收到 TCP 报文，通过 TCP 头里面的端口号，来找到我们的应用程序，然后把数据传递给我们。&lt;/li&gt;
&lt;li&gt;绑定 IP 地址的目的：一台机器是可以有多个网卡的，每个网卡都有对应的 IP 地址，当绑定一个网卡时，内核在收到该网卡上的包，才会发给我们；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;绑定完 IP 地址和端口后，就可以调用 &lt;code&gt;listen()&lt;/code&gt; 函数进行监听，此时对应 TCP 状态图中的 &lt;code&gt;listen&lt;/code&gt;，如果我们要判定服务器中一个网络程序有没有启动，可以通过 &lt;code&gt;netstat&lt;/code&gt; 命令查看对应的端口号是否有被监听。&lt;/p&gt;
&lt;p&gt;服务端进入了监听状态后，通过调用 &lt;code&gt;accept()&lt;/code&gt; 函数，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。&lt;/p&gt;
&lt;p&gt;那客户端是怎么发起连接的呢？客户端在创建好 Socket 后，调用 &lt;code&gt;connect()&lt;/code&gt; 函数发起连接，该函数的参数要指明服务端的 IP 地址和端口号，然后万众期待的 TCP 三次握手就开始了。&lt;/p&gt;
&lt;p&gt;在 TCP 连接的过程中，服务器的内核实际上为每个 Socket 维护了两个队列：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个是「还没完全建立」连接的队列，称为 &lt;strong&gt;TCP 半连接队列&lt;/strong&gt;，这个队列都是没有完成三次握手的连接，此时服务端处于 &lt;code&gt;syn_rcvd&lt;/code&gt; 的状态；&lt;/li&gt;
&lt;li&gt;一个是「已经建立」连接的队列，称为 &lt;strong&gt;TCP 全连接队列&lt;/strong&gt;，这个队列都是完成了三次握手的连接，此时服务端处于 &lt;code&gt;established&lt;/code&gt; 状态；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当 TCP 全连接队列不为空后，服务端的 &lt;code&gt;accept()&lt;/code&gt; 函数，就会从内核中的 TCP 全连接队列里拿出一个已经完成连接的 Socket 返回应用程序，后续数据传输都用这个 Socket。&lt;/p&gt;
&lt;p&gt;注意，监听的 Socket 和真正用来传数据的 Socket 是两个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个叫作&lt;strong&gt;监听 Socket&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;一个叫作&lt;strong&gt;已连接 Socket&lt;/strong&gt;；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;连接建立后，客户端和服务端就开始相互传输数据了，双方都可以通过 &lt;code&gt;read()&lt;/code&gt; 和 &lt;code&gt;write()&lt;/code&gt; 函数来读写数据。&lt;/p&gt;
&lt;p&gt;至此， TCP 协议的 Socket 程序的调用过程就结束了，整个过程如下图：&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/ko7YTBVvjfPeMzR.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;看到这，不知道你有没有觉得读写 Socket 的方式，好像读写文件一样。&lt;/p&gt;
&lt;p&gt;是的，基于 Linux 一切皆文件的理念，在内核中 Socket 也是以「文件」的形式存在的，也是有对应的文件描述符。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;PS : 下面会说到内核里的数据结构，不感兴趣的可以跳过这一部分，不会对后续的内容有影响。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;文件描述符的作用是什么？每一个进程都有一个数据结构 &lt;code&gt;task_struct&lt;/code&gt;，该结构体里有一个指向「文件描述符数组」的成员指针。该数组里列出这个进程打开的所有文件的文件描述符。数组的下标是文件描述符，是一个整数，而数组的内容是一个指针，指向内核中所有打开的文件的列表，也就是说内核可以通过文件描述符找到对应打开的文件。&lt;/p&gt;
&lt;p&gt;然后每个文件都有一个 inode，Socket 文件的 inode 指向了内核中的 Socket 结构，在这个结构体里有两个队列，分别是&lt;strong&gt;发送队列&lt;/strong&gt;和&lt;strong&gt;接收队列&lt;/strong&gt;，这个两个队列里面保存的是一个个 &lt;code&gt;struct sk_buff&lt;/code&gt;，用链表的组织形式串起来。&lt;/p&gt;
&lt;p&gt;sk_buff 可以表示各个层的数据包，在应用层数据包叫 data，在 TCP 层我们称为 segment，在 IP 层我们叫 packet，在数据链路层称为 frame。&lt;/p&gt;
&lt;p&gt;你可能会好奇，为什么全部数据包只用一个结构体来描述呢？协议栈采用的是分层结构，上层向下层传递数据时需要增加包头，下层向上层数据时又需要去掉包头，如果每一层都用一个结构体，那在层之间传递数据的时候，就要发生多次拷贝，这将大大降低 CPU 效率。&lt;/p&gt;
&lt;p&gt;于是，为了在层级之间传递数据时，不发生拷贝，只用 sk_buff 一个结构体来描述所有的网络包，那它是如何做到的呢？是通过调整 sk_buff 中 &lt;code&gt;data&lt;/code&gt; 的指针，比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当接收报文时，从网卡驱动开始，通过协议栈层层往上传送数据报，通过增加 skb-&amp;gt;data 的值，来逐步剥离协议首部。&lt;/li&gt;
&lt;li&gt;当要发送报文时，创建 sk_buff 结构体，数据缓存区的头部预留足够的空间，用来填充各层首部，在经过各下层协议时，通过减少 skb-&amp;gt;data 的值来增加协议首部。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你可以从下面这张图看到，当发送报文时，data 指针的移动过程。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;3&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/IqARTfb7xQWzgV5.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h2 id=&#34;-2&#34;&gt;&lt;a href=&#34;#%E5%A6%82%E4%BD%95%E6%9C%8D%E5%8A%A1%E6%9B%B4%E5%A4%9A%E7%9A%84%E7%94%A8%E6%88%B7&#34;&gt;#&lt;/a&gt; 如何服务更多的用户？&lt;/h2&gt;
&lt;p&gt;前面提到的 TCP Socket 调用流程是最简单、最基本的，它基本只能一对一通信，因为使用的是同步阻塞的方式，当服务端在还没处理完一个客户端的网络 I/O 时，或者 读写操作发生阻塞时，其他客户端是无法与服务端连接的。&lt;/p&gt;
&lt;p&gt;可如果我们服务器只能服务一个客户，那这样就太浪费资源了，于是我们要改进这个网络 I/O 模型，以支持更多的客户端。&lt;/p&gt;
&lt;p&gt;在改进网络 I/O 模型前，我先来提一个问题，你知道服务器单机理论最大能连接多少个客户端？&lt;/p&gt;
&lt;p&gt;相信你知道 TCP 连接是由四元组唯一确认的，这个四元组就是：&lt;strong&gt;本机IP, 本机端口, 对端IP, 对端端口&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;服务器作为服务方，通常会在本地固定监听一个端口，等待客户端的连接。因此服务器的本地 IP 和端口是固定的，于是对于服务端 TCP 连接的四元组只有对端 IP 和端口是会变化的，所以&lt;strong&gt;最大 TCP 连接数 = 客户端 IP 数×客户端端口数&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;对于 IPv4，客户端的 IP 数最多为 2 的 32 次方，客户端的端口数最多为 2 的 16 次方，也就是&lt;strong&gt;服务端单机最大 TCP 连接数约为 2 的 48 次方&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这个理论值相当“丰满”，但是服务器肯定承载不了那么大的连接数，主要会受两个方面的限制：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;文件描述符&lt;/strong&gt;，Socket 实际上是一个文件，也就会对应一个文件描述符。在 Linux 下，单个进程打开的文件描述符数是有限制的，没有经过修改的值一般都是 1024，不过我们可以通过 ulimit 增大文件描述符的数目；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;系统内存&lt;/strong&gt;，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占用一定内存的；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那如果服务器的内存只有 2 GB，网卡是千兆的，能支持并发 1 万请求吗？&lt;/p&gt;
&lt;p&gt;并发 1 万请求，也就是经典的 C10K 问题 ，C 是 Client 单词首字母缩写，C10K 就是单机同时处理 1 万个请求的问题。&lt;/p&gt;
&lt;p&gt;从硬件资源角度看，对于 2GB 内存千兆网卡的服务器，如果每个请求处理占用不到 200KB 的内存和 100Kbit 的网络带宽就可以满足并发 1 万个请求。&lt;/p&gt;
&lt;p&gt;不过，要想真正实现 C10K 的服务器，要考虑的地方在于服务器的网络 I/O 模型，效率低的模型，会加重系统开销，从而会离 C10K 的目标越来越远。&lt;/p&gt;
&lt;h2 id=&#34;-3&#34;&gt;&lt;a href=&#34;#%E5%A4%9A%E8%BF%9B%E7%A8%8B%E6%A8%A1%E5%9E%8B&#34;&gt;#&lt;/a&gt; 多进程模型&lt;/h2&gt;
&lt;p&gt;基于最原始的阻塞网络 I/O， 如果服务器要支持多个客户端，其中比较传统的方式，就是使用&lt;strong&gt;多进程模型&lt;/strong&gt;，也就是为每个客户端分配一个进程来处理请求。&lt;/p&gt;
&lt;p&gt;服务器的主进程负责监听客户的连接，一旦与客户端连接完成，accept() 函数就会返回一个「已连接 Socket」，这时就通过 &lt;code&gt;fork()&lt;/code&gt; 函数创建一个子进程，实际上就把父进程所有相关的东西都&lt;strong&gt;复制&lt;/strong&gt;一份，包括文件描述符、内存地址空间、程序计数器、执行的代码等。&lt;/p&gt;
&lt;p&gt;这两个进程刚复制完的时候，几乎一模一样。不过，会根据&lt;strong&gt;返回值&lt;/strong&gt;来区分是父进程还是子进程，如果返回值是 0，则是子进程；如果返回值是其他的整数，就是父进程。&lt;/p&gt;
&lt;p&gt;正因为子进程会&lt;strong&gt;复制父进程的文件描述符&lt;/strong&gt;，于是就可以直接使用「已连接 Socket 」和客户端通信了，&lt;/p&gt;
&lt;p&gt;可以发现，子进程不需要关心「监听 Socket」，只需要关心「已连接 Socket」；父进程则相反，将客户服务交给子进程来处理，因此父进程不需要关心「已连接 Socket」，只需要关心「监听 Socket」。&lt;/p&gt;
&lt;p&gt;下面这张图描述了从连接请求到连接建立，父进程创建生子进程为客户服务。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;4&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/cSOvCwFk7nEz4a1.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;另外，当「子进程」退出时，实际上内核里还会保留该进程的一些信息，也是会占用内存的，如果不做好“回收”工作，就会变成&lt;strong&gt;僵尸进程&lt;/strong&gt;，随着僵尸进程越多，会慢慢耗尽我们的系统资源。&lt;/p&gt;
&lt;p&gt;因此，父进程要“善后”好自己的孩子，怎么善后呢？那么有两种方式可以在子进程退出后回收资源，分别是调用 &lt;code&gt;wait()&lt;/code&gt; 和 &lt;code&gt;waitpid()&lt;/code&gt; 函数。&lt;/p&gt;
&lt;p&gt;这种用多个进程来应付多个客户端的方式，在应对 100 个客户端还是可行的，但是当客户端数量高达一万时，肯定扛不住的，因为每产生一个进程，必会占据一定的系统资源，而且进程间上下文切换的“包袱”是很重的，性能会大打折扣。&lt;/p&gt;
&lt;p&gt;进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。&lt;/p&gt;
&lt;h2 id=&#34;-4&#34;&gt;&lt;a href=&#34;#%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B&#34;&gt;#&lt;/a&gt; 多线程模型&lt;/h2&gt;
&lt;p&gt;既然进程间上下文切换的“包袱”很重，那我们就搞个比较轻量级的模型来应对多用户的请求 —— &lt;strong&gt;多线程模型&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;线程是运行在进程中的一个“逻辑流”，单进程中可以运行多个线程，同进程里的线程可以共享进程的部分资源，比如文件描述符列表、进程空间、代码、全局数据、堆、共享库等，这些共享些资源在上下文切换时不需要切换，而只需要切换线程的私有数据、寄存器等不共享的数据，因此同一个进程下的线程上下文切换的开销要比进程小得多。&lt;/p&gt;
&lt;p&gt;当服务器与客户端 TCP 完成连接后，通过 &lt;code&gt;pthread_create()&lt;/code&gt; 函数创建线程，然后将「已连接 Socket」的文件描述符传递给线程函数，接着在线程里和客户端进行通信，从而达到并发处理的目的。&lt;/p&gt;
&lt;p&gt;如果每来一个连接就创建一个线程，线程运行完后，还得操作系统还得销毁线程，虽说线程切换的上写文开销不大，但是如果频繁创建和销毁线程，系统开销也是不小的。&lt;/p&gt;
&lt;p&gt;那么，我们可以使用&lt;strong&gt;线程池&lt;/strong&gt;的方式来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若干个线程，这样当由新连接建立时，将这个已连接的 Socket 放入到一个队列里，然后线程池里的线程负责从队列中取出「已连接 Socket 」进行处理。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;5&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/qnYQkuFoRlWpvyX.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁。&lt;/p&gt;
&lt;p&gt;上面基于进程或者线程模型的，其实还是有问题的。新到来一个 TCP 连接，就需要分配一个进程或者线程，那么如果要达到 C10K，意味着要一台机器维护 1 万个连接，相当于要维护 1 万个进程/线程，操作系统就算死扛也是扛不住的。&lt;/p&gt;
&lt;h2 id=&#34;-5&#34;&gt;&lt;a href=&#34;#i-o-%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8&#34;&gt;#&lt;/a&gt; I/O 多路复用&lt;/h2&gt;
&lt;p&gt;既然为每个请求分配一个进程/线程的方式不合适，那有没有可能只使用一个进程来维护多个 Socket 呢？答案是有的，那就是 &lt;strong&gt;I/O 多路复用&lt;/strong&gt;技术。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;6&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/nCGoMmliT9rSjy7.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这就是多路复用，这种思想很类似一个 CPU 并发多个进程，所以也叫做时分多路复用。&lt;/p&gt;
&lt;p&gt;我们熟悉的 select/poll/epoll 内核提供给用户态的多路复用系统调用，&lt;strong&gt;进程可以通过一个系统调用函数从内核中获取多个事件&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;select/poll/epoll 是如何获取网络事件的呢？在获取事件时，先把所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，然后在用户态中再处理这些连接对应的请求即可。&lt;/p&gt;
&lt;p&gt;select/poll/epoll 这是三个多路复用接口，都能实现 C10K 吗？接下来，我们分别说说它们。&lt;/p&gt;
&lt;h2 id=&#34;-6&#34;&gt;&lt;a href=&#34;#select-poll&#34;&gt;#&lt;/a&gt; select/poll&lt;/h2&gt;
&lt;p&gt;select 实现多路复用的方式是，将已连接的 Socket 都放到一个&lt;strong&gt;文件描述符集合&lt;/strong&gt;，然后调用 select 函数将文件描述符集合&lt;strong&gt;拷贝&lt;/strong&gt;到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过&lt;strong&gt;遍历&lt;/strong&gt;文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合&lt;strong&gt;拷贝&lt;/strong&gt;回用户态里，然后用户态还需要再通过&lt;strong&gt;遍历&lt;/strong&gt;的方法找到可读或可写的 Socket，然后再对其处理。&lt;/p&gt;
&lt;p&gt;所以，对于 select 这种方式，需要进行 &lt;strong&gt;2 次「遍历」文件描述符集合&lt;/strong&gt;，一次是在内核态里，一个次是在用户态里 ，而且还会发生 &lt;strong&gt;2 次「拷贝」文件描述符集合&lt;/strong&gt;，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。&lt;/p&gt;
&lt;p&gt;select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 &lt;code&gt;1024&lt;/code&gt;，只能监听 0~1023 的文件描述符。&lt;/p&gt;
&lt;p&gt;poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。&lt;/p&gt;
&lt;p&gt;但是 poll 和 select 并没有太大的本质区别，&lt;strong&gt;都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合&lt;/strong&gt;，这种方式随着并发数上来，性能的损耗会呈指数级增长。&lt;/p&gt;
&lt;h2 id=&#34;-7&#34;&gt;&lt;a href=&#34;#epoll&#34;&gt;#&lt;/a&gt; epoll&lt;/h2&gt;
&lt;p&gt;先复习下 epoll 的用法。如下的代码中，先用e poll_create 创建一个 epol l对象 epfd，再通过 epoll_ctl 将需要监视的 socket 添加到epfd中，最后调用 epoll_wait 等待数据。&lt;/p&gt;
&lt;p&gt;epoll 通过两个方面，很好解决了 select/poll 的问题。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;第一点&lt;/em&gt;，epoll 在内核里使用&lt;strong&gt;红黑树来跟踪进程所有待检测的文件描述字&lt;/strong&gt;，把需要监控的 socket 通过 &lt;code&gt;epoll_ctl()&lt;/code&gt; 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删改一般时间复杂度是 &lt;code&gt;O(logn)&lt;/code&gt;。而 select/poll 内核里没有类似 epoll 红黑树这种保存所有待检测的 socket 的数据结构，所以 select/poll 每次操作时都传入整个 socket 集合给内核，而 epoll 因为在内核维护了红黑树，可以保存所有待检测的 socket ，所以只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;第二点&lt;/em&gt;， epoll 使用&lt;strong&gt;事件驱动&lt;/strong&gt;的机制，内核里&lt;strong&gt;维护了一个链表来记录就绪事件&lt;/strong&gt;，当某个 socket 有事件发生时，通过&lt;strong&gt;回调函数&lt;/strong&gt;内核会将其加入到这个就绪事件列表中，当用户调用 &lt;code&gt;epoll_wait()&lt;/code&gt; 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。&lt;/p&gt;
&lt;p&gt;从下图你可以看到 epoll 相关的接口作用：&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;7&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/NzxaPrEKjvutJsS.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;epoll 的方式即使监听的 Socket 数量越多的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了，上限就为系统定义的进程打开的最大文件描述符个数。因而，&lt;strong&gt;epoll 被称为解决 C10K 问题的利器&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;插个题外话，网上文章不少说，&lt;code&gt;epoll_wait&lt;/code&gt; 返回时，对于就绪的事件，epoll 使用的是共享内存的方式，即用户态和内核态都指向了就绪链表，所以就避免了内存拷贝消耗。&lt;/p&gt;
&lt;p&gt;这是错的！看过 epoll 内核源码的都知道，&lt;strong&gt;压根就没有使用共享内存这个玩意&lt;/strong&gt;。你可以从下面这份代码看到， epoll_wait 实现的内核代码中调用了 &lt;code&gt;__put_user&lt;/code&gt; 函数，这个函数就是将数据从内核拷贝到用户空间。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;8&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/64wT1B9aRiO8gun.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;好了，这个题外话就说到这了，我们继续！&lt;/p&gt;
&lt;h3 id=&#34;-8&#34;&gt;&lt;a href=&#34;#%E8%BE%B9%E7%BC%98%E8%A7%A6%E5%8F%91%E5%92%8C%E6%B0%B4%E5%B9%B3%E8%A7%A6%E5%8F%91&#34;&gt;#&lt;/a&gt; 边缘触发和水平触发&lt;/h3&gt;
&lt;p&gt;epoll 支持两种事件触发模式，分别是&lt;strong&gt;边缘触发（&lt;em&gt;edge-triggered，ET&lt;/em&gt;）&lt;strong&gt;和&lt;/strong&gt;水平触发（&lt;em&gt;level-triggered，LT&lt;/em&gt;）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这两个术语还挺抽象的，其实它们的区别还是很好理解的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，&lt;strong&gt;服务器端只会从 epoll_wait 中苏醒一次&lt;/strong&gt;，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；&lt;/li&gt;
&lt;li&gt;使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，&lt;strong&gt;服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束&lt;/strong&gt;，目的是告诉我们有数据需要读取；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;举个例子，你的快递被放到了一个快递箱里，如果快递箱只会通过短信通知你一次，即使你一直没有去取，它也不会再发送第二条短信提醒你，这个方式就是边缘触发；如果快递箱发现你的快递没有被取出，它就会不停地发短信通知你，直到你取出了快递，它才消停，这个就是水平触发的方式。&lt;/p&gt;
&lt;p&gt;这就是两者的区别，水平触发的意思是只要满足事件的条件，比如内核中有数据需要读，就一直不断地把这个事件传递给用户；而边缘触发的意思是只有第一次满足条件的时候才触发，之后就不会再传递同样的事件了。&lt;/p&gt;
&lt;p&gt;如果使用水平触发模式，当内核通知文件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。所以在收到通知后，没必要一次执行尽可能多的读写操作。&lt;/p&gt;
&lt;p&gt;如果使用边缘触发模式，I/O 事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会&lt;strong&gt;循环&lt;/strong&gt;从文件描述符读写数据，那么如果文件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那里，程序就没办法继续往下执行。所以，&lt;strong&gt;边缘触发模式一般和非阻塞 I/O 搭配使用&lt;/strong&gt;，程序会一直执行 I/O 操作，直到系统调用（如 &lt;code&gt;read&lt;/code&gt; 和 &lt;code&gt;write&lt;/code&gt;）返回错误，错误类型为 &lt;code&gt;EAGAIN&lt;/code&gt; 或 &lt;code&gt;EWOULDBLOCK&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;一般来说，边缘触发的效率比水平触发的效率要高，因为边缘触发可以减少 epoll_wait 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。&lt;/p&gt;
&lt;p&gt;select/poll 只有水平触发模式，epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。&lt;/p&gt;
&lt;p&gt;另外，使用 I/O 多路复用时，最好搭配非阻塞 I/O 一起使用，Linux 手册关于 select 的内容中有如下说明：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Under Linux, select() may report a socket file descriptor as &amp;quot;ready for reading&amp;quot;, while nevertheless a subsequent read blocks. This could for example happen when data has arrived but upon examination has wrong checksum and is discarded. There may be other circumstances in which a file descriptor is spuriously reported as ready. Thus it may be safer to use O_NONBLOCK on sockets that should not block.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我谷歌翻译的结果：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在Linux下，select() 可能会将一个 socket 文件描述符报告为 &amp;quot;准备读取&amp;quot;，而后续的读取块却没有。例如，当数据已经到达，但经检查后发现有错误的校验和而被丢弃时，就会发生这种情况。也有可能在其他情况下，文件描述符被错误地报告为就绪。因此，在不应该阻塞的 socket 上使用 O_NONBLOCK 可能更安全。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;简单点理解，就是&lt;strong&gt;多路复用 API 返回的事件并不一定可读写的&lt;/strong&gt;，如果使用阻塞 I/O， 那么在调用 read/write 时则会发生程序阻塞，因此最好搭配非阻塞 I/O，以便应对极少数的特殊情况。&lt;/p&gt;
&lt;h2 id=&#34;-9&#34;&gt;&lt;a href=&#34;#%E6%80%BB%E7%BB%93&#34;&gt;#&lt;/a&gt; 总结&lt;/h2&gt;
&lt;p&gt;最基础的 TCP 的 Socket 编程，它是阻塞 I/O 模型，基本上只能一对一通信，那为了服务更多的客户端，我们需要改进网络 I/O 模型。&lt;/p&gt;
&lt;p&gt;比较传统的方式是使用多进程/线程模型，每来一个客户端连接，就分配一个进程/线程，然后后续的读写都在对应的进程/线程，这种方式处理 100 个客户端没问题，但是当客户端增大到 10000 个时，10000 个进程/线程的调度、上下文切换以及它们占用的内存，都会成为瓶颈。&lt;/p&gt;
&lt;p&gt;为了解决上面这个问题，就出现了 I/O 的多路复用，可以只在一个进程里处理多个文件的 I/O，Linux 下有三种提供 I/O 多路复用的 API，分别是：select、poll、epoll。&lt;/p&gt;
&lt;p&gt;select 和 poll 并没有本质区别，它们内部都是使用「线性结构」来存储进程关注的 Socket 集合。&lt;/p&gt;
&lt;p&gt;在使用的时候，首先需要把关注的 Socket 集合通过 select/poll 系统调用从用户态拷贝到内核态，然后由内核检测事件，当有网络事件产生时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置其状态为可读/可写，然后把整个 Socket 集合从内核态拷贝到用户态，用户态还要继续遍历整个 Socket 集合找到可读/可写的 Socket，然后对其处理。&lt;/p&gt;
&lt;p&gt;很明显发现，select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越大，Socket 集合的遍历和拷贝会带来很大的开销，因此也很难应对 C10K。&lt;/p&gt;
&lt;p&gt;epoll 是解决 C10K 问题的利器，通过两个方面解决了 select/poll 的问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;epoll 在内核里使用「红黑树」来关注进程所有待检测的 Socket，红黑树是个高效的数据结构，增删改一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，不需要像 select/poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。&lt;/li&gt;
&lt;li&gt;epoll 使用事件驱动的机制，内核里维护了一个「链表」来记录就绪事件，只将有事件发生的 Socket 集合传递给应用程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和无事件的 Socket ），大大提高了检测的效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而且，epoll 支持边缘触发和水平触发的方式，而 select/poll 只支持水平触发，一般而言，边缘触发的方式会比水平触发的效率高。&lt;/p&gt;
">I/O多路复用</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/fen-bu-shi-xiang-guan-yi-zhi-xing-suan-fa-raft/"" data-c="
          &lt;h3 id=&#34;什么是raft-算法&#34;&gt;什么是Raft 算法&lt;/h3&gt;
&lt;p&gt;首先说什么是 Raft 算法：Raft 是一种为了管理复制日志的一致性算法 。 Raft提供了和Paxos算法相同的功能和性能，但是它的算法结构和Paxos不同。Raft算法更加容易理解并且更容易构建实际的系统。&lt;/p&gt;
&lt;p&gt;Raft将一致性算法分解成了3模块&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;领导人选举&lt;/li&gt;
&lt;li&gt;日志复制&lt;/li&gt;
&lt;li&gt;安全性&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Raft算法分为两个阶段，首先是选举过程，然后在选举出来的领导人带领进行正常操作，比如日志复制等。&lt;/p&gt;
&lt;h3 id=&#34;领导人leader选举&#34;&gt;领导人Leader选举&lt;/h3&gt;
&lt;p&gt;做法：Raft 通过选举一个领导人，然后给予他全部的管理复制日志的责任来实现一致性。 在Raft中，任何时候一个服务器都可以扮演下面的角色之一：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;领导者(leader)：处理客户端交互，日志复制等动作，一般一次只有一个领导者&lt;/li&gt;
&lt;li&gt;候选者(candidate)：候选者就是在选举过程中提名自己的实体，一旦选举成功，则成为领导者&lt;/li&gt;
&lt;li&gt;跟随者(follower)：类似选民，完全被动的角色，这样的服务器等待被通知投票 而影响他们身份变化的则是 选举。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/6J75ZnMQ829xwav.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Raft使用心跳机制来触发选举。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当server启动时，初始状态都是follower。&lt;/li&gt;
&lt;li&gt;每一个server都有一个定时器，超时时间为election timeout（一般为150-300ms）
&lt;ul&gt;
&lt;li&gt;如果在超时时间内,收到来自领导者或者候选者的任何消息，重启定时器&lt;/li&gt;
&lt;li&gt;如果到达了超时时间，还没有收到其他领导发过来的消息，会认为现在就没有领导，它就开始一次选举，就开始向别的服务器发送消息，让他们投自己一票。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://link.juejin.cn/?target=http%3A%2F%2Fthesecretlivesofdata.com%2Fraft%2F&#34; title=&#34;http://thesecretlivesofdata.com/raft/&#34;&gt;thesecretlivesofdata.com/raft/&lt;/a&gt; 动画演示&lt;/p&gt;
&lt;h4 id=&#34;领导者选举过程&#34;&gt;领导者选举过程&lt;/h4&gt;
&lt;p&gt;下面用图示展示这个过程：&lt;/p&gt;
&lt;p&gt;初始状态下集群中的所有节点都处于 follower 状态。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/9tjTdaKVZrPefAo.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;某一时刻，其中的一个 follower 由于没有收到 leader 的 heartbeat 率先发生 election timeout 进而发起选举。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/C7Ka1y9oDnirGNj.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;只要集群中超过半数的节点接受投票，candidate 节点将成为即切换 leader 状态。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/YnjuNV98KofEFhS.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;成为 leader 节点之后，leader 将定时向 follower 节点同步日志并发送 heartbeat。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/UDQAFKdYBVayus6.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;节点异常&#34;&gt;节点异常&lt;/h4&gt;
&lt;p&gt;集群中各个节点的状态随时都有可能发生变化。从实际的变化上来分类的话，节点的异常大致可以分为四种类型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;leader 不可用；&lt;/li&gt;
&lt;li&gt;follower 不可用；&lt;/li&gt;
&lt;li&gt;多个 candidate 或多个 leader；&lt;/li&gt;
&lt;li&gt;新节点加入集群。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;leader-不可用&#34;&gt;leader 不可用&lt;/h5&gt;
&lt;p&gt;下面将说明当集群中的 leader 节点不可用时，raft 集群是如何应对的。&lt;/p&gt;
&lt;p&gt;➢ 一般情况下，leader 节点定时发送 heartbeat 到 follower 节点。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/C1aFDs9NLMSqb4w.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 由于某些异常导致 leader 不再发送 heartbeat ，或 follower 无法收到 heartbeat 。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/Zk7yPr9H615uLSY.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 当某一 follower 发生 election timeout 时，其状态变更为 candidate，并向其他 follower 发起投票。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/jgGQ5bB8r913PIh.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;➢ 当超过半数的 follower 接受投票后，这一节点将成为新的 leader，leader 的&lt;code&gt;步进数加 1&lt;/code&gt; 并开始向 follower 同 步日志。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/as8C3wZqeO5Lm2v.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 当一段时间之后，如果之前的 leader 再次加入集群，&lt;code&gt;则两个 leader 比较彼此的步进数&lt;/code&gt;，步进数低的 leader 将 切换自己的状态为 follower。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/LHeARsK3tGbQYJI.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 较早前 leader 中不一致的日志将被清除，并与现有 leader 中的日志保持一致。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/HutifFCcpaxUoRz.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h5 id=&#34;follower-节点不可用&#34;&gt;follower 节点不可用&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;follower 节点不可用的情况相对容易解决。因为集群中的日志内容始终是从 leader 节点同步的，只要这一节点再 次加入集群时重新从 leader 节点处复制日志即可。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;➢ 集群中的某个 follower 节点发生异常，不再同步日志以及接收 heartbeat &lt;img src=&#34;https://s2.loli.net/2023/08/23/a5tZrF9JoumwjAf.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 经过一段时间之后，原来的 follower 节点重新加入集群。这个时候他很懵逼，究竟发生了什么？我是谁，我在哪里？ &lt;img src=&#34;https://s2.loli.net/2023/08/23/Nfm8nStbIWcyopz.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 这一节点的日志将从当时的 leader 处同步。直接认当前的君主为王就行了，别的也不考虑这么多 &lt;img src=&#34;https://s2.loli.net/2023/08/23/2LPyqjEiMAZpCSH.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h5 id=&#34;多个-candidate-或多个-leader&#34;&gt;多个 candidate 或多个 leader&lt;/h5&gt;
&lt;p&gt;在集群中出现多个 candidate 或多个 leader 通常是由于数据传输不畅造成的。出现多个 leader 的情况相对少见， 但多个 candidate 比较容易出现在集群节点启动初期尚未选出 leader 的“混沌”时期。&lt;/p&gt;
&lt;p&gt;➢ 初始状态下集群中的所有节点都处于 follower 状态。 【刀耕火种，安居乐业】 &lt;img src=&#34;https://s2.loli.net/2023/08/23/Zr5MDzwARqhWspO.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 两个节点同时成为 candidate 发起选举。【东汉末年，群雄割据】 &lt;img src=&#34;https://s2.loli.net/2023/08/23/gmBjN6PY8Aa7CH4.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 两个 candidate 都只得到了少部分 follower 的接受投票。【势单力薄，一主一仆走天下】 &lt;img src=&#34;https://s2.loli.net/2023/08/23/tsfSFObzB48wRmg.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ candidate 继续向其他的 follower 询问。【天下兴亡，匹夫有责】 &lt;img src=&#34;https://s2.loli.net/2023/08/23/EfJuiQUjYkZB95F.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 由于一些 follower 已经投过票了，所以均返回拒绝接受。【吾意已决，夫子不必多言】 &lt;img src=&#34;https://s2.loli.net/2023/08/23/L4S2cy8fMoIvheZ.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ candidate 也可能向一个 candidate 询问投票。【曹操，刘备酒席相见】 &lt;img src=&#34;https://s2.loli.net/2023/08/23/NjLVu1Fz9mWZSXR.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 在步进数相同的情况下，candidate 将拒绝接受另一个 candidate 的请求。【宁为玉碎，不为瓦全】 &lt;img src=&#34;https://s2.loli.net/2023/08/23/2fKq8Y1XkGaSmDn.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 由于第一次未选出 leader，candidate 将随机选择一个等待间隔（150ms ~ 300ms）再次发起投票。【厚积薄发，东山再起】 &lt;img src=&#34;https://s2.loli.net/2023/08/23/tm3p4DvQBNi6CFf.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 如果得到集群中半数以上的 follower 的接受，这一 candidate 将成为 leader。【近水楼台先得月，向阳花木易为春】 &lt;img src=&#34;https://s2.loli.net/2023/08/23/pLPvDo41f75sNli.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 稍后另一个 candidate 也将再次发起投票。【天下兴亡，匹夫有责】 &lt;img src=&#34;https://s2.loli.net/2023/08/23/l9OuftEAzWkqi6R.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 由于集群中已经选出 leader，candidate 将收到拒绝接受的投票。【吾意已决，夫子不必多言】 &lt;img src=&#34;https://s2.loli.net/2023/08/23/XVq2Pumd5aQnNSc.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;➢ 在被多数节点拒绝之后，并已知集群中已存在 leader 后，这一 candidate 节点将终止投票请求、切换为 follower，从 leader 节点同步日志。【命里有时终须有，命里无时莫强求】 &lt;img src=&#34;https://s2.loli.net/2023/08/23/vuaVlHhwQsIkqB7.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;日志复制保证数据一致性&#34;&gt;日志复制（保证数据一致性）&lt;/h3&gt;
&lt;p&gt;日志复制的过程&lt;/p&gt;
&lt;p&gt;Leader选出后，就开始接收客户端的请求。Leader把请求作为日志条目（Log entries）加入到它的日志中， 然后并行的向其他服务器发起 AppendEntries RPC复制日志条目。当这条日志被复制到大多数服务器上，Leader 将这条日志应用到它的状态机并向客户端返回执行结果。&lt;/p&gt;
&lt;p&gt;下图表示了当一个客户端发送一个请求给领导者，随后领导者复制给跟随者的整个过程。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/P5YaNtVWIfpQ7Gn.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;客户端的每一个请求都包含被复制状态机执行的指令。&lt;/li&gt;
&lt;li&gt;leader把这个指令作为一条新的日志条目添加到日志中，然后并行发起 RPC 给其他的服务器，让他们复制这 条信息。&lt;/li&gt;
&lt;li&gt;跟随者响应ACK,如果 follower 宕机或者运行缓慢或者丢包，leader会不断的重试，直到所有的 follower 最终 都复制了所有的日志条目。&lt;/li&gt;
&lt;li&gt;通知所有的Follower提交日志，同时领导人提交这条日志到自己的状态机中，并返回给客户端。 可以看到&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;直到第四步骤，整个事务才会达成。中间任何一个步骤发生故障，都不会影响日志一致性&lt;/p&gt;
">分布式相关--raft算法</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/fen-bu-shi-1/"" data-c="
          &lt;h2 id=&#34;分布式架构系统回顾&#34;&gt;分布式架构系统回顾&lt;/h2&gt;
&lt;h3 id=&#34;1分布式系统概念&#34;&gt;1）分布式系统概念&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调 的系统。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所谓分布式系统，就是一个业务拆分成多个子业务，分布在不同的服务器节点，共同构成的系统称为分 布式系统，同一个分布式系统中的服务器节点在空间部署上是可以随意分布的，这些服务器可能放在不同的机柜 中，也可能在不同的机房中，甚至分布在不同的城市。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/wmOuvXM9kyLaUsR.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;分布式与集群的区别：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-null&#34;&gt;集群：多个人在一起作同样的事 。
分布式 ：多个人在一起作不同的事 。

&lt;/code&gt;&lt;/pre&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/yQMOK82u31CDsEY.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;分布式系统的特点：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-null&#34;&gt;（1）分布性
（2）对等性
（3）并发性
（4）缺乏全局时钟
（5）故障总是会发生

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2分布式系统的发展&#34;&gt;2）分布式系统的发展&lt;/h3&gt;
&lt;p&gt;阿里巴巴发起的&amp;quot;去 IOE&amp;quot;运动 (IOE 指的是 IBM 小型机、Oracle 数据库、EMC 的高端存储)。阿里巴巴2009 年“去 IOE”战略技术总监透露，截止到 2013 年 5 月 17 日阿里巴巴最后一台 IBM 小型机在支付宝下线。&lt;/p&gt;
&lt;p&gt;为什么要去IOE?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;升级单机处理能力的性价比越来越低&lt;/li&gt;
&lt;li&gt;单机处理能力存在瓶颈&lt;/li&gt;
&lt;li&gt;稳定性和可用性这两个指标很难达到&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;3分布式架构的演变&#34;&gt;3）分布式架构的演变&lt;/h3&gt;
&lt;p&gt;阶段一：单应用架构 &lt;img src=&#34;https://s2.loli.net/2023/08/23/sgb8SVzn1E7FOeA.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;阶段二：应用服务器与数据库服务器分离 &lt;img src=&#34;https://s2.loli.net/2023/08/23/GY2sqvOD6jHL3ib.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;阶段三：应用服务器集群 &lt;img src=&#34;https://s2.loli.net/2023/08/23/C2Mk3dc1nr9AgOV.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;阶段四：应用服务器负载均衡 &lt;img src=&#34;https://s2.loli.net/2023/08/23/hbe3jPidc18TpKO.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;阶段五：数据库读写分离 &lt;img src=&#34;https://s2.loli.net/2023/08/23/vR1L2CEwY5inQ7M.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;阶段六：添加搜索引擎缓解读库的压力 &lt;img src=&#34;https://s2.loli.net/2023/08/23/wIVDdMj4l5ovrs1.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;阶段七：添加缓存机制缓解数据库的压力 &lt;img src=&#34;https://s2.loli.net/2023/08/23/2bKWuxHzT3cgp4O.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;阶段八：数据库水平/垂直拆分 &lt;img src=&#34;https://s2.loli.net/2023/08/23/Y8Owy2tJvsUgNPT.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;阶段九：应用拆分 &lt;img src=&#34;https://s2.loli.net/2023/08/23/u8MUZQrR3WHdJPA.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;阶段十：服务化 &lt;img src=&#34;https://s2.loli.net/2023/08/23/KnTFLfMSOmw1uWs.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;分布式系统面临的问题&#34;&gt;分布式系统面临的问题&lt;/h2&gt;
&lt;h3 id=&#34;1通信异常&#34;&gt;1）通信异常&lt;/h3&gt;
&lt;p&gt;网络本身的不可靠性，因此每次网络通信都会伴随着网络不可用的风险（光纤、路由、DNS等硬件设备或系统的不 可用），都会导致最终分布式系统无法顺利进行一次网络通信，另外，即使分布式系统各节点之间的网络通信能够 正常执行，其延时也会大于单机操作，存在巨大的延时差别，也会影响消息的收发过程，因此消息丢失和消息延迟 变的非常普遍。&lt;/p&gt;
&lt;h3 id=&#34;2网络分区&#34;&gt;2）网络分区&lt;/h3&gt;
&lt;p&gt;网络之间出现了网络不连通，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个 孤立的区域，分布式系统就会出现局部小集群，在极端情况下，这些小集群会独立完成原本需要整个分布式系统才 能完成的功能，包括数据的事务处理，这就对分布式一致性提出非常大的挑战。&lt;/p&gt;
&lt;h3 id=&#34;3节点故障&#34;&gt;3）节点故障&lt;/h3&gt;
&lt;p&gt;节点故障是分布式系统下另一个比较常见的问题，指的是组成分布式系统的服务器节点出现的宕机或&amp;quot;僵死&amp;quot;现象， 根据经验来说，每个节点都有可能出现故障，并且经常发生.&lt;/p&gt;
&lt;h3 id=&#34;4三态&#34;&gt;4）三态&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;分布式系统每一次请求与响应存在特有的“三态”概念，即成功、失败和超时。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;分布式系统中，由于网络是不可靠的，虽然绝大部分情况下，网络通信能够接收到成功或失败的响应，但当网络出 现异常的情况下，就会出现超时现象，通常有以下两种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;由于网络原因，该请求并没有被成功的发送到接收方，而是在发送过程就发生了丢失现象。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;该请求成功的被接收方接收后，并进行了处理，但在响应反馈给发送方过程中，发生了消息丢失现象。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;分布式理论一致性&#34;&gt;分布式理论：一致性&lt;/h2&gt;
&lt;h3 id=&#34;什么是分布式一致性&#34;&gt;什么是分布式一致性&lt;/h3&gt;
&lt;p&gt;分布式数据一致性，指的是数据在多份副本中存储时，各副本中的数据是一致的。&lt;/p&gt;
&lt;h3 id=&#34;副本一致性&#34;&gt;副本一致性&lt;/h3&gt;
&lt;p&gt;分布式系统当中，数据往往会有多个&lt;strong&gt;副本&lt;/strong&gt;。如果是一台数据库处理所有的数据请求，那么通过&lt;code&gt;ACID四原则&lt;/code&gt;，基本 可以保证数据的一致性。而&lt;code&gt;多个副本&lt;/code&gt;就需要保证数据会有&lt;code&gt;多份拷贝&lt;/code&gt;。这就带来了同步的问题，因为我们几乎没有办 法保证可以&lt;code&gt;同时更新所有机器&lt;/code&gt;当中的包括备份所有数据。 &lt;code&gt;网络延迟&lt;/code&gt;，即使我在同一时间给所有机器发送了更新数据 的请求，也不能保证这些请求被响应的时间保持一致&lt;code&gt;存在时间差&lt;/code&gt;，就会存在某些机器之间的&lt;code&gt;数据不一致&lt;/code&gt;的情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/bFZGnmj3756sBrl.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
总得来说，我们无法找到一种能够满足分布式系统所有系统属性的分布式一致性解决方案。因此，如何既保证数据 的一致性，同时又不影响系统运行的性能，是每一个分布式系统都需要重点考虑和权衡的。于是，一致性级别由此 诞生：&lt;/p&gt;
&lt;h3 id=&#34;一致性分类&#34;&gt;一致性分类&lt;/h3&gt;
&lt;h4 id=&#34;强一致性&#34;&gt;强一致性&lt;/h4&gt;
&lt;p&gt;这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往 对系统的性能影响大。但是强一致性很难实现。&lt;/p&gt;
&lt;h4 id=&#34;弱一致性&#34;&gt;弱一致性&lt;/h4&gt;
&lt;p&gt;这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致， 但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;读写一致性&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-makefile&#34;&gt;用户读取自己写入结果的一致性，保证用户永远能够第一时间看到自己更新的内容。 比如我们发一条朋友圈，朋友圈的内容是不是第一时间被朋友看见不重要，但是一定要显示在自己的列表上.
解决方案: 
方案1：一种方案是对于一些特定的内容我们每次都去主库读取。 （问题主库压力大） 

方案2：我们设置一个更新时间窗口，在刚刚更新的一段时间内，我们默认都从主库读取，过了这个窗口之后，我们会挑 选最近有过更新的从库进行读取 

方案3：我们直接记录用户更新的时间戳，在请求的时候把这个时间戳带上，凡是最后更新时间小于这个时间戳的从库都 不予以响应。

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;单调读一致性&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;本次读到的数据不能比上次读到的旧。 由于主从节点更新数据的时间不一致，导致用户在不停地刷新的时候，有时候能刷出来，再次刷新之后会发现数据不见 了，再刷新又可能再刷出来，就好像遇见灵异事件一样 

解决方案:
就是根据用户ID计算一个hash值，再通过hash值映射到机器。同一个用户不管怎么刷新，都只会被映射到同 一台机器上。这样就保证了不会读到其他从库的内容，带来用户体验不好的影响。

&lt;/code&gt;&lt;/pre&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/cSoa721k3pIdgeD.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;因果一致性&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;指的是：如果节点 A 在更新完某个数据后通知了节点 B，那么节点 B 之后对该数据的访问和修改都是基于 A 更新后 的值。于此同时，和节点 A 无因果关系的节点 C 的数据访问则没有这样的限制。

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;最终一致性&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-null&#34;&gt;最终一致性是所有分布式一致性模型当中最弱的。可以认为是没有任何优化的“最”弱一致性，它的意思是说，我不考虑 所有的中间状态的影响，只保证当没有新的更新之后，经过一段时间之后，最终系统内所有副本的数据是正确的。 它最大程度上保证了系统的并发能力，也因此，在高并发的场景下，它也是使用最广的一致性模型

&lt;/code&gt;&lt;/pre&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;3&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/qwlDtXmV2d1nxIQ.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h2 id=&#34;分布式理论cap定理&#34;&gt;分布式理论：CAP定理&lt;/h2&gt;
&lt;p&gt;CAP 定理&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2000 年7月的时候，加州大学伯克利分校的Eric Brewer 教授提出了 CAP 猜想，2年后，被 来自于麻省理工 的Seth Gilbert 和 Nancy Lynch 从理论上证明了猜想的可能性，从此，CAP 定理正式在学术上成为了分布式 计算领域的公认定理。并深深的影响了分布式计算的发展。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;CAP 理论含义是，一个分布式系统不可能同时满足一致性（C:Consistency)，可用性（A: Availability）和分区容错 性（P：Partition tolerance）这三个基本需求，最多只能同时满足其中的2个。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;选项&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;C 一致性&lt;/td&gt;
&lt;td&gt;分布式系统当中的一致性指的是所有节点的数据一致，或者说是所有副本的数据一致&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;A 可用性&lt;/td&gt;
&lt;td&gt;Reads and writes always succeed. 也就是说系统一直可用，而且服务一直保持正常&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;P 分区容错性&lt;/td&gt;
&lt;td&gt;系统在遇到一些节点或者网络分区故障的时候，仍然能够提供满足一致性和可用性的服务&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;4&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/Cwxi3u6BZrN8DmX.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;C - Consistency 一致性是值写操作后读操作可以读到最新的数据状态,当数据分布在多个节点上时,从任意节点读取到的数据都是最 新的. 商品信息读写要满足一致性需要实现如下目标: 1.商品服务写入主数据库成功, 则想从数据库查询数据也成功 2.商品服务写入主数据库失败,则向从数据库查询也失败 如何实现一致性? 1.写入主数据库后要数据同步到从数据库 2.写入主数据库后,在向从数据库同步期间要将从数据库锁定, 等待同步完成后在释放锁,以免在写新数据后,向从数据 库查询到旧的数据. 分布式一致性的特点: 1.由于存在数据库同步过程,写操作的响应会有一定的延迟 2.为了保定数据的一致性,对资源暂时锁定,待数据同步完成后释放锁定资源 3.如果请求数据同步失败的节点则会返回错误信息, 一定不会返回旧数据&lt;/p&gt;
&lt;p&gt;A - Availability 可用性是指任何操作都可以得到响应的结果,且不会出现响应超时或响应错误。 商品信息读写要满足可用性需要实现如下目标: 1.从数据库接收到数据库查询的请求则立即能够响应数据查询结果 2.从数据库不允许出现响应超时或错误 如何实现可用性? 1.写入主数据库后要将数据同步到从数据 2.由于要保证数据库的可用性,不可以将数据库中资源锁定 3.即使数据还没有同步过来,从数据库也要返回查询数据, 哪怕是旧数据,但不能返回错误和超时.&lt;/p&gt;
&lt;p&gt;P - Partition tolerance 分布式系统的各个节点部署在不同的子网中, 不可避免的会出现由于网络问题导致节点之间通信失败,此时仍可以对 外提供服务, 这个就是分区容错性 (分区容忍性). 商品信息读写要满足分区容错性需要实现如下目标: 1.主数据库想从数据库同步数据失败不形象写操作 2.其中一个节点挂掉不会影响另一个节点对外提供服务 如何实现分区容错性? 1.尽量使用异步取代同步操作,举例 使用异步方式将数据从主数据库同步到从数据库, 这样节点之间能有效的实现松 耦合; 2.添加数据库节点,其中一个从节点挂掉,由其他从节点提供服务&lt;/p&gt;
&lt;p&gt;CAP只能3选2 &lt;img src=&#34;https://s2.loli.net/2023/08/23/ewJiFKWf8oIbhgT.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;5&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/oW1fGtKn9hmOB8g.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;关于CAP这三个特性我们就介绍完了，接下来我们试着证明一下&lt;strong&gt;为什么CAP不能同时满足。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;假设有一个系统如下：&lt;/strong&gt;  &lt;img src=&#34;https://s2.loli.net/2023/08/23/BCuAtohMG86knWi.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;有用户向N1发送了请求更改了数据，将数据库从V0更新成了V1。由于网络断开，所以N2数据库依然是V0，如果这个时候 有一个请求发给了N2，但是N2并没有办法可以直接给出最新的结果V1，这个时候该怎么办呢？

这个时候无法两种方法，一种是将错就错，将错误的V0数据返回给用户。第二种是阻塞等待，等待网络通信恢复，N2中 的数据更新之后再返回给用户。显然前者牺牲了一致性，后者牺牲了可用性。

这个例子虽然简单，但是说明的内容却很重要。在分布式系统当中，CAP三个特性我们是无法同时满足的，必然要舍弃一 个。三者舍弃一个，显然排列组合一共有三种可能。

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1. 舍弃A(可用性)，保留CP(一致性和分区容错性)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-null&#34;&gt;一个系统保证了一致性和分区容错性，舍弃可用性。也就是说在极端情况下，允许出现系统无法访问的情况出现，这个 时候往往会牺牲用户体验，让用户保持等待，一直到系统数据一致了之后，再恢复服务。

比如12306的时候，就让你重试就好了

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. 舍弃C(一致性)，保留AP(可用性和分区容错性)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-null&#34;&gt;这种是大部分的分布式系统的设计，保证高可用和分区容错，但是会牺牲一致性。

比如：更新个人状态时，并不是马上所有人都知道，而是等待一定时间，所有人才能收到

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. 舍弃P(分区容错性)，保留CA(一致性和可用性)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;如果要舍弃P，那么就是要舍弃分布式系统，CAP也就无从谈起了。可以说P是分布式系统的前提，所以这种情况是不存在 的。

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;分布式理论base-理论&#34;&gt;分布式理论：BASE 理论&lt;/h2&gt;
&lt;p&gt;什么是BASE理论&lt;/p&gt;
&lt;p&gt;BASE：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basically Available(基本可用)&lt;/li&gt;
&lt;li&gt;Soft state（软状态）&lt;/li&gt;
&lt;li&gt;Eventually consistent（最终一致性）三个 短语的缩写，来自 ebay 的架构师提出。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BASE是对CAP中一致性和可用性权衡的结果，BASE理论的核心思想是&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;1-basically-available基本可用&#34;&gt;1-Basically Available(基本可用)&lt;/h3&gt;
&lt;p&gt;基本可用是指分布式系统在出现不可预知故障的时候，&lt;code&gt;允许损失部分可用性&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;但请注意，这&lt;code&gt;绝不等价于系统不可用&lt;/code&gt;。以下就是两个&amp;quot;基本可用&amp;quot;的例子&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;响应时间上的损失：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;功能上的损失：正常情况下，在一个电子商务网站（比如淘宝）上购物，消费者几乎能够顺利地完成每一笔订单。但在一些节日大促购物高峰的时候（比如双十一、双十二），由于消费者的购物行为激增，为了保护 系统的稳定性（或者保证一致性），部分消费者可能会被引导到一个降级页面，如下 &lt;img src=&#34;https://s2.loli.net/2023/08/23/1PJnx7Yzm3TbyNq.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-soft-state软状态&#34;&gt;2-Soft state（软状态）&lt;/h3&gt;
&lt;p&gt;什么是软状态呢？相对于一致性，要求多个节点的数据副本都是一致的，这是一种 “硬状态”。&lt;/p&gt;
&lt;p&gt;软状态指的是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;允许系统中的数据&lt;code&gt;存在中间状态&lt;/code&gt;，并认为该状态&lt;code&gt;不影响系统的整体可用性&lt;/code&gt;，即允许系统在多个不同节点的数据副本之间进行数据同步的过程中存在延迟。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;3-eventually-consistent最终一致性&#34;&gt;3-Eventually consistent（最终一致性）&lt;/h3&gt;
&lt;p&gt;最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，&lt;code&gt;最终能够达到一个一致的状态&lt;/code&gt;。因此最终一致性的本质是需要系统保证最终数据能够达到一致，而&lt;code&gt;不需要实时保证&lt;/code&gt;系统数据的强一致性。&lt;/p&gt;
&lt;h2 id=&#34;分布式事务&#34;&gt;分布式事务&lt;/h2&gt;
&lt;h3 id=&#34;数据库事务回顾&#34;&gt;数据库事务回顾&lt;/h3&gt;
&lt;p&gt;事务的基本特性：&lt;/p&gt;
&lt;p&gt;我们知道&lt;code&gt;事务有4个非常重要的特性&lt;/code&gt;，即我们常说的（ACID）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Atomicity（原子性）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;是说事务是一个不可分割的整体，所有操作要么全做，要么全不做；只要事务中有一个操作出错，回滚到事务开始前的状态的话，那么之前已经执行的所有操作都是无效的，都应该回滚到开始前的状态。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consistency（一致性）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;是说事务执行前后，数据从一个状态到另一个状态必须是一致的，比如A向B转账（A、 B的总金额就是一个一致性状态），不可能出现A扣了钱，B却没收到的情况发生。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Isolation（隔离性）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;多个并发事务之间相互隔离，不能互相干扰。关于事务的隔离性，可能不是特别好理解，这里的并发事务是指两个事务操作了同一份数据的情况；而对于并发事务操作同一份数据的隔离性问题，则是要求不能出现脏读、幻读的情况，即事务A不能读取事务B还没有提交的数据，或者在事务A读取数据进行更新操作时，不允许事务B率先更新掉这条数据。而为了解决这个问题，常用的手段就是加锁了，对于数据库来说就是通过数据库的相关锁机制来保证。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Durablity（持久性）&lt;/strong&gt; 事务完成后，对数据库的更改是永久保存的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;什么是分布式事务&#34;&gt;什么是分布式事务&lt;/h3&gt;
&lt;p&gt;其实分布式事务从实质上看与数据库事务的概念是一致的，既然是事务也就需要满足事务的基本特性（ACID），只是分布式事务相对于本地事务而言其表现形式有很大的不同&lt;/p&gt;
&lt;h2 id=&#34;分布式理论一致性协议-2pc&#34;&gt;分布式理论：一致性协议 2PC&lt;/h2&gt;
&lt;h3 id=&#34;什么是-2pc&#34;&gt;什么是 2PC&lt;/h3&gt;
&lt;p&gt;2PC （ Two-Phase Commit缩写）即两阶段提交协议，是将整个事务流程分为两个阶段，准备阶段（Prepare phase）、提交阶段（commit phase），2是指两个阶段，P是指准备阶段，C是指提交阶段。&lt;/p&gt;
&lt;p&gt;在计算机中部分关系数据库如Oracle、MySQL支持两阶段提交协议.&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;6&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/JUun6roDj93FKIE.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;两个阶段过程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;准备阶段（Prepare phase）：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;事务管理器给每个参与者发送Prepare消息&lt;/li&gt;
&lt;li&gt;每个数据库参与者在本地执行事务，并写本地的Undo/Redo日志&lt;/li&gt;
&lt;li&gt;此时事务没有提交。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;Undo日志是记录修改前的数据，用于数据库回滚，Redo日志是记录修改后的数据，用于提交事务后写入数据文件&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;提交阶段（commit phase）&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;如果事务管理器收到了参与者的执行失败或者超时消息时，直接给每个参与者发送回滚(Rollback)消息；&lt;/li&gt;
&lt;li&gt;否则，发送提交(Commit)消息；参与者根据事务管理器的指令执行提交或者回滚操作，并释放事务处理过程中使用的锁资源。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;注意:必须在最后阶段释放锁资源。&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;协议说明： 顾名思义，二阶段提交就是将事务的提交过程分成了两个阶段来进行处理。流程如下：&lt;/p&gt;
&lt;h3 id=&#34;2pc执行流程&#34;&gt;2PC执行流程&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;成功执行事务事务提交流程&lt;/strong&gt; &lt;img src=&#34;https://s2.loli.net/2023/08/23/m6QrRBgs7pbf2yE.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-makefile&#34;&gt;阶段一: 
1. 事务询问
	协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应。 
2. 执行事务 (写本地的Undo/Redo日志) 
3. 各参与者向协调者反馈事务询问的响应 

总结: 各个参与者进行投票是否让事务进行

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tip: 什么是Ack&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-null&#34;&gt;ACK 确认字符，在数据通信中，接收站发给发送站的一种传输类控制字符。表示发来的数据已确认接收无误。

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;阶段二: 
1. 发送提交请求： 协调者向所有参与者发出 commit 请求。 
2. 事务提交： 参与者收到 commit 请求后，会正式执行事务提交操作，并在完成提交之后释放整个事务执行期间占用的事务资 源。
3. 反馈事务提交结果： 参与者在完成事务提交之后，向协调者发送 Ack 信息。 
4. 完成事务： 协调者接收到所有参与者反馈的 Ack 信息后，完成事务

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;中断事务步骤如下：&lt;/strong&gt;  假如任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响 应，那么就会中断事务 &lt;img src=&#34;https://s2.loli.net/2023/08/23/lRde25ZfcEwmI3L.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;阶段一:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;1. 事务询问 协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应。
2. 执行事务 (写本地的Undo/Redo日志) 
3. 各参与者向协调者反馈事务询问的响应 总结: 各个参与者进行投票是否让事务进行.

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;阶段二:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;1. 发送回滚请求： 协调者向所有参与者发出 Rollback 请求。 
2. 事务回滚： 参与者接收到 Rollback 请求后，会利用其在阶段一中记录的 Undo 信息来执行事务回滚操作，并在完成回滚之后 释放在整个事务执行期间占用的资源。 
3. 反馈事务回滚结果： 参与者在完成事务回滚之后，向协调者发送 Ack 信息。 
4. 中断事务： 协调者接收到所有参与者反馈的 Ack 信息后，完成事务中断。 

从上面的逻辑可以看出，二阶段提交就做了2个事情：投票，执行。

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2pc-优点缺点&#34;&gt;2PC 优点缺点&lt;/h3&gt;
&lt;h4 id=&#34;优点&#34;&gt;优点&lt;/h4&gt;
&lt;p&gt;原理简单，实现方便&lt;/p&gt;
&lt;h4 id=&#34;缺点&#34;&gt;缺点&lt;/h4&gt;
&lt;p&gt;同步阻塞，单点问题，数据不一致，过于保守&lt;/p&gt;
&lt;h5 id=&#34;同步阻塞&#34;&gt;同步阻塞：&lt;/h5&gt;
&lt;p&gt;二阶段提交协议存在最明显也是最大的一个问题就是&lt;code&gt;同步阻塞&lt;/code&gt;，在二阶段提交的执行过程中，&lt;code&gt;所有参与该事务操作的逻辑都处于阻塞状态&lt;/code&gt;，也就是说，各个参与者在等待其他参与者响应的过程中，&lt;code&gt;无法进行其他操作&lt;/code&gt;。这种同步阻 塞极大的限制了分布式系统的性能。&lt;/p&gt;
&lt;h5 id=&#34;单点问题&#34;&gt;单点问题：&lt;/h5&gt;
&lt;p&gt;协调者在整个二阶段提交过程中很重要，如果协调者在&lt;code&gt;提交阶段出现问题&lt;/code&gt;，那么整个流程将&lt;code&gt;无法运转&lt;/code&gt;，更重要的是：其他参与者将会处于&lt;code&gt;一直锁定事务资源&lt;/code&gt;的状态中，而无法继续完成事务操作。&lt;/p&gt;
&lt;h5 id=&#34;数据不一致&#34;&gt;数据不一致：&lt;/h5&gt;
&lt;p&gt;假设当协调者向所有的参与者发送 commit 请求之后，发生了&lt;code&gt;局部网络异常&lt;/code&gt;或者是协调者在尚未发送完所有commit 请求之前&lt;code&gt;自身发生了崩溃&lt;/code&gt;，导致最终只有&lt;code&gt;部分参与者&lt;/code&gt;收到了 commit 请求。这将导致严重的数据不一致问题。&lt;/p&gt;
&lt;h5 id=&#34;过于保守&#34;&gt;过于保守：&lt;/h5&gt;
&lt;p&gt;如果在二阶段提交的提交询问阶段中，参与者出现故障而导致协调者始终无法获取到所有参与者的响应信息的话，这时协调者只能依靠&lt;code&gt;其自身的超时机制&lt;/code&gt;来判断是否需要中断事务，显然，这种策略过于保守。换句话说，二阶段提 交协议&lt;code&gt;没有设计较为完善的容错机制&lt;/code&gt;，任意一个节点失败都会导致&lt;code&gt;整个事务的失败。&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;分布式理论一致性协议-3pc&#34;&gt;分布式理论：一致性协议 3PC&lt;/h2&gt;
&lt;h3 id=&#34;什么是三阶段提交&#34;&gt;什么是三阶段提交&lt;/h3&gt;
&lt;p&gt;3PC，全称 “three phase commit”，是 2PC 的改进版，将 2PC 的 “提交事务请求” 过程一分为二，共形成了由CanCommit、PreCommit和doCommit三个阶段组成的事务处理协议。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;7&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/wHhKUzgjbARvlt6.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h3 id=&#34;阶段一cancommit&#34;&gt;阶段一：CanCommit&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1-事务询问&lt;/p&gt;
&lt;p&gt;协调者向&lt;code&gt;所有的参与者&lt;/code&gt;发送一个包含事务内容的canCommit请求，询问是否可以执行事务提交操作，并开始&lt;code&gt;等待各参与者的响应&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2-各参与者向协调者反馈事务询问的响应&lt;/p&gt;
&lt;p&gt;参与者在接收到来自协调者的包含了事务内容的&lt;code&gt;canCommit请求&lt;/code&gt;后，正常情况下，如果自身认为可以顺利执行事务，则&lt;code&gt;反馈Yes响应&lt;/code&gt;，并进入预备状态，否则&lt;code&gt;反馈No响应&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;8&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/b6tW9DvFaIfuXdJ.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h3 id=&#34;阶段二precommit&#34;&gt;阶段二：PreCommit&lt;/h3&gt;
&lt;p&gt;协调者在得到所有参与者的响应之后，会根据结果有2种执行操作的情况：执行事务预提交，或者中断事务&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;假如所有参与反馈的都是Yes，那么就会执行事务预提交。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;**(1)执行事务预提交分为 3 个步骤 **&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;1.  发送预提交请求：
协调者向所有参与者节点发出preCommit请求，并进入prepared阶段。

2. 事务预提交：
参与者接收到preCommit请求后，会执行事务操作，并将Undo和Redo信息记录到事务日志中
 
3. 各参与者向协调者反馈事务执行的结果:
若参与者成功执行了事务操作，那么反馈Ack

&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;若任一参与者反馈了No响应，或者在等待超时后，协调者尚无法接收到所有参与者反馈，则中断事务&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;(2)中断事务也分为2个步骤：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;1. 发送中断请求：
协调者向所有参与者发出abort请求。

2. 中断事务：
无论是收到来自协调者的abort请求或者等待协调者请求过程中超时，参与者都会中断事务

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;阶段三do-commit&#34;&gt;阶段三：do Commit&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/xpqec7PZV9OlkF3.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
该阶段做真正的事务提交或者完成事务回滚，所以就会出现两种情况：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;执行事务提交&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-null&#34;&gt;1- 发送提交请求：
进入这一阶段，假设协调者处于正常工作状态，并且它接收到了来自所有参与者的Ack响应，那么他将从预提交状态转化为提交状态，并向所有的参与者发送doCommit请求。

2- 事务提交：
参与者接收到doCommit请求后，会正式执行事务提交操作，并在完成提交之后释放整个事务执行过程中占用的事务资源。

3- 反馈事务提交结果：
参与者在完成事务提交后，向协调者发送Ack响应。

4- 完成事务：
协调者接收到所有参与者反馈的Ack消息后，完成事务。

&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;中断事务&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;1-发送中断请求：协调者向所有的参与者节点发送abort请求。

2-事务回滚：参与者收到abort请求后，会根据记录的Undo信息来执行事务回滚，并在完成回滚之后释放整个事务执行期间占用的资源。

3-反馈事务回滚结果：参与者在完成事务回滚后，向协调者发送Ack消息。

4-中断事务：协调者接收到所有参与者反馈的Ack消息后，中断事务。

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意：一旦进入阶段三，可能会出现 2 种故障：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;协调者出现问题&lt;/li&gt;
&lt;li&gt;协调者和参与者之间的网络故障&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;如果出现了任一一种情况，最终都会导致参与者无法收到 doCommit 请求或者 abort 请求，针对这种情况，参与者都会在等待超时之后，继续进行事务提交&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;2pc对比3pc&#34;&gt;2PC对比3PC&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;首先对于协调者和参与者&lt;code&gt;都设置了超时机制&lt;/code&gt;（在2PC中，只有协调者拥有超时机制，即如果在一定时间内没有收到参与者的消息则默认失败）,主要是避免了参与者在长时间无法与协调者节点通讯（协调者挂掉了）的情况下，无法释放资源的问题，&lt;code&gt;因为参与者自身拥有超时机制会在超时后，自动进行本地commit从而进行释放资源&lt;/code&gt;。而这种机制也侧面降低了整个事务的阻塞时间和范围。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过CanCommit、PreCommit、DoCommit三个阶段的设计，相较于2PC而言，&lt;code&gt;多设置了一个缓冲阶段&lt;/code&gt;保证了在最后提交阶段之前各参与节点的状态是一致的 。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PreCommit是一个缓冲，保证了在最后提交阶段之前各参与节点的状态是一致的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;问题：3PC协议并没有完全解决数据不一致问题。&lt;/p&gt;
&lt;h2 id=&#34;分布式理论一致性算法-paxos&#34;&gt;分布式理论：一致性算法 Paxos&lt;/h2&gt;
&lt;h3 id=&#34;paxos-解决了什么问题&#34;&gt;Paxos 解决了什么问题？&lt;/h3&gt;
&lt;p&gt;在常见的分布式系统中，总会发生诸如机器宕机或者网络异常等情况。Paxos算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;快速且正确的在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;举个例子：&lt;/p&gt;
&lt;p&gt;分布式系统才用多副本进行存储数据 , 如果对多个副本执行序列不控制, 那多个副本执行更新操作,由于网络延迟 超时 等故障到值各个副本的数据不一致. 我们希望每个副本的执行序列是&lt;code&gt;[ op1 op2 op3 .... opn ]&lt;/code&gt;不变的, 相同的. Paxos 一次来确定不可变变量 opi的取值 , 每次确定完Opi之后,各个副本执行opi操作,一次类推。&lt;/p&gt;
&lt;h3 id=&#34;问题引入一切从一件小事开始&#34;&gt;问题引入，一切从一件小事开始&lt;/h3&gt;
&lt;p&gt;在一个集群环境中，要求所有机器上的状态是一致的，其中有2台机器想修改某个状态，&lt;code&gt;机器A 想把状态改为 A，机器 B 想把状态改为 B&lt;/code&gt;，那么到底听谁的呢？&lt;/p&gt;
&lt;p&gt;这不很简单嘛？不就像 2PC，3PC 一样引入一个协调者，谁先到，听谁的 &lt;img src=&#34;https://s2.loli.net/2023/08/23/LgjoRef3NE1HWVM.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;那么要是协调者蹦了呢？ &lt;img src=&#34;https://s2.loli.net/2023/08/23/pm1k2fltOnGejXS.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;所以需要对协调者也做备份，也要做集群。这时候，问题来了，这么多协调者，听谁的呢？ &lt;img src=&#34;https://s2.loli.net/2023/08/23/AmtclHoeE87DguK.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;想要解决这个问题，我们就需要用到了paxos算法&lt;/p&gt;
&lt;h3 id=&#34;paxos相关概念&#34;&gt;Paxos相关概念&lt;/h3&gt;
&lt;h4 id=&#34;什么是paxos算法&#34;&gt;什么是Paxos算法&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/rhoKMptw1U5sAV9.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
Paxos由Lamport于1998年在《The Part-Time Parliament》论文中首次公开，最初的描述使用希腊的一个小岛 Paxos作为比喻，描述了Paxos小岛中通过决议的流程，并以此命名这个算法，但是这个没几个人能理解，并且他拒绝使用数学证明他的算法。后来微软的Butlet Lampson提出重新省视这篇论文。后来在2001年，Lamport也做出了让步，简单版本 Paxos Made Simple，但是还是没有用算法来证明他的算法&lt;/p&gt;
&lt;h4 id=&#34;基本概念-提案proposal&#34;&gt;基本概念-提案（Proposal）&lt;/h4&gt;
&lt;p&gt;最终要达成一致的value就在提案里&lt;/p&gt;
&lt;p&gt;Proposal信息包括提案编号 (Proposal ID) 和提议的值 (Value)&lt;/p&gt;
&lt;h4 id=&#34;基本概念-4角色&#34;&gt;基本概念-4角色&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Client&lt;/code&gt;：客户端&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;客户端向分布式系统发出请求，并等待响应。例如，对分布式文件服务器中文件的写请求。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Proposer&lt;/code&gt;：提案发起者&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提案者提倡客户请求，试图说服Acceptor对此达成一致，并在发生冲突时充当协调者以推动协议向前发展&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Acceptor&lt;/code&gt;：决策者，可以批准提案&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Acceptor可以接受（accept）提案；如果某个提案被选定（chosen），那么该提案里的value就被选定了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Learners&lt;/code&gt;：最终决策的学习者&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;学习者充当该协议的复制因素&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里需要说明的是，Proposer，Acceptor，Learners 会存在多份实例，一个进程可能充当不只一种角色&lt;/p&gt;
&lt;p&gt;他们之间协作的流程是： Proposer提出提案，Accepter接收建议，然后Accepter之间 选定出一个最终提案Proposal &lt;img src=&#34;https://s2.loli.net/2023/08/23/9RjdDlqX7GvJCEt.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;问题描述&#34;&gt;问题描述&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;假设有一组可以提出提案的进程集合，那么对于一个一致性算法需要保证以下几点：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;在这些被提出的提案中，只有一个会被选定&lt;/li&gt;
&lt;li&gt;如果没有提案被提出，就不应该有被选定的提案。&lt;/li&gt;
&lt;li&gt;当一个提案被选定后，那么所有进程都应该能学习（learn）到这个被选定的value&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;推导过程&#34;&gt;推导过程&lt;/h3&gt;
&lt;h4 id=&#34;最简单的方案只有一个acceptor&#34;&gt;最简单的方案——只有一个Acceptor&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/kg39OhvLIRJry2t.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
假设只有一个Acceptor（可以有多个Proposer），&lt;code&gt;只要Acceptor接受它收到的第一个提案，则该提案被选定&lt;/code&gt;，该 提案里的value就是被选定的value。这样就保证只有一个value会被选定。 但是，如果这个唯一的Acceptor&lt;code&gt;宕机&lt;/code&gt;了，那么整个系统就无法工作了！ 因此，必须要有多个Acceptor！&lt;/p&gt;
&lt;h4 id=&#34;多个proposer和多个acceptor&#34;&gt;多个Proposer和多个Acceptor&lt;/h4&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;9&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/cWje9PXyaku6F3Q.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;我们如何保证在多个Proposer和Acceptor的情况下，选定一个值？ 首先我们希望即使只有一个Proposer提出了一个value，该value也最终被选定。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;P1：一个Acceptor必须接受它收到的第一个提案 【An acceptor must accept the first proposal that it receives.】&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;但是，这又会引出另一个问题：如果每个Proposer分别提出不同的value，发给不同的Acceptor。根据P1， Acceptor分别接受自己收到的第一个提案，就导致不同的value被选定。出现了不一致。如下图： &lt;img src=&#34;https://s2.loli.net/2023/08/23/o1U87YBNOVfqcJA.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
因为刚刚的规则，导致了这个不一致的情况，所以我们要加入一条规定&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;规定：一个提案被选定需要被半数以上的Acceptor接受&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这个规定又暗示了：『一个Acceptor必须能够接受不止一个提案！』不然可能导致最终没有value被选定。比如上 图的情况。v1、v2、v3都没有被选定，因为它们都只被一个Acceptor的接受。&lt;/p&gt;
&lt;p&gt;所以在这种情况下，我们使用一个&lt;code&gt;全局的编号&lt;/code&gt;来标识每一个Acceptor批准的提案，当一个具有某value值的提案被 &lt;code&gt;半数以上&lt;/code&gt;的Acceptor批准后，我们就认为该value被选定了.&lt;/p&gt;
&lt;p&gt;根据上面的内容，我们现在虽然允许多个提案被选定，但必须保证所有被选定的提案都具有相同的value值。否则 又会出现不一致。 于是有了下面的约束：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;P2：如果某个value为v的提案被选定了，那么每个编号更高的被选定提案的value必须也是v。【If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v.】&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一个提案只有被Acceptor接受才可能被选定，因此我们可以把P2约束改写成对Acceptor接受的提案的约束P2a。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;P2a：如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v【If a proposal with value v is chosen, then every higher-numbered proposal accepted by any acceptor has value v.】&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;只要满足了P2a，就能满足P2。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/xoM3w2YHesTNZcn.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;但是，考虑如下的情况：假设总的有5个Acceptor。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Proposer2提出[M1,V1]的提案，&lt;/li&gt;
&lt;li&gt;Acceptor2~5（半数以上）均接受了该提案&lt;/li&gt;
&lt;li&gt;于是对于Acceptor2~5和Proposer2来讲，它们都认为V1被选定。&lt;/li&gt;
&lt;li&gt;Acceptor1刚刚从宕机状态恢复过来（之前Acceptor1没有收到过任何提案）&lt;/li&gt;
&lt;li&gt;此时Proposer1向Acceptor1发送了[M2,V2]的提案（V2≠V1且M2&amp;gt;M1）&lt;/li&gt;
&lt;li&gt;对于Acceptor1来讲，这是它收到的第一个提案。根据P1（一个Acceptor必须接受它收到的第一个提 案。）,Acceptor1必须接受该提案！同时Acceptor1认为V2被选定。这就出现了两个问题：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(1) Acceptor1认为&lt;code&gt;V2被选定&lt;/code&gt;，Acceptor2~5和Proposer2认为&lt;code&gt;V1被选定&lt;/code&gt;。出现了不一致。&lt;/p&gt;
&lt;p&gt;(2) V1被选定了，但是编号更高的被Acceptor1接受的提案[M2,V2]的value为V2，且V2≠V1。这就跟P2a（如果某 个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v）矛盾了。&lt;/p&gt;
&lt;p&gt;所以我们要对P2a约束进行强化！&lt;/p&gt;
&lt;p&gt;P2a是对Acceptor接受的提案约束，但其实提案是Proposer提出来的，所有我们可以对Proposer提出的提案进行 约束。得到P2b：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;P2b：如果某个value为v的提案被选定了，那么之后任何Proposer提出的编号更高的提案的value必须也是v。【If a proposal with value v is chosen, then every higher-numbered proposal issued by any proposer has value v.】&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;由P2b可以推出P2a进而推出P2。&lt;/p&gt;
&lt;p&gt;那么，如何确保在某个value为v的提案被选定后，Proposer提出的编号更高的提案的value都是v呢？&lt;/p&gt;
&lt;p&gt;只要满足P2c即可：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;P2c：对于任意的Mn和Vn,如果提案[Mn,Vn]被提出，那么肯定存在一个由半数以上的Acceptor组成的集合S，满足以下 两个条件中的任意一个：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;要么S中每个Acceptor都没有接受过编号小于Mn的提案。&lt;/li&gt;
&lt;li&gt;要么S中所有Acceptor批准的所有编号小于Mn的提案中，编号最大的那个提案的value值为Vn&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;P2c:For any v and n, if a proposal with value v and number n is issued, then there is a set S consisting of a majority of acceptors such that either (a) no acceptor in S has accepted any proposal numbered less than n, or (b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;从上面的内容，可以看出，从P1到P2c的过程其实是对一系列条件的逐步增强，如果需要证明这些条件可以保证一 致性，那么就可以进行反向推导：P2c =&amp;gt;P2b=&amp;gt;P2a=&amp;gt;P2,然后通过P2和P1来保证一致性&lt;/p&gt;
&lt;h3 id=&#34;proposer生成提案&#34;&gt;Proposer生成提案&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;接下来来学习，在P2c的基础上如何进行提案的生成&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这里有个比较重要的思想：Proposer生成提案之前，应该先去&lt;code&gt;『学习』&lt;/code&gt;已经被选定或者可能被选定的value，然后 以该value作为自己提出的提案的value。如果没有value被选定，Proposer才可以自己决定value的值。这样才能达 成一致。这个学习的阶段是通过一个&lt;code&gt;『Prepare请求』&lt;/code&gt;实现的。&lt;/p&gt;
&lt;p&gt;于是我们得到了如下的提案生成算法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Proposer选择一个新的提案编号N，然后向某个Acceptor集合（半数以上）发送请求，要求该集合中的每个 Acceptor做出如下响应（response）&lt;/p&gt;
&lt;p&gt;(a) Acceptor向Proposer承诺保证不再接受任何编号小于N的提案。&lt;/p&gt;
&lt;p&gt;(b) 如果Acceptor已经接受过提案，那么就向Proposer反馈已经接受过的编号小于N的，但为最大编号的提案的值。 我们将该请求称为编号为N的Prepare请求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果Proposer收到了半数以上的Acceptor的响应，那么它就可以生成编号为N，Value为V的提案[N,V]。这里 的V是所有的响应中编号最大的提案的Value。如果所有的响应中都没有提案，那 么此时V就可以由Proposer 自己选择。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;生成提案后，Proposer将该提案发送给半数以上的Acceptor集合，并期望这些Acceptor能接受该提案。我们 称该请求为Accept请求。&lt;/p&gt;
&lt;h3 id=&#34;acceptor接受提案&#34;&gt;Acceptor接受提案&lt;/h3&gt;
&lt;p&gt;刚刚讲解了Paxos算法中Proposer的处理逻辑，怎么去生成的提案，下面来看看Acceptor是如何批准提案的&lt;/p&gt;
&lt;p&gt;根据刚刚的介绍，一个Acceptor可能会受到来自Proposer的两种请求，分别是Prepare请求和Accept请求，对这两 类请求作出响应的条件分别如下&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prepare请求：Acceptor可以在任何时候响应一个Prepare请求&lt;/li&gt;
&lt;li&gt;Accept请求：在不违背Accept现有承诺的前提下，可以任意响应Accept请求 因此，对Acceptor接受提案给出如下约束：&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;P1a：一个Acceptor只要尚未响应过任何编号大于N的Prepare请求，那么他就可以接受这个编号为N的提案。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;算法优化&#34;&gt;算法优化&lt;/h3&gt;
&lt;p&gt;上面的内容中，分别从Proposer和Acceptor对提案的生成和批准两方面来讲解了Paxos算法在提案选定过程中的算 法细节，同时也在提案的编号全局唯一的前提下，获得了一个提案选定算法，接下来我们再对这个初步算法做一个 小优化，尽可能的忽略Prepare请求&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;10&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/Kzxhl6i4oeR5bNc.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;如果Acceptor收到一个编号为N的Prepare请求，在此之前它已经响应过编号大于N的Prepare请求。根据P1a，该 Acceptor不可能接受编号为N的提案。因此，该Acceptor可以忽略编号为N的Prepare请求。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;通过这个优化，每个Acceptor只需要记住它已经批准的提案的最大编号以及它已经做出Prepare请求响应的提案的 最大编号，以便出现故障或节点重启的情况下，也能保证P2c的不变性，而对于Proposer来说，只要它可以保证不 会产生具有相同编号的提案，那么就可以丢弃任意的提案以及它所有的运行时状态信息&lt;/p&gt;
&lt;h3 id=&#34;paxos算法描述&#34;&gt;Paxos算法描述&lt;/h3&gt;
&lt;p&gt;综合前面的讲解，我们来对Paxos算法的提案选定过程进行下总结，那结合Proposer和Acceptor对提案的处理逻 辑，就可以得到类似于两阶段提交的算法执行过程&lt;/p&gt;
&lt;p&gt;Paxos算法分为两个阶段。具体如下： &lt;img src=&#34;https://s2.loli.net/2023/08/23/7Qw4GrHxlN8UuzI.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;阶段一&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(a) Proposer选择一个提案编号&lt;code&gt;N&lt;/code&gt;，然后向半数以上的Acceptor发送编号为&lt;code&gt;N&lt;/code&gt;的Prepare请求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(b) 如果一个Acceptor收到一个&lt;code&gt;编号为N&lt;/code&gt;的Prepare请求，且N大于该Acceptor已经响应过的所有Prepare请求 的编号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响应反馈给Proposer，同时该Acceptor承诺不再接受任何编号小于N的提案。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;阶段二&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(a) 如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针 对[N,V]提案的Accept请求给半数以上的Acceptor。注意：V就是收到的响应中编号最大的提案的value，如果 响应中不包含任何提案，那么V就由Proposer自己决定。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(b) 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor没有对编号大于N的Prepare请求做出过响应，它就接受该提案。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然，实际运行过程中，每一个Proposer都有可能产生多个提案，但只要每个Proposer都遵循如上所述的算法运 行，就一定能够保证算法执行的正确性&lt;/p&gt;
&lt;h3 id=&#34;learner学习被选定的value&#34;&gt;Learner学习被选定的value&lt;/h3&gt;
&lt;p&gt;方案一：&lt;/p&gt;
&lt;p&gt;Learner获取一个已经被选定的提案的前提是，该提案已经被&lt;code&gt;半数以上的Acceptor批准&lt;/code&gt;，因此，最简单的 做法就是一旦Acceptor批准了一个提案，就将该提案发送给所有的Learner 很显然，这种做法虽然可以让Learner尽快地获取被选定的提案，但是却需要让每个Acceptor与所有的Learner逐 个进行一次通信，通信的次数至少为二者个数的乘积&lt;/p&gt;
&lt;p&gt;方案二：&lt;/p&gt;
&lt;p&gt;另一种可行的方案是，我们可以让所有的Acceptor将它们对提案的批准情况，统一发送给一个特定的&lt;code&gt;Learner（称 为主Learner）&lt;/code&gt;, 各个Learner之间可以通过消息通信来互相感知提案的选定情况，基于这样的前提，当主Learner 被通知一个提案已经被选定时，它会负责通知其他的learner 在这种方案中，Acceptor首先会将&lt;code&gt;得到批准的提案发送给主Learner&lt;/code&gt;,再由其&lt;code&gt;同步给其他Learner&lt;/code&gt;.因此较方案一而 言，方案二虽然需要多一个步骤才能将提案通知到所有的learner，但其通信次数却大大减少了，通常只是 Acceptor和Learner的个数总和，但同时，该方案引入了一个新的不稳定因素：&lt;code&gt;主Learner随时可能出现故障&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;方案三：&lt;/p&gt;
&lt;p&gt;在讲解方案二的时候，我们提到，方案二最大的问题在于&lt;code&gt;主Learner存在单点问题&lt;/code&gt;，即主Learner随时可能出现故 障，因此，对方案二进行改进，可以将&lt;code&gt;主Learner的范围扩大&lt;/code&gt;，即Acceptor可以将批准的提案发送给一个&lt;code&gt;特定的 Learner集合&lt;/code&gt;，该集合中每个Learner都可以在一个提案被选定后通知其他的Learner。这个Learner集合中的 Learner个数越多，可靠性就越好，但同时网络通信的复杂度也就越高&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;11&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/UL3SDvAZ12kz9FC.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h3 id=&#34;如何保证paxos算法的活性&#34;&gt;如何保证Paxos算法的活性&lt;/h3&gt;
&lt;p&gt;根据前面的内容讲解，我们已经基本上了解了Paxos算法的核心逻辑，那接下来再来看看Paxos算法在实际过程中 的一些细节 活性：最终一定会发生的事情：最终一定要选定value&lt;/p&gt;
&lt;p&gt;假设存在这样一种极端情况，有两个Proposer依次提出了一系列编号递增的提案，导致最终陷入死循环，没有 value被选定,具体流程如下: &lt;img src=&#34;https://s2.loli.net/2023/08/23/t72rgplJeZybOC4.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/N5hxea9MrtlYKpB.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
下面来详细描述下这个场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提案者1 发出&lt;code&gt;编号为1&lt;/code&gt;的Prepare请求，收到过半请求，完成阶段一流程 --&amp;gt; 【决策者集群保证&lt;code&gt;不再接受编号小于1&lt;/code&gt;的提案】&lt;/li&gt;
&lt;li&gt;提案者2 发出&lt;code&gt;编号为2&lt;/code&gt;的Prepare请求，收到过半请求，完成阶段一流程 --&amp;gt; 【决策者集群保证&lt;code&gt;不再接受编号小于2&lt;/code&gt;的提案】&lt;/li&gt;
&lt;li&gt;提案者1 进入第二阶段的时候【提案为1】，发送的Accept请求被Acceptor忽略&lt;/li&gt;
&lt;li&gt;提案者1 发出&lt;code&gt;编号为3&lt;/code&gt;的Prepare请求，收到过半请求，完成阶段一流程 --&amp;gt; 【决策者集群保证&lt;code&gt;不再接受编号小于3&lt;/code&gt;的提案】&lt;/li&gt;
&lt;li&gt;提案者2 进入第二阶段的时候【提案为2 】，发送的Accept请求被Acceptor忽略&lt;/li&gt;
&lt;li&gt;......进入死循环中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;解决：通过&lt;code&gt;选取主Proposer&lt;/code&gt;，并规定只有&lt;code&gt;主Proposer才能提出议案&lt;/code&gt;。这样一来只要主Proposer和过半的Acceptor 能够正常进行网络通信，那么但凡主Proposer提出一个编号更高的提案，该提案终将会被批准，这样通过选择一个 主Proposer，整套Paxos算法就能够保持活性&lt;/p&gt;
">分布式</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/kafka-diu-xiao-xi-fen-xi/"" data-c="
          &lt;p&gt;大型互联网公司一般都会要求消息传递最大限度的不丢失，比如用户服务给代金券服务发送一个消息，如果消息丢失会造成用户未收到应得的代金券，最终用户会投诉。&lt;/p&gt;
&lt;p&gt;为避免上面类似情况的发生，除了做好补偿措施，更应该在系设计的时候充分考虑各种异常，设计一个稳定、高可用的消息系统。&lt;/p&gt;
&lt;p&gt;看一下维基百科的定义&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Kafka 是分布式发布-订阅消息系统。&lt;/p&gt;
&lt;p&gt;它最初由 LinkedIn 公司开发，之后成为 Apache 项目的一部分。&lt;/p&gt;
&lt;p&gt;Kafka 是一个分布式的，可划分的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;kafka 架构&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Kafka 的整体架构非常简单，是显式分布式架构，主要由 producer、broker（kafka）和 consumer 组成。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/30/J7jTzWwtIQDnH8P.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Producer&lt;/strong&gt;（生产者）可以将数据发布到所选择的 topic（主题）中。生产者负责将记录分配到 topic 的哪一个 partition（分区）中。可以使用循环的方式来简单地实现负载均衡，也可以根据某些语义分区函数(如记录中的 key)来完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Consumer&lt;/strong&gt;（消费者）使用一个 consumer group（消费组）名称来进行标识，发布到 topic 中的每条记录被分配给订阅消费组中的一个消费者实例。消费者实例可以分布在多个进程中或者多个机器上。&lt;/p&gt;
&lt;p&gt;在讨论 kafka 是否丢消息前先来了解一下什么是&lt;strong&gt;消息传递语义&lt;/strong&gt;。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/30/7kVnfScxsuW34ML.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;message delivery semantic 也就是消息传递语义，简单说就是消息传递过程中消息传递的保证性。主要分为三种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;at most once&lt;/strong&gt;：最多一次。消息可能丢失也可能被处理，但最多只会被处理一次。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;at least once&lt;/strong&gt;：至少一次。消息不会丢失，但可能被处理多次。可能重复，不会丢失。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;exactly once&lt;/strong&gt;：精确传递一次。消息被处理且只会被处理一次。不丢失不重复就一次。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;理想情况下肯定是希望系统的消息传递是严格 exactly once，也就是保证不丢失、只会被处理一次，但是很难做到。&lt;/p&gt;
&lt;p&gt;回到主角 Kafka，Kafka 有三次消息传递的过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;生产者发消息给 Kafka Broker。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka Broker 消息同步和持久化&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka Broker 将消息传递给消费者。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这三步中每一步都有可能会丢失消息，下面详细分析为什么会丢消息，如何最大限度避免丢失消息。&lt;/p&gt;
&lt;p&gt;先介绍一下生产者发送消息的一般流程（部分流程与具体配置项强相关，这里先忽略）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;生产者是与 leader 直接交互，所以先从集群获取 topic 对应分区的 leader 元数据；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;获取到 leader 分区元数据后直接将消息发给过去；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka Broker 对应的 leader 分区收到消息后写入文件持久化；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Follower 拉取 Leader 消息与 Leader 的数据保持一致；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Follower 消息拉取完毕需要给 Leader 回复 ACK 确认消息；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka Leader 和 Follower 分区同步完，Leader 分区会给生产者回复 ACK 确认消息。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;3&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/30/pRCUEqMbZnHQLSf.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;生产者采用 push 模式将数据发布到 broker，每条消息追加到分区中，顺序写入磁盘。消息写入 Leader 后，Follower 是主动与 Leader 进行同步。&lt;/p&gt;
&lt;p&gt;Kafka 消息发送有两种方式：同步（sync）和异步（async），默认是同步方式，可通过 producer.type 属性进行配置。&lt;/p&gt;
&lt;p&gt;Kafka 通过配置 request.required.acks 属性来确认消息的生产：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;0 表示不进行消息接收是否成功的确认；不能保证消息是否发送成功，生成环境基本不会用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1 表示当 Leader 接收成功时确认；只要 Leader 存活就可以保证不丢失，保证了吞吐量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;-1 或者 all 表示 Leader 和 Follower 都接收成功时确认；可以最大限度保证消息不丢失，但是吞吐量低。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;kafka producer 的参数 acks 的默认值为 1，所以默认的 producer 级别是 at least once，并不能 exactly once。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;敲黑板了，这里可能会丢消息的！&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;如果 acks 配置为 0，发生网络抖动消息丢了，生产者不校验 ACK 自然就不知道丢了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果 acks 配置为 1 保证 leader 不丢，但是如果 leader 挂了，恰好选了一个没有 ACK 的 follower，那也丢了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;all：保证 leader 和 follower 不丢，但是如果网络拥塞，没有收到 ACK，会有重复发的问题。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kafka Broker 接收到数据后会将数据进行持久化存储，你以为是下面这样的：&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;4&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/30/aEAW3cwULxl8sJz.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;没想到是这样的：&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;5&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/30/6rJgGKv5OBYTNoF.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;操作系统本身有一层缓存，叫做 Page Cache，当往磁盘文件写入的时候，系统会先将数据流写入缓存中，至于什么时候将缓存的数据写入文件中是由操作系统自行决定。&lt;/p&gt;
&lt;p&gt;Kafka 提供了一个参数 producer.type 来控制是不是主动 flush，如果 Kafka 写入到 mmap 之后就立即 flush 然后再返回 Producer 叫同步 (sync)；写入 mmap 之后立即返回 Producer 不调用 flush 叫异步 (async)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;敲黑板了，这里可能会丢消息的！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Kafka 通过多分区多副本机制中已经能最大限度保证数据不会丢失，如果数据已经写入系统 cache 中但是还没来得及刷入磁盘，此时突然机器宕机或者掉电那就丢了，当然这种情况很极端。&lt;/p&gt;
&lt;p&gt;消费者通过 pull 模式主动的去 kafka 集群拉取消息，与 producer 相同的是，消费者在拉取消息的时候也是找 leader 分区去拉取。&lt;/p&gt;
&lt;p&gt;多个消费者可以组成一个消费者组（consumer group），每个消费者组都有一个组 id。同一个消费组者的消费者可以消费同一 topic 下不同分区的数据，但是不会出现多个消费者消费同一分区的数据。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;6&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/30/1YhMagbHZAtDQul.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;消费者消费的进度通过 offset 保存在 kafka 集群的__consumer_offsets 这个 topic 中。&lt;/p&gt;
&lt;p&gt;消费消息的时候主要分为两个阶段：&lt;/p&gt;
&lt;p&gt;1、标识消息已被消费，commit offset 坐标；&lt;/p&gt;
&lt;p&gt;2、处理消息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;敲黑板了，这里可能会丢消息的！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;场景一：先 commit 再处理消息。如果在处理消息的时候异常了，但是 offset 已经提交了，这条消息对于该消费者来说就是丢失了，再也不会消费到了。&lt;/p&gt;
&lt;p&gt;场景二：先处理消息再 commit。如果在 commit 之前发生异常，下次还会消费到该消息，重复消费的问题可以通过业务保证消息幂等性来解决。&lt;/p&gt;
&lt;p&gt;那么问题来了，kafka 到底会不会丢消息？答案是：会！&lt;/p&gt;
&lt;p&gt;Kafka 可能会在三个阶段丢失消息：&lt;/p&gt;
&lt;p&gt;（1）生产者发送数据；&lt;/p&gt;
&lt;p&gt;（2）Kafka Broker 存储数据；&lt;/p&gt;
&lt;p&gt;（3）消费者消费数据；&lt;/p&gt;
&lt;p&gt;在生产环境中严格做到 exactly once 其实是难的，同时也会牺牲效率和吞吐量，最佳实践是业务侧做好补偿机制，万一出现消息丢失可以兜底。&lt;/p&gt;
">kafka丢消息分析</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/kafka/"" data-c="
          &lt;p&gt;名词：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;消息：Record。Kafka 是消息引擎嘛，这里的消息就是指 Kafka 处理的主要对象。&lt;/li&gt;
&lt;li&gt;主题：Topic。主题是承载消息的逻辑容器，在实际使用中多用来区分具体的业务。&lt;/li&gt;
&lt;li&gt;分区：Partition。一个有序不变的消息序列。每个主题下可以有多个分区。&lt;/li&gt;
&lt;li&gt;消息位移：Offset。表示分区中每条消息的位置信息，是一个单调递增且不变的值。&lt;/li&gt;
&lt;li&gt;副本：Replica。Kafka 中同一条消息能够被拷贝到多个地方以提供数据冗余，这些地方就是所谓的副本。副本还分为领导者副本和追随者副本，各自有不同的角色划分。副本是在分区层级下的，即每个分区可配置多个副本实现高可用。&lt;/li&gt;
&lt;li&gt;生产者：Producer。向主题发布新消息的应用程序。&lt;/li&gt;
&lt;li&gt;消费者：Consumer。从主题订阅新消息的应用程序。&lt;/li&gt;
&lt;li&gt;消费者位移：Consumer Offset。表征消费者消费进度，每个消费者都有自己的消费者位移。&lt;/li&gt;
&lt;li&gt;消费者组：Consumer Group。多个消费者实例共同组成的一个组，同时消费多个分区以实现高吞吐。&lt;/li&gt;
&lt;li&gt;重平衡：Rebalance。消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。Rebalance 是 Kafka 消费者端实现高可用的重要手段。&lt;/li&gt;
&lt;/ul&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1669780607083.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;kafka 无消息丢失最佳实践&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。记住，一定要使用带有回调通知的 send 方法。&lt;/li&gt;
&lt;li&gt;设置 acks = all。acks 是 Producer 的一个参数，代表了你对“已提交”消息的定义。如果设置成 all，则表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”。这是最高等级的“已提交”定义。&lt;/li&gt;
&lt;li&gt;设置 retries 为一个较大的值。这里的 retries 同样是 Producer 的参数，对应前面提到的 Producer 自动重试。当出现网络的瞬时抖动时，消息发送可能会失败，此时配置了 retries &amp;gt; 0 的 Producer 能够自动重试消息发送，避免消息丢失。&lt;/li&gt;
&lt;li&gt;设置 unclean.leader.election.enable = false。这是 Broker 端的参数，它控制的是哪些 Broker 有资格竞选分区的 Leader。如果一个 Broker 落后原先的 Leader 太多，那么它一旦成为新的 Leader，必然会造成消息的丢失。故一般都要将该参数设置成 false，即不允许这种情况的发生。&lt;/li&gt;
&lt;li&gt;设置 replication.factor &amp;gt;= 3。这也是 Broker 端的参数。其实这里想表述的是，最好将消息多保存几份，毕竟目前防止消息丢失的主要机制就是冗余。&lt;/li&gt;
&lt;li&gt;设置 min.insync.replicas &amp;gt; 1。这依然是 Broker 端参数，控制的是消息至少要被写入到多少个副本才算是“已提交”。设置成大于 1 可以提升消息持久性。在实际环境中千万不要使用默认值 1。&lt;/li&gt;
&lt;li&gt;确保 replication.factor &amp;gt; min.insync.replicas。如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作了。我们不仅要改善消息的持久性，防止数据丢失，还要在不降低可用性的基础上完成。推荐设置成 replication.factor = min.insync.replicas + 1。&lt;/li&gt;
&lt;li&gt;确保消息消费完成再提交。Consumer 端有个参数 enable.auto.commit，最好把它设置成 false，并采用手动提交位移的方式。就像前面说的，这对于单 Consumer 多线程处理的场景而言是至关重要的。&lt;/li&gt;
&lt;/ul&gt;
">Kafka</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-zong-jie/"" data-c="
          &lt;h3 id=&#34;11-slice&#34;&gt;1.1 Slice&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Slice底层实现原理&lt;/p&gt;
&lt;p&gt;切片是基于数组实现的，它的底层是数组，它自己本身非常小，可以理解为对&lt;strong&gt;底层数组的抽象&lt;/strong&gt;。因为基于数组实现，所以它的底层的&lt;strong&gt;内存是连续分配&lt;/strong&gt;的，效率非常高，还可以通过&lt;strong&gt;索引&lt;/strong&gt;获得数据，可以&lt;strong&gt;迭代以及垃圾回收优化&lt;/strong&gt;。 切片本身并不是动态数组或者数组指针。它内部实现的数据结构通过指针引用底层数组，设定相关属性将数据读写操作限定在指定的区域内。切片本身是一 个只读对象，其工作机制类似数组指针的一种封装。&lt;/p&gt;
&lt;p&gt;切片对象非常小，是因为它是&lt;strong&gt;只有 3 个字段的数据结构&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;指向底层数组的指针&lt;/li&gt;
&lt;li&gt;切片的长度&lt;/li&gt;
&lt;li&gt;切片的容量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Slice扩容机制&lt;/p&gt;
&lt;p&gt;在使用 append 向 slice 追加元素时，若 slice 空间不足则会发生扩容，扩容会重新分配一块更大的内存，将原 slice 拷贝到新 slice ，然后返回新 slice。扩容后再将数据追加进去。&lt;/p&gt;
&lt;p&gt;扩容操作只对容量，扩容后的 slice 长度不变，容量变化规则如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若 slice 容量小于1024个元素，那么扩容的时候slice的cap就翻番，乘以2；一旦元素个数超过1024个元素，增长因子就变成1.25，即每次增加原来容量的四分之一。&lt;/li&gt;
&lt;li&gt;若 slice 容量够用，则将新元素追加进去，slice.len++，返回原 slice&lt;/li&gt;
&lt;li&gt;若 slice 容量不够用，将 slice 先扩容，扩容得到新 slice，将新元素追加进新 slice，slice.len++，返回新 slice。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Slice与数组区别&lt;/p&gt;
&lt;p&gt;array是固定长度的数组，使用前必须确定数组长度，是值类型。&lt;/p&gt;
&lt;p&gt;slice是一个引用类型，是一个动态的指向数组切片的指针。 slice是一个不定长的，总是指向底层的数组array的数据结构，可以动态扩容。&lt;/p&gt;
&lt;p&gt;创建方式不一样，Slice使用make创建或者根据数组创建。&lt;/p&gt;
&lt;p&gt;作为函数参数时，数组传递的是数组的副本，而slice传递的是指针。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;12-map&#34;&gt;1.2 Map&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Map底层实现原理&lt;/p&gt;
&lt;p&gt;Golang 中 map 的底层实现是一个散列表，因此实现 map 的过程实际上就是实现散表的过程。在这个散列表中，主要出现的结构体有两个，一个叫 hmap(a header for a go map)，一个叫 bmap(a bucket for a Go map，通常叫其 bucket)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;hmap 哈希表&lt;/strong&gt; hmap是Go map的底层实现，每个hmap内都含有多个bmap（buckets桶、oldbuckets旧桶、overflow溢出桶），既每个哈希表都由多个桶组成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;buckets buckets是一个指针，指向一个bmap数组，存储多个桶。&lt;/li&gt;
&lt;li&gt;oldbuckets oldbuckets是一个指针，指向一个bmap数组，存储多个旧桶，用于扩容。&lt;/li&gt;
&lt;li&gt;overflow overflow是一个指针，指向一个元素个数为2的数组，数组的类型是一个指针，指向一个slice，slice的元素是桶(bmap)的地址，这些桶都是溢出桶。为什么有两个？因为Go map在哈希冲突过多时，会发生扩容操作。[0]表示当前使用的溢出桶集合，[1]是在发生扩容时，保存了旧的溢出桶集合。overflow存在的意义在于防止溢出桶被gc。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;bmap 哈希桶&lt;/strong&gt; bmap是一个隶属于hmap的结构体，一个桶（bmap）可以存储8个键值对。如果有第9个键值对被分配到该桶，那就需要再创建一个桶，通过overflow指针将两个桶连接起来。在hmap中，多个bmap桶通过overflow指针相连，组成一个链表。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Map进行有序的排序&lt;/p&gt;
&lt;p&gt;map每次遍历,都会从一个随机值序号的桶,再从其中随机的cell开始遍历,并且扩容后,原来桶中的key会落到其他桶中,本身就会造成失序&lt;/p&gt;
&lt;p&gt;如果想顺序遍历map,先把key放到切片排序,再按照key的顺序遍历map。&lt;/p&gt;
&lt;p&gt;或者可以先把map中的key，通过sort包排序，再遍历map。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;map 为什么是不安全的&lt;/p&gt;
&lt;p&gt;Go map 默认是并发不安全的，同时对 map 进行并发读写的时，程序会 panic，原因如下：Go 官方经过长时间的讨论，认为 map 适配的场景应该是简单的（不需要从多个 gorountine 中进行安全访问的），而不是为了小部分情况（并发访问），导致大部分程序付出锁的代价，因此决定了不支持。&lt;/p&gt;
&lt;p&gt;map 在扩缩容时，需要进行数据迁移，迁移的过程并没有采用锁机制防止并发操作，而是会对某个标识位标记为 1，表示此时正在迁移数据。如果有其他 goroutine 对 map 也进行写操作，当它检测到标识位为 1 时，将会直接 panic。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果想实现map线程安全，有两种方式：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;方式一：使用读写锁 &lt;code&gt;map&lt;/code&gt; + &lt;code&gt;sync.RWMutex&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;方式二：使用golang提供的 &lt;code&gt;sync.Map&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Map扩容策略&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;扩容时机：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;向 map 插入新 key 的时候，会进行条件检测，符合下面这 2 个条件，就会触发扩容&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;扩容条件：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;超过负载 map元素个数 &amp;gt; 6.5（负载因子） * 桶个数&lt;/li&gt;
&lt;li&gt;溢出桶太多&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当桶总数&amp;lt;2^15时，如果溢出桶总数&amp;gt;=桶总数，则认为溢出桶过多&lt;/p&gt;
&lt;p&gt;当桶总数&amp;gt;2&lt;sup&gt;15时，如果溢出桶总数&amp;gt;=2&lt;/sup&gt;15，则认为溢出桶过多&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;扩容机制：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;双倍扩容：针对条件1，新建一个buckets数组，新的buckets大小是原来的2倍，然后旧buckets数据搬迁到新的buckets。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;等量扩容：针对条件2，并不扩大容量，buckets数量维持不变，重新做一遍类似双倍扩容的搬迁动作，把松散的键值对重新排列一次，使得同一个 bucket 中的 key 排列地更紧密，节省空间，提高 bucket 利用率，进而保证更快的存取。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;渐进式扩容：&lt;/p&gt;
&lt;p&gt;插入修改删除key的时候，都会尝试进行搬迁桶的工作，每次都会检查oldbucket是否nil，如果不是nil则每次搬迁2个桶，蚂蚁搬家一样渐进式扩容&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Map和Slice区别&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数组：数组是一个由固定长度的特定类型元素组成的序列，一个数组可以由零个或多个元素组成。声明方式：var a [3]int&lt;/li&gt;
&lt;li&gt;slice（切片）：Slice（切片）代表变长的序列，序列中每个元素都有相同的类型，slice的语法和数组很像，只是没有固定长度而已。&lt;/li&gt;
&lt;li&gt;map：在Go语言中，一个map就是一个哈希表的引用，是一个无序的key/value对的集合&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Map总结&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;map是引用类型&lt;/li&gt;
&lt;li&gt;map遍历是无序的&lt;/li&gt;
&lt;li&gt;map是非线程安全的&lt;/li&gt;
&lt;li&gt;map的哈希冲突解决方式是链表法&lt;/li&gt;
&lt;li&gt;map的扩容不是一定会新增空间，也有可能是只是做了内存整理&lt;/li&gt;
&lt;li&gt;map的迁移是逐步进行的，在每次赋值时，会做至少一次迁移工作&lt;/li&gt;
&lt;li&gt;map中删除key，有可能导致出现很多空的kv，这会导致迁移操作，如果可以避免，尽量避免&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;13-channel&#34;&gt;1.3 Channel&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;介绍一下Channel（有缓冲和无缓冲）&lt;/p&gt;
&lt;p&gt;Go 语言中，不要通过共享内存来通信，而要通过通信来实现内存共享。Go 的CSP(Communicating Sequential Process)并发模型，中文可以叫做通信顺序进程，是通过 goroutine 和 channel 来实现的。&lt;/p&gt;
&lt;p&gt;所以 channel 收发遵循先进先出 FIFO，分为有缓存和无缓存，channel 中大致有 buffer(当缓冲区大小部位 0 时，是个 ring buffer)、sendx 和 recvx 收发的位置(ring buffer 记录实现)、sendq、recvq 当前 channel 因为缓冲区不足 而阻塞的队列、使用双向链表存储、还有一个 mutex 锁控制并发、其他原属等。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;
ch := make(chan int)

ch := make(chan int, 2)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;channel 无缓冲时，发送阻塞直到数据被接收，接收阻塞直到读到数据；channel有缓冲时，当缓冲满时发送阻塞，当缓冲空时接收阻塞。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Channel实现原理&lt;/p&gt;
&lt;p&gt;channel 内部维护了两个 goroutine 队列，一个是待发送数据的 goroutine 队列，另一个是待读取数据的 goroutine 队列。&lt;/p&gt;
&lt;p&gt;每当对 channel 的读写操作超过了可缓冲的 goroutine 数量，那么当前的 goroutine 就会被挂到对应的队列上，直到有其他 goroutine 执行了与之相反的读写操作，将它重新唤起。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Channel读写流程&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;向 channel 写数据:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;若等待接收队列 recvq 不为空，则缓冲区中无数据或无缓冲区，将直接从 recvq 取出 G ，并把数据写入，最后把该 G 唤醒，结束发送过程。&lt;/p&gt;
&lt;p&gt;若缓冲区中有空余位置，则将数据写入缓冲区，结束发送过程。&lt;/p&gt;
&lt;p&gt;若缓冲区中没有空余位置，则将发送数据写入 G，将当前 G 加入 sendq ，进入睡眠，等待被读 goroutine 唤醒。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;从 channel 读数据&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;若等待发送队列 sendq 不为空，且没有缓冲区，直接从 sendq 中取出 G ，把 G 中数据读出，最后把 G 唤醒，结束读取过程。&lt;/p&gt;
&lt;p&gt;如果等待发送队列 sendq 不为空，说明缓冲区已满，从缓冲区中首部读出数据，把 G 中数据写入缓冲区尾部，把 G 唤醒，结束读取过程。&lt;/p&gt;
&lt;p&gt;如果缓冲区中有数据，则从缓冲区取出数据，结束读取过程。&lt;/p&gt;
&lt;p&gt;将当前 goroutine 加入 recvq ，进入睡眠，等待被写 goroutine 唤醒。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关闭 channel&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1.关闭 channel 时会将 recvq 中的 G 全部唤醒，本该写入 G 的数据位置为 nil。将 sendq 中的 G 全部唤醒，但是这些 G 会 panic。&lt;/p&gt;
&lt;p&gt;panic 出现的场景还有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;关闭值为 nil 的 channel&lt;/li&gt;
&lt;li&gt;关闭已经关闭的 channel&lt;/li&gt;
&lt;li&gt;向已经关闭的 channel 中写数据&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Channel为什么能做到线程安全&lt;/p&gt;
&lt;p&gt;Channel 可以理解是一个先进先出的队列，通过管道进行通信,发送一个数据到Channel和从Channel接收一个数据都是原子性的。不要通过共享内存来通信，而是通过通信来共享内存，前者就是传统的加锁，后者就是Channel。设计Channel的主要目的就是在多任务间传递数据的，本身就是安全的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Channel是同步进行还是异步的（Channel的三种状态）&lt;/p&gt;
&lt;p&gt;Channel是异步进行的, channel存在3种状态：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nil，未初始化的状态，只进行了声明，或者手动赋值为nil&lt;/li&gt;
&lt;li&gt;active，正常的channel，可读或者可写&lt;/li&gt;
&lt;li&gt;closed，已关闭，千万不要误认为关闭channel后，channel的值是nil&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;操作&lt;/th&gt;
&lt;th&gt;一个零值nil通道&lt;/th&gt;
&lt;th&gt;一个非零值但已关闭的通道&lt;/th&gt;
&lt;th&gt;一个非零值且尚未关闭的通道&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;关闭&lt;/td&gt;
&lt;td&gt;产生恐慌&lt;/td&gt;
&lt;td&gt;产生恐慌&lt;/td&gt;
&lt;td&gt;成功关闭&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;发送数据&lt;/td&gt;
&lt;td&gt;永久阻塞&lt;/td&gt;
&lt;td&gt;产生恐慌&lt;/td&gt;
&lt;td&gt;阻塞或者成功发送&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;接收数据&lt;/td&gt;
&lt;td&gt;永久阻塞&lt;/td&gt;
&lt;td&gt;永不阻塞&lt;/td&gt;
&lt;td&gt;阻塞或者成功接收&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ol&gt;
&lt;li&gt;给一个 nil channel 发送数据，造成永远阻塞&lt;/li&gt;
&lt;li&gt;从一个 nil channel 接收数据，造成永远阻塞&lt;/li&gt;
&lt;li&gt;给一个已经关闭的 channel 发送数据，引起 panic&lt;/li&gt;
&lt;li&gt;从一个已经关闭的 channel 接收数据，如果缓冲区中为空，则返回一个零值&lt;/li&gt;
&lt;li&gt;无缓冲的 channel 是同步的，而有缓冲的 channel 是非同步的&lt;/li&gt;
&lt;li&gt;关闭一个 nil channel 将会发生 panic&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;!https://s2.loli.net/2023/08/23/WwRvIhkeBmuLdgi.webp&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;14-goroutine&#34;&gt;1.4 Goroutine&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;进程、线程和协程的区别&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;进程&lt;/strong&gt;: 进程是具有一定独立功能的程序，进程是系统资源分配和调度的最小单位。每个进程都有自己的独立内存空间，不同进程通过进程间通信来通信。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;线程&lt;/strong&gt;: 线程是进程的一个实体,线程是内核态,而且是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位。线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;协程&lt;/strong&gt;: 协程是一种用户态的轻量级线程，协程的调度完全是由用户来控制的。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。&lt;/li&gt;
&lt;li&gt;线程和协程的区别
&lt;ol&gt;
&lt;li&gt;线程切换需要陷入内核，然后进行上下文切换，而协程在用户态由协程调度器完成，不需要陷入内核，这样代价就小了。&lt;/li&gt;
&lt;li&gt;协程的切换时间点是由调度器决定，而不是由系统内核决定的，尽管它们的切换点都是时间片超过一定阈值，或者是进入I/O或睡眠等状态时。&lt;/li&gt;
&lt;li&gt;基于垃圾回收的考虑，Go实现了垃圾回收，但垃圾回收的必要条件是内存位于一致状态，因此就需要暂停所有的线程。如果交给系统去做，那么会暂停所有的线程使其一致。对于Go语言来说，调度器知道什么时候内存位于一致状态，所以也就没有必要暂停所有运行的线程。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;介绍一下Goroutine&lt;/p&gt;
&lt;p&gt;Goroutine 是一个与其他 goroutines 并行运行在同一地址空间的 Go 函数或方法。&lt;/p&gt;
&lt;p&gt;goroutine的概念类似于线程，但 goroutine是由Go的运行时（runtime）调度和管理的。Go程序会智能地将 goroutine 中的任务合理地分配给每个CPU。它在语言层面已经内置了调度和上下文切换的机制。&lt;/p&gt;
&lt;p&gt;goroutine是Go并发设计的核心，也叫协程，它比线程更加轻量，因此可以同时运行成千上万个并发任务。在Go语言中，每一个并发的执行单元叫作一个goroutine。我们只需要在调用的函数前面添加go关键字，就能使这个函数以协程的方式运行。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;context包结构原理和用途&lt;/p&gt;
&lt;p&gt;Context（上下文）是Golang应用开发常用的并发控制技术 ，它可以控制一组呈树状结构的goroutine，每个goroutine拥有相同的上下文。Context 是并发安全的，主要是用于控制多个协程之间的协作、取消操作。&lt;/p&gt;
&lt;p&gt;Context 只定义了接口，凡是实现该接口的类都可称为是一种 context。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;「Deadline」 方法：可以获取设置的截止时间，返回值 deadline 是截止时间，到了这个时间，Context 会自动发起取消请求，返回值 ok 表示是否设置了截止时间。&lt;/li&gt;
&lt;li&gt;「Done」 方法：返回一个只读的 channel ，类型为 struct{}。如果这个 chan 可以读取，说明已经发出了取消信号，可以做清理操作，然后退出协程，释放资源。&lt;/li&gt;
&lt;li&gt;「Err」 方法：返回Context 被取消的原因。&lt;/li&gt;
&lt;li&gt;「Value」 方法：获取 Context 上绑定的值，是一个键值对，通过 key 来获取对应的值。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;goroutine调度&lt;/p&gt;
&lt;p&gt;GPM是Go语言运行时（runtime）层面的实现，是go语言自己实现的一套调度系统。区别于操作系统调度OS线程。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;G很好理解，就是个goroutine的，里面除了存放本goroutine信息外 还有与所在P的绑定等信息。&lt;/li&gt;
&lt;li&gt;P管理着一组goroutine队列，P里面会存储当前goroutine运行的上下文环境（函数指针，堆栈地址及地址边界），P会对自己管理的goroutine队列做一些调度（比如把占用CPU时间较长的goroutine暂停、运行后续的goroutine等等）当自己的队列消费完了就去全局队列里取，如果全局队列里也消费完了会去其他P的队列里抢任务。&lt;/li&gt;
&lt;li&gt;M（machine）是Go运行时（runtime）对操作系统内核线程的虚拟， M与内核线程一般是一一映射的关系， 一个groutine最终是要放到M上执行的；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;P与M一般也是一一对应的。他们关系是： P管理着一组G挂载在M上运行。当一个G长久阻塞在一个M上时，runtime会新建一个M，阻塞G所在的P会把其他的G 挂载在新建的M上。当旧的G阻塞完成或者认为其已经死掉时 回收旧的M。&lt;/p&gt;
&lt;p&gt;P的个数是通过runtime.GOMAXPROCS设定（最大256），Go1.5版本之后默认为物理线程数。 在并发量大的时候会增加一些P和M，但不会太多，切换太频繁的话得不偿失。&lt;/p&gt;
&lt;p&gt;单从线程调度讲，Go语言相比起其他语言的优势在于OS线程是由OS内核来调度的，goroutine则是由Go运行时（runtime）自己的调度器调度的，这个调度器使用一个称为m:n调度的技术（复用/调度m个goroutine到n个OS线程）。 其一大特点是goroutine的调度是在用户态下完成的， 不涉及内核态与用户态之间的频繁切换，包括内存的分配与释放，都是在用户态维护着一块大的内存池， 不直接调用系统的malloc函数（除非内存池需要改变），成本比调度OS线程低很多。 另一方面充分利用了多核的硬件资源，近似的把若干goroutine均分在物理线程上， 再加上本身goroutine的超轻量，以上种种保证了go调度方面的性能。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如何避免Goroutine泄露和泄露场景&lt;/p&gt;
&lt;p&gt;gorouinte 里有关于 channel 的操作，如果没有正确处理 channel 的读取，会导致 channel 一直阻塞住, goroutine 不能正常结束&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;waitgroup 用法和原理&lt;/p&gt;
&lt;p&gt;waitgroup 内部维护了一个计数器，当调用 &lt;code&gt;wg.Add(1)&lt;/code&gt; 方法时，就会增加对应的数量；当调用 &lt;code&gt;wg.Done()&lt;/code&gt; 时，计数器就会减一。直到计数器的数量减到 0 时，就会调用 runtime_Semrelease 唤起之前因为 &lt;code&gt;wg.Wait()&lt;/code&gt; 而阻塞住的 goroutine。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;使用方法：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;main 协程通过调用 wg.Add(delta int) 设置 worker 协程的个数，然后创建 worker 协程；&lt;/li&gt;
&lt;li&gt;worker 协程执行结束以后，都要调用 wg.Done()；&lt;/li&gt;
&lt;li&gt;main 协程调用 wg.Wait() 且被 block，直到所有 worker 协程全部执行结束后返回。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;实现原理：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;WaitGroup 主要维护了 2 个计数器，一个是请求计数器 v，一个是等待计数器 w，二者组成一个 64bit 的值，请求计数器占高 32bit，等待计数器占低 32bit。&lt;/li&gt;
&lt;li&gt;每次 Add 执行，请求计数器 v 加 1，Done 方法执行，请求计数器减 1，v为0 时通过信号量唤醒 Wait()。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;15-gmp调度&#34;&gt;1.5 GMP调度&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;GMP是什么&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;G（Goroutine）：即Go协程，每个go关键字都会创建一个协程。&lt;/li&gt;
&lt;li&gt;M（Machine）：工作线程，在Go中称为Machine，数量对应真实的CPU数（真正干活的对象）。&lt;/li&gt;
&lt;li&gt;P（Processor）：处理器（Go中定义的一个摡念，非CPU），包含运行Go代码的必要资源，用来调度 G 和 M 之间的关联关系，其数量可通过 GOMAXPROCS() 来设置，默认为核心数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;M必须拥有P才可以执行G中的代码，P含有一个包含多个G的队列，P可以调度G交由M执行。&lt;/p&gt;
&lt;p&gt;优先从 P 的本地队列获取 goroutine 来执行；如果本地队列没有，从全局队列获取，如果全局队列也没有，会从其他的 P 上偷取 goroutine。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GMP goroutine调度策略&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;队列轮转：P 会周期性的将G调度到M中执行，执行一段时间后，保存上下文，将G放到队列尾部，然后从队列中再取出一个G进行调度。除此之外，P还会周期性的查看全局队列是否有G等待调度到M中执行。&lt;/li&gt;
&lt;li&gt;系统调用：当G0即将进入系统调用时，M0将释放P，进而某个空闲的M1获取P，继续执行P队列中剩下的G。M1的来源有可能是M的缓存池，也可能是新建的。&lt;/li&gt;
&lt;li&gt;当G0系统调用结束后，如果有空闲的P，则获取一个P，继续执行G0。如果没有，则将G0放入全局队列，等待被其他的P调度。然后M0将进入缓存池睡眠。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调度器的设计策略&lt;/p&gt;
&lt;p&gt;复用线程：避免频繁的创建、销毁线程，而是对线程的复用。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;work stealing 机制
&lt;ul&gt;
&lt;li&gt;当本线程无可运行的 G 时，尝试从其他线程绑定的 P 偷取 G，而不是销毁线程。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hand off 机制
&lt;ul&gt;
&lt;li&gt;当本线程因为 G 进行系统调用阻塞时，线程释放绑定的 P，把 P 转移给其他空闲的线程执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;利用并行&lt;/strong&gt;：GOMAXPROCS 设置 P 的数量，最多有 GOMAXPROCS 个线程分布在多个 CPU 上同时运行。GOMAXPROCS 也限制了并发的程度，比如 GOMAXPROCS = 核数/2，则最多利用了一半的 CPU 核进行并行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;抢占&lt;/strong&gt;：在 coroutine 中要等待一个协程主动让出 CPU 才执行下一个协程，在 Go 中，一个 goroutine 最多占用 CPU 10ms，防止其他 goroutine 被饿死，这就是 goroutine 不同于 coroutine 的一个地方。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;全局 G 队列&lt;/strong&gt;：，当 M 执行 work stealing 从其他 P 偷不到 G 时，它可以从全局 G 队列获取 G。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CSP 模型是“以通信的方式来共享内存”，不同于传统的多线程通过共享内存来通信。用于描述两个独立的并发实体通过共享的通讯 channel (管道)进行通信的并发模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;两种抢占式调度&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;协作式的抢占式调度&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在 1.14 版本之前，程序只能依靠 Goroutine 主动让出 CPU 资源才能触发调度，存在问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;某些 Goroutine 可以长时间占用线程，造成其它 Goroutine 的饥饿&lt;/li&gt;
&lt;li&gt;垃圾回收需要暂停整个程序（Stop-the-world，STW），最长可能需要几分钟的时间，导致整个程序无法工作。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;基于信号的抢占式调度&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在任何情况下，Go 运行时并行执行（注意，不是并发）的 goroutines 数量是小于等于 P 的数量的。为了提高系统的性能，P 的数量肯定不是越小越好，所以官方默认值就是 CPU 的核心数，设置的过小的话，如果一个持有 P 的 M， 由于 P 当前执行的 G 调用了 syscall 而导致 M 被阻塞，那么此时关键点：GO 的调度器是迟钝的，它很可能什么都没做，直到 M 阻塞了相当长时间以后，才会发现有一个 P/M 被 syscall 阻塞了。然后，才会用空闲的 M 来强这个 P。通过 sysmon 监控实现的抢占式调度，最快在 20us，最慢在 10-20ms 才 会发现有一个 M 持有 P 并阻塞了。操作系统在 1ms 内可以完成很多次线程调度（一般情况 1ms 可以完成几十次线程调度），Go 发起 IO/syscall 的时候执行该 G 的 M 会阻塞然后被 OS 调度走，P 什么也不干，sysmon 最慢要 10-20ms才能发现这个阻塞，说不定那时候阻塞已经结束了，宝贵的 P 资源就这么被阻塞的 M 浪费。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GMP 调度过程中存在哪些阻塞&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I/O，select&lt;/li&gt;
&lt;li&gt;block on syscall&lt;/li&gt;
&lt;li&gt;channel&lt;/li&gt;
&lt;li&gt;等待锁&lt;/li&gt;
&lt;li&gt;runtime.Gosched()&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GMP 调度流程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个 P 有个局部队列，局部队列保存待执行的 goroutine(流程 2)，当 M 绑 定的 P 的的局部队列已经满了之后就会把 goroutine 放到全局队列(流程 2- 1)&lt;/li&gt;
&lt;li&gt;每个 P 和一个 M 绑定，M 是真正的执行 P 中 goroutine 的实体(流程 3)，M 从绑定的 P 中的局部队列获取 G 来执行&lt;/li&gt;
&lt;li&gt;当 M 绑定的 P 的局部队列为空时，M 会从全局队列获取到本地队列来执行G(流程 3.1)，当从全局队列中没有获取到可执行的 G 时候，M 会从其他 P 的局部队列中偷取 G 来执行(流程 3.2)，这种从其他 P 偷的方式称为 work stealing&lt;/li&gt;
&lt;li&gt;当 G 因系统调用(syscall)阻塞时会阻塞 M，此时 P 会和 M 解绑即 hand off，并寻找新的 idle 的 M，若没有 idle 的 M 就会新建一个 M(流程 5.1)。&lt;/li&gt;
&lt;li&gt;当 G 因 channel 或者 network I/O 阻塞时，不会阻塞 M，M 会寻找其他 runnable 的 G；当阻塞的 G 恢复后会重新进入 runnable 进入 P 队列等待执 行(流程 5.3)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;16-垃圾回收机制&#34;&gt;1.6 垃圾回收机制&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;GC 原理&lt;/p&gt;
&lt;p&gt;垃圾回收就是对程序中不再使用的内存资源进行自动回收的操作。&lt;/p&gt;
&lt;p&gt;三色标记法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始状态下所有对象都是白色的。&lt;/li&gt;
&lt;li&gt;从根节点开始遍历所有对象，把遍历到的对象变成灰色对象&lt;/li&gt;
&lt;li&gt;遍历灰色对象，将灰色对象引用的对象也变成灰色对象，然后将遍历过的灰色对象变成黑色对象。&lt;/li&gt;
&lt;li&gt;循环步骤3，直到灰色对象全部变黑色。&lt;/li&gt;
&lt;li&gt;通过写屏障(write-barrier)检测对象有变化，重复以上操作&lt;/li&gt;
&lt;li&gt;收集所有白色对象（垃圾）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;STW（Stop The World）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为了避免在 GC 的过程中，对象之间的引用关系发生新的变更，使得GC的结果发生错误（如GC过程中新增了一个引用，但是由于未扫描到该引用导致将被引用的对象清除了），停止所有正在运行的协程。&lt;/li&gt;
&lt;li&gt;STW对性能有一些影响，Golang目前已经可以做到1ms以下的STW。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GC 的触发条件&lt;/p&gt;
&lt;p&gt;主动触发(手动触发)，通过调用 runtime.GC 来触发GC，此调用阻塞式地等待当前GC运行完毕。 被动触发，分为两种方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用步调（Pacing）算法，其核心思想是控制内存增长的比例,每次内存分配时检查当前内存分配量是否已达到阈值（环境变量GOGC）：默认100%，即当内存扩大一倍时启用GC。&lt;/li&gt;
&lt;li&gt;使用系统监控，当超过两分钟没有产生任何GC时，强制触发 GC。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Golang为什么小对象多了会造成gc压力&lt;/p&gt;
&lt;p&gt;通常小对象过多会导致GC三色法消耗过多的GPU。优化思路是，减少对象分配。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GC的屏障介绍&lt;/p&gt;
&lt;p&gt;写屏障(Write Barrier)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为了避免GC的过程中新修改的引用关系到GC的结果发生错误，我们需要进行STW。但是STW会影响程序的性能，所以我们要通过写屏障技术尽可能地缩短STW的时间。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;写屏障&lt;/strong&gt;：并发gc会产生黑色节点引用白色节点情况，导致正常的指针变量错误的被清除；解决方法为写屏障；&lt;/p&gt;
&lt;p&gt;主要包括强三色不变式和弱三色不变式；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强三色不变&lt;/strong&gt;：黑色节点不能引用白色节点，如果引用白色节点需要将白色节点置灰(插入写屏障)；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;弱三色不变&lt;/strong&gt;：黑节点可以引用白节点，但白节点有其他灰色节点或递归指向存在灰色节点，删除白色节点引用时，需要把白色节点置灰(删除写屏障);&lt;/p&gt;
&lt;p&gt;栈上变量较小，且频繁开辟或删除，不开启写屏障；需要之后一次rescan；&lt;/p&gt;
&lt;p&gt;stw时机：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;插入写屏障：结束时需要STW来重新扫描栈，标记栈上引用的白色对象的存活；(1.5版本采用)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;删除写屏障：回收精度低，GC开始时STW扫描堆栈来记录初始快照，这个过程会保护开始时刻的所有存活对象；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;混合写屏障&lt;/strong&gt;：1.8版本加入&lt;/p&gt;
&lt;p&gt;原因是stw需要耗时；加入混合写屏障，解决这个问题；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;流程&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;1.开始标记时候，栈上可达节点均置黑，之后不进行rescan，不用stw；&lt;/p&gt;
&lt;p&gt;2.gc时产生的在栈上创建的对象，均置黑；&lt;/p&gt;
&lt;p&gt;3.堆空间删除的对象置灰；&lt;/p&gt;
&lt;p&gt;4.堆空间插入的对象置灰；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;混合写屏障继承了插入写屏障的优点，起始无需 STW 打快照，直接并发扫描垃圾即可；&lt;/li&gt;
&lt;li&gt;混合写屏障继承了删除写屏障的优点，赋值器是黑色赋值器，GC 期间，任何在栈上创建的新对象，均为黑色。扫描过一次就不需要扫描了，这样就消除了插入写屏障时期最后 STW 的重新扫描栈；&lt;/li&gt;
&lt;li&gt;混合写屏障扫描精度继承了删除写屏障，比插入写屏障更低，随着带来的是 GC 过程全程无 STW；&lt;/li&gt;
&lt;li&gt;混合写屏障扫描栈虽然没有 STW，但是扫描某一个具体的栈的时候，还是要停止这个 goroutine 赋值器的工作的哈（针对一个 goroutine 栈来说，是暂停扫的，要么全灰，要么全黑哈，原子状态切换）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GC 的流程是什么&lt;/p&gt;
&lt;p&gt;当前版本的 Go 以 STW 为界限，可以将 GC 划分为五个阶段：&lt;/p&gt;
&lt;p&gt;阶段说明赋值器状态 GCMark 标记准备阶段，为并发标记做准备工作，启动写屏障 STWGCMark 扫描标记阶段，与赋值器并发执行，写屏障开启并发&lt;/p&gt;
&lt;p&gt;GCMarkTermination 标记终止阶段，保证一个周期内标记任务完成，停止写屏障 STWGCoff 内存清扫阶段，将需要回收的内存归还到堆中，写屏障关闭并发&lt;/p&gt;
&lt;p&gt;GCoff 内存归还阶段，将过多的内存归还给操作系统，写屏障关闭并发&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GC 如何调优&lt;/p&gt;
&lt;p&gt;优化内存的申请速度，尽可能少申请内存，复用已申请的内存。三个关键字：&lt;strong&gt;控制、减少、复用&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;通过 go tool pprof 和 go tool trace 等工具&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;控制内存分配的速度，限制 goroutine 的数量，从而提高赋值器对 CPU 的利用率。&lt;/li&gt;
&lt;li&gt;减少并复用内存，例如使用 sync.Pool 来复用需要频繁创建临时对象，例如提前分配足够的内存来降低多余的拷贝。&lt;/li&gt;
&lt;li&gt;需要时，增大 GOGC 的值，降低 GC 的运行频率。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;17-其他知识点&#34;&gt;1.7 其他知识点&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;new和make的区别&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;make 仅用来分配及初始化类型为 slice、map、chan 的数据。&lt;/li&gt;
&lt;li&gt;new 可分配任意类型的数据，根据传入的类型申请一块内存，返回指向这块内存的指针，即类型 *Type。&lt;/li&gt;
&lt;li&gt;make 返回引用，即 Type，new 分配的空间被清零， make 分配空间后，会进行初始。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;go的内存分配是怎么样的&lt;/p&gt;
&lt;p&gt;Go 的内存分配借鉴了 Google 的 TCMalloc 分配算法，其核心思想是内存池 + 多级对象管理。内存池主要是预先分配内存，减少向系统申请的频率；多级对象有：mheap、mspan、arenas、mcentral、mcache。它们以 mspan 作为基本分配单位。具体的分配逻辑如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当要分配大于 32K 的对象时，从 mheap 分配。&lt;/li&gt;
&lt;li&gt;当要分配的对象小于等于 32K 大于 16B 时，从 P 上的 mcache 分配，如果 mcache 没有内存，则从 mcentral 获取，如果 mcentral 也没有，则向 mheap 申请，如果 mheap 也没有，则从操作系统申请内存。&lt;/li&gt;
&lt;li&gt;当要分配的对象小于等于 16B 时，从 mcache 上的微型分配器上分配。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;竞态、内存逃逸&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;竞态&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;资源竞争，就是在程序中，同一块内存同时被多个 goroutine 访问。我们使用 go build、go run、go test 命令时，添加 -race 标识可以检查代码中是否存在资源竞争。&lt;/p&gt;
&lt;p&gt;解决这个问题，我们可以给资源进行加锁，让其在同一时刻只能被一个协程来操作。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sync.Mutex&lt;/li&gt;
&lt;li&gt;sync.RWMutex&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;逃逸分析&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;「逃逸分析」就是程序运行时内存的分配位置(栈或堆)，是由编译器来确定的。堆适合不可预知大小的内存分配。但是为此付出的代价是分配速度较慢，而且会形成内存碎片。&lt;/p&gt;
&lt;p&gt;在 Go 里变量的内存分配方式则是由编译器来决定的。如果变量在作用域（比如函数范围）之外，还会被引用的话，那么称之为发生了逃逸行为，此时将会把对象放到堆上，即使声明为值类型；如果没有发生逃逸行为的话，则会被分配到栈上，即使 new 了一个对象。&lt;/p&gt;
&lt;p&gt;逃逸场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;指针逃逸&lt;/li&gt;
&lt;li&gt;栈空间不足逃逸&lt;/li&gt;
&lt;li&gt;动态类型逃逸&lt;/li&gt;
&lt;li&gt;闭包引用对象逃逸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;什么是 rune 类型&lt;/p&gt;
&lt;p&gt;rune 类型，代表一个 UTF-8 字符，当需要处理中文、日文或者其他复合字符时，则需要用到 rune 类型。rune 类型等价于 int32 类型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;go语言触发异常的场景有哪些&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;空指针解析&lt;/li&gt;
&lt;li&gt;下标越界&lt;/li&gt;
&lt;li&gt;除数为0&lt;/li&gt;
&lt;li&gt;调用panic函数&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;go的接口&lt;/p&gt;
&lt;p&gt;Go 语言提供了另外一种数据类型即接口，它把所有的具有共性的方法定义在一起，任何其他类型只要实现了这些方法就是实现了这个接口。&lt;/p&gt;
&lt;p&gt;接口可以让我们将不同的类型绑定到一组公共的方法上，从而实现多态和灵活的设计。&lt;/p&gt;
&lt;p&gt;Go 语言中的接口是隐式实现的，也就是说，如果一个类型实现了一个接口定义的所有方法，那么它就自动地实现了该接口。因此，我们可以通过将接口作为参数来实现对不同类型的调用，从而实现多态。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;相比较于其他语言, Go 有什么优势或者特点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go代码的设计是务实的。每个功能和语法决策都旨在让程序员的生活更轻松。&lt;/li&gt;
&lt;li&gt;Golang 针对并发进行了优化，并且在规模上运行良好。&lt;/li&gt;
&lt;li&gt;由于单一的标准代码格式，Golang 通常被认为比其他语言更具可读性。&lt;/li&gt;
&lt;li&gt;自动垃圾收集明显比 Java 或 Python 更有效，因为它与程序同时执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;defer、panic、recover 三者的用法&lt;/p&gt;
&lt;p&gt;defer 函数调用的顺序是后进先出，当产生 panic 的时候，会先执行 panic 前面的 defer 函数后才真的抛出异常。一般的，recover 会在 defer 函数里执行并捕获异常，防止程序崩溃。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go反射&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;介绍&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Go语言提供了一种机制在运行时更新和检查变量的值、调用变量的方法和变量支持的内在操作，但是在编译时并不知道这些变量的具体类型，这种机制被称为反射。反射也可以让我们将类型本身作为第一类的值类型处理。&lt;/p&gt;
&lt;p&gt;反射是指在程序运行期对程序本身进行访问和修改的能力，程序在编译时变量被转换为内存地址，变量名不会被编译器写入到可执行部分，在运行程序时程序无法获取自身的信息。&lt;/p&gt;
&lt;p&gt;Go语言中的反射是由 reflect 包提供支持的，它定义了两个重要的类型 Type 和 Value 任意接口值在反射中都可以理解为由 reflect.Type 和 reflect.Value 两部分组成，并且 reflect 包提供了 reflect.TypeOf 和 reflect.ValueOf 两个函数来获取任意对象的 Value 和 Type。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;反射三定律&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;反射第一定律：反射可以将interface类型变量转换成反射对象&lt;/li&gt;
&lt;li&gt;反射第二定律：反射可以将反射对象还原成interface对象&lt;/li&gt;
&lt;li&gt;反射第三定律：反射对象可修改，value值必须是可设置的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go语言函数传参是值类型还是引用类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在Go语言中只存在值传递，要么是值的副本，要么是指针的副本。无论是值类型的变量还是引用类型的变量亦或是指针类型的变量作为参数传递都会发生值拷贝，开辟新的内存空间。&lt;/li&gt;
&lt;li&gt;另外值传递、引用传递和值类型、引用类型是两个不同的概念，不要混淆了。引用类型作为变量传递可以影响到函数外部是因为发生值拷贝后新旧变量指向了相同的内存地址。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go语言中的内存对齐&lt;/p&gt;
&lt;p&gt;CPU 访问内存时，并不是逐个字节访问，而是以字长（word size）为单位访问。比如 32 位的 CPU ，字长为 4 字节，那么 CPU 访问内存的单位也是 4 字节。&lt;/p&gt;
&lt;p&gt;CPU 始终以字长访问内存，如果不进行内存对齐，很可能增加 CPU 访问内存的次数，例如：&lt;/p&gt;
&lt;p&gt;变量 a、b 各占据 3 字节的空间，内存对齐后，a、b 占据 4 字节空间，CPU 读取 b 变量的值只需要进行一次内存访问。如果不进行内存对齐，CPU 读取 b 变量的值需要进行 2 次内存访问。第一次访问得到 b 变量的第 1 个字节，第二次访问得到 b 变量的后两个字节。&lt;/p&gt;
&lt;p&gt;内存对齐对实现变量的原子性操作也是有好处的，每次内存访问是原子的，如果变量的大小不超过字长，那么内存对齐后，对该变量的访问就是原子的，这个特性在并发场景下至关重要。&lt;/p&gt;
&lt;p&gt;简言之：合理的内存对齐可以提高内存读写的性能，并且便于实现变量操作的原子性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;空 struct{} 的用途&lt;/p&gt;
&lt;p&gt;因为空结构体不占据内存空间，因此被广泛作为各种场景下的占位符使用。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 map 作为集合(Set)使用时，可以将值类型定义为空结构体，仅作为占位符使用即可。&lt;/li&gt;
&lt;li&gt;不发送数据的信道(channel) 使用 channel 不需要发送任何的数据，只用来通知子协程(goroutine)执行任务，或只用来控制协程并发度。&lt;/li&gt;
&lt;li&gt;结构体只包含方法，不包含任何的字段&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;值传递和地址传递（引用传递）&lt;/p&gt;
&lt;p&gt;Go 语言中&lt;strong&gt;所有的传参都是值传递&lt;/strong&gt;（传值），都是一个副本，一个拷贝。因为拷贝的内容有时候是非引用类型（int、string、struct 等这些），这样就在函数中就无法修改原内容数据；有的是&lt;strong&gt;引用类型（指针、map、slice、chan&lt;/strong&gt;等 这些），这样就可以修改原内容数据。&lt;/p&gt;
&lt;p&gt;Golang 的引用类型包括 slice、map 和 channel。它们有复杂的内部结构，除了申请内存外，还需要初始化相关属性。内置函数 new 计算类型大小，为其分配零值内存，返回指针。而 make 会被编译器翻译成具体的创建函数，由其分 配内存和初始化成员结构，返回对象而非指针。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;原子操作&lt;/p&gt;
&lt;p&gt;一个或者多个操作在 CPU 执行过程中不被中断的特性，称为原子性 (atomicity)。&lt;/p&gt;
&lt;p&gt;这些操作对外表现成一个不可分割的整体，他们要么都执行，要么都不执行，外界不会看到他们只执行到一半的状态。而在现实世界中，CPU不可能不中断的执行一系列操作，但如果我们在执行多个操作时，能让他们的中间状态对外不可见，那我们就可以宣城他们拥有了“不可分割”的原子性。&lt;/p&gt;
&lt;p&gt;在 Go 中，一条普通的赋值语句其实不是一个原子操作。列如，在 32 位机器上写 int64 类型的变量就会有中间状态，因为他会被拆成两次写操作(MOV)——写低 32 位和写高 32 位。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
">Go实现原理-总结</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-nei-cun-guan-li/"" data-c="
          &lt;p&gt;这篇文章&lt;strong&gt;主要介绍Go内存分配和Go内存管理&lt;/strong&gt;，会轻微涉及内存申请和释放，以及Go垃圾回收。&lt;/p&gt;
&lt;p&gt;从非常宏观的角度讲，Go的内存管理就是下图这个样子，我们今天主要关注其中标红的部分。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/gykn8JAcmMLYG92.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;Go内存管理&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;友情提醒： 文章有点长，建议先收藏，后阅读，绝对是学习内存管理的好资料。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;本文基于go1.11.2，不同版本Go的内存管理可能存在差别，比如1.9与1.11的mheap定义就是差别比较大的，后续看源码的时候，请注意你的go版本，但无论你用哪个go版本，这都是一个优秀的资料，因为内存管理的思想和框架始终未变。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Go这门语言抛弃了C/C++中的开发者管理内存的方式：主动申请与主动释放，增加了逃逸分析和GC，将开发者从内存管理中释放出来，让开发者有更多的精力去关注软件设计，而不是底层的内存问题。这是Go语言成为高生产力语言的原因之一。&lt;/p&gt;
&lt;p&gt;我们不需要精通内存的管理，因为它确实很复杂，但掌握内存的管理，可以让你写出更高质量的代码，另外，还能助你定位Bug。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这篇文章采用层层递进的方式，依次会介绍关于存储的基本知识，Go内存管理的“前辈”TCMalloc，然后是Go的内存管理和分配，最后是总结。这么做的目的是，希望各位能通过全局的认识和思考，拥有更好的编码思维和架构思维。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;最后，这不是一篇源码分析文章，因为Go源码分析的文章已经有很多了，这些源码文章能够帮助你去学习具体的工程实践和奇淫巧计了，文章的末尾会推荐一些优秀文章，如果你对内存感兴趣，建议每一篇都去看一下，挑出自己喜欢的，多花时间研究下。&lt;/p&gt;
&lt;p&gt;目录：&lt;/p&gt;
&lt;p&gt;1. 存储基础知识回顾存储金字塔虚拟内存栈和堆堆内存管理2. TCMalloc基本原理精彩文章推荐3. Go内存管理Go内存管理的基本概念PageSpanmcachemcentralmheap大小转换Go内存分配小对象分配为对象寻找span从span分配对象空间span没有空间怎么分配对象mcentral向mcache提供spanmheap的span管理mcentral向mheap要spanmheap向OS申请内存大对象分配Go垃圾回收和内存释放4. Go栈内存5. 总结6. 参考资料7. 彩蛋&lt;/p&gt;
&lt;h2 id=&#34;1-存储基础知识回顾&#34;&gt;&lt;strong&gt;1. 存储基础知识回顾&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;这部分我们简单回顾一下计算机存储体系、虚拟内存、栈和堆，以及堆内存的管理，这部分内容对理解和掌握Go内存管理比较重要，建议忘记或不熟悉的朋友不要跳过。&lt;/p&gt;
&lt;h3 id=&#34;存储金字塔&#34;&gt;&lt;strong&gt;存储金字塔&lt;/strong&gt;&lt;/h3&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/sSfocMAFLmZTXNu.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;img&lt;/p&gt;
&lt;p&gt;这幅图表达了计算机的存储体系，从上至下依次是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU寄存器&lt;/li&gt;
&lt;li&gt;Cache&lt;/li&gt;
&lt;li&gt;内存&lt;/li&gt;
&lt;li&gt;硬盘等辅助存储设备&lt;/li&gt;
&lt;li&gt;鼠标等外接设备&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从上至下，访问速度越来越慢，访问时间越来越长。&lt;/p&gt;
&lt;p&gt;你有没有思考过下面2个简单的问题，如果没有不妨想想：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果CPU直接访问硬盘，CPU能充分利用吗？&lt;/li&gt;
&lt;li&gt;如果CPU直接访问内存，CPU能充分利用吗？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;CPU速度很快，但硬盘等持久存储很慢，如果CPU直接访问磁盘，磁盘可以拉低CPU的速度，机器整体性能就会低下，为了弥补这2个硬件之间的速率差异，所以在CPU和磁盘之间增加了比磁盘快很多的内存。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;3&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/d79DKkASIx5RBFW.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;CPU和内存速率差异&lt;/p&gt;
&lt;p&gt;然而，CPU跟内存的速率也不是相同的，从上图可以看到，CPU的速率提高的很快（摩尔定律），然而内存速率增长的很慢，&lt;em&gt;虽然CPU的速率现在增加的很慢了，但是内存的速率也没增加多少，速率差距很大&lt;/em&gt;，从1980年开始CPU和内存速率差距在不断拉大，为了弥补这2个硬件之间的速率差异，所以在CPU跟内存之间增加了比内存更快的Cache，Cache是内存数据的缓存，可以降低CPU访问内存的时间。&lt;/p&gt;
&lt;p&gt;不要以为有了Cache就万事大吉了，CPU的速率还在不断增大，Cache也在不断改变，从最初的1级，到后来的2级，到当代的3级Cache，&lt;em&gt;（有兴趣看cache历史）&lt;/em&gt;。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;4&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/ivklQXa9LOMIEqt.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;MBP的CPU和Cache信息&lt;/p&gt;
&lt;p&gt;三级Cache分别是L1、L2、L3，它们的速率是三个不同的层级，L1速率最快，与CPU速率最接近，是RAM速率的100倍，L2速率就降到了RAM的25倍，L3的速率更靠近RAM的速率。&lt;/p&gt;
&lt;p&gt;看到这了，你有没有Get到整个&lt;strong&gt;存储体系的分层设计&lt;/strong&gt;？&lt;strong&gt;自顶向下，速率越来越低，访问时间越来越长，从磁盘到CPU寄存器，上一层都可以看做是下一层的缓存。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;看了分层设计，我们看一下内存，毕竟我们是介绍内存管理的文章。&lt;/p&gt;
&lt;h3 id=&#34;虚拟内存&#34;&gt;&lt;strong&gt;虚拟内存&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;虚拟内存是当代操作系统必备的一项重要功能了，它向进程屏蔽了底层了RAM和磁盘，并向进程提供了远超物理内存大小的内存空间。我们看一下虚拟内存的&lt;strong&gt;分层设计&lt;/strong&gt;。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;5&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/SaCrZ8MnTvYF7lL.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;虚拟内存原理&lt;/p&gt;
&lt;p&gt;上图展示了某进程访问数据，当Cache没有命中的时候，访问虚拟内存获取数据的过程。&lt;/p&gt;
&lt;p&gt;访问内存，实际访问的是虚拟内存，虚拟内存通过页表查看，当前要访问的虚拟内存地址，是否已经加载到了物理内存，如果已经在物理内存，则取物理内存数据，如果没有对应的物理内存，则从磁盘加载数据到物理内存，并把物理内存地址和虚拟内存地址更新到页表。&lt;/p&gt;
&lt;p&gt;有没有Get到：&lt;strong&gt;物理内存就是磁盘存储缓存层&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;另外，在没有虚拟内存的时代，物理内存对所有进程是共享的，多进程同时访问同一个物理内存存在并发访问问题。&lt;strong&gt;引入虚拟内存后，每个进程都要各自的虚拟内存，内存的并发访问问题的粒度从多进程级别，可以降低到多线程级别&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;栈和堆&#34;&gt;&lt;strong&gt;栈和堆&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;我们现在从虚拟内存，再进一层，看虚拟内存中的栈和堆，也就是进程对内存的管理。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;6&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/ZV3CGMHB51rWhoq.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;虚拟内存布局&lt;/p&gt;
&lt;p&gt;上图展示了一个进程的虚拟内存划分，代码中使用的内存地址都是虚拟内存地址，而不是实际的物理内存地址。栈和堆只是虚拟内存上2块不同功能的内存区域：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;栈在高地址，从高地址向低地址增长。&lt;/li&gt;
&lt;li&gt;堆在低地址，从低地址向高地址增长。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;栈和堆相比有这么几个好处&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;栈的内存管理简单，分配比堆上快。&lt;/li&gt;
&lt;li&gt;栈的内存不需要回收，而堆需要，无论是主动free，还是被动的垃圾回收，这都需要花费额外的CPU。&lt;/li&gt;
&lt;li&gt;栈上的内存有更好的局部性，堆上内存访问就不那么友好了，CPU访问的2块数据可能在不同的页上，CPU访问数据的时间可能就上去了。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;堆内存管理&#34;&gt;&lt;strong&gt;堆内存管理&lt;/strong&gt;&lt;/h3&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;7&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/NqYG2isBetFWohx.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;内存管理&lt;/p&gt;
&lt;p&gt;我们再进一层，当我们说内存管理的时候，主要是指堆内存的管理，因为栈的内存管理不需要程序去操心。这小节看下堆内存管理干的是啥，如上图所示主要是3部分：&lt;strong&gt;分配内存块，回收内存块和组织内存块&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在一个最简单的内存管理中，堆内存最初会是一个完整的大块，即未分配内存，当来申请的时候，就会从未分配内存，分割出一个小内存块(block)，然后用链表把所有内存块连接起来。需要一些信息描述每个内存块的基本信息，比如大小(size)、是否使用中(used)和下一个内存块的地址(next)，内存块实际数据存储在data中。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;8&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/iCmeGvwc2hx41lZ.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;内存块链表&lt;/p&gt;
&lt;p&gt;一个内存块包含了3类信息，如下图所示，元数据、用户数据和对齐字段，内存对齐是为了提高访问效率。下图申请5Byte内存的时候，就需要进行内存对齐。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;9&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/UH3DGdAbP8xWwqE.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;内存块和对齐&lt;/p&gt;
&lt;p&gt;释放内存实质是把使用的内存块从链表中取出来，然后标记为未使用，当分配内存块的时候，可以从未使用内存块中有先查找大小相近的内存块，如果找不到，再从未分配的内存中分配内存。&lt;/p&gt;
&lt;p&gt;上面这个简单的设计中还没考虑内存碎片的问题，因为随着内存不断的申请和释放，内存上会存在大量的碎片，降低内存的使用率。为了解决内存碎片，可以将2个连续的未使用的内存块合并，减少碎片。&lt;/p&gt;
&lt;p&gt;以上就是内存管理的基本思路，关于基本的内存管理，想了解更多，可以阅读这篇文章《Writing a Memory Allocator》，本节的3张图片也是来自这片文章。&lt;/p&gt;
&lt;h2 id=&#34;2-tcmalloc&#34;&gt;&lt;strong&gt;2. TCMalloc&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;TCMalloc是Thread Cache Malloc的简称，是Go内存管理的起源&lt;/strong&gt;，Go的内存管理是借鉴了TCMalloc，随着Go的迭代，Go的内存管理与TCMalloc不一致地方在不断扩大，但&lt;strong&gt;其主要思想、原理和概念都是和TCMalloc一致的&lt;/strong&gt;，如果跳过TCMalloc直接去看Go的内存管理，也许你会似懂非懂。&lt;/p&gt;
&lt;p&gt;掌握TCMalloc的理念，&lt;em&gt;无需去关注过多的源码细节&lt;/em&gt;，就可以为掌握Go的内存管理打好基础，基础打好了，后面知识才扎实。&lt;/p&gt;
&lt;p&gt;在Linux里，其实有不少的内存管理库，比如glibc的ptmalloc，FreeBSD的jemalloc，Google的tcmalloc等等，为何会出现这么多的内存管理库？本质都是&lt;strong&gt;在多线程编程下，追求更高内存管理效率&lt;/strong&gt;：更快的分配是主要目的。&lt;/p&gt;
&lt;p&gt;那如何更快的分配内存？&lt;/p&gt;
&lt;p&gt;我们前面提到：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;引入虚拟内存后，让内存的并发访问问题的粒度从多进程级别，降低到多线程级别。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这是&lt;strong&gt;更快分配内存的第一个层次&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;同一进程的所有线程共享相同的内存空间，他们申请内存时需要加锁，如果不加锁就存在同一块内存被2个线程同时访问的问题。&lt;/p&gt;
&lt;p&gt;TCMalloc的做法是什么呢？&lt;strong&gt;为每个线程预分配一块缓存，线程申请小内存时，可以从缓存分配内存&lt;/strong&gt;，这样有2个好处：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为线程预分配缓存需要进行1次系统调用，后续线程申请小内存时，从缓存分配，都是在用户态执行，没有系统调用，&lt;strong&gt;缩短了内存总体的分配和释放时间，这是快速分配内存的第二个层次&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;多个线程同时申请小内存时，从各自的缓存分配，访问的是不同的地址空间，无需加锁，&lt;strong&gt;把内存并发访问的粒度进一步降低了，这是快速分配内存的第三个层次&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;基本原理&#34;&gt;&lt;strong&gt;基本原理&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;下面就简单介绍下TCMalloc，细致程度够我们理解Go的内存管理即可。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;声明：我没有研究过TCMalloc，以下介绍根据TCMalloc官方资料和其他博主资料总结而来，错误之处请朋友告知我。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;10&#34;&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692782705558.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;TCMalloc概要图&lt;/p&gt;
&lt;p&gt;结合上图，介绍TCMalloc的几个重要概念：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Page&lt;/strong&gt;：操作系统对内存管理以页为单位，TCMalloc也是这样，只不过TCMalloc里的Page大小与操作系统里的大小并不一定相等，而是倍数关系。《TCMalloc解密》里称x64下Page大小是8KB。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Span&lt;/strong&gt;：一组连续的Page被称为Span，比如可以有2个页大小的Span，也可以有16页大小的Span，Span比Page高一个层级，是为了方便管理一定大小的内存区域，Span是TCMalloc中内存管理的基本单位。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ThreadCache&lt;/strong&gt;：每个线程各自的Cache，一个Cache包含多个空闲内存块链表，每个链表连接的都是内存块，同一个链表上内存块的大小是相同的，也可以说按内存块大小，给内存块分了个类，这样可以根据申请的内存大小，快速从合适的链表选择空闲内存块。由于每个线程有自己的ThreadCache，所以ThreadCache访问是无锁的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CentralCache&lt;/strong&gt;：是所有线程共享的缓存，也是保存的空闲内存块链表，链表的数量与ThreadCache中链表数量相同，当ThreadCache内存块不足时，可以从CentralCache取，当ThreadCache内存块多时，可以放回CentralCache。由于CentralCache是共享的，所以它的访问是要加锁的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PageHeap&lt;/strong&gt;：PageHeap是堆内存的抽象，PageHeap存的也是若干链表，链表保存的是Span，当CentralCache没有内存的时，会从PageHeap取，把1个Span拆成若干内存块，添加到对应大小的链表中，当CentralCache内存多的时候，会放回PageHeap。如下图，分别是1页Page的Span链表，2页Page的Span链表等，最后是large span set，这个是用来保存中大对象的。毫无疑问，PageHeap也是要加锁的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692782590901.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
PageHeap&lt;/p&gt;
&lt;p&gt;上文提到了小、中、大对象，Go内存管理中也有类似的概念，我们瞄一眼TCMalloc的定义：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;小对象大小：0~256KB&lt;/li&gt;
&lt;li&gt;中对象大小：257~1MB&lt;/li&gt;
&lt;li&gt;大对象大小：&amp;gt;1MB&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;小对象的分配流程：ThreadCache -&amp;gt; CentralCache -&amp;gt; HeapPage，大部分时候，ThreadCache缓存都是足够的，不需要去访问CentralCache和HeapPage，无锁分配加无系统调用，分配效率是非常高的。&lt;/p&gt;
&lt;p&gt;中对象分配流程：直接在PageHeap中选择适当的大小即可，128 Page的Span所保存的最大内存就是1MB。&lt;/p&gt;
&lt;p&gt;大对象分配流程：从large span set选择合适数量的页面组成span，用来存储数据。&lt;/p&gt;
&lt;p&gt;通过本节的介绍，你应当对TCMalloc主要思想有一定了解了，我建议再回顾一下上面的内容。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;本节图片皆来自《TCMalloc解密》，图片版权归原作者所有。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;精彩文章推荐&#34;&gt;&lt;strong&gt;精彩文章推荐&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;本文对于TCMalloc的介绍并不多，&lt;strong&gt;重要的是3个快速分配内存的层次&lt;/strong&gt;，如果想了解更多，可阅读下面文章。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;TCMalloc &lt;strong&gt;必读&lt;/strong&gt;，通过这篇你能掌握TCMalloc的原理和性能，对掌握Go的内存管理有非常大的帮助，虽然如今Go的内存管理与TCMalloc已经相差很大，但是，这是&lt;strong&gt;Go内存管理的起源和“大道”&lt;/strong&gt;，这篇文章顶看十几篇Go内存管理的文章。&lt;/li&gt;
&lt;li&gt;TCMalloc解密 &lt;strong&gt;可选&lt;/strong&gt;，&lt;strong&gt;异常详细，包含大量精美图片&lt;/strong&gt;，看完得花小时级别，理解就需要更多时间了，看完这篇不需要看其他TCMalloc的文章了。&lt;/li&gt;
&lt;li&gt;TCMalloc介绍 &lt;strong&gt;可选&lt;/strong&gt;，算是TCMalloc的文档的中文版，多数是从英文版翻译过来的，如果你英文不好，看看。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;3-go内存管理&#34;&gt;&lt;strong&gt;3. Go内存管理&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;前面铺垫了那么多，终于到了本文核心的地方。前面的铺垫不是不重要，相反它们很重要，Go语言内存管理源自前面的基础知识和内存管理思维，如果你跳过了前面的内容，建议你回头看一看，它可以帮助你更好的掌握Go内存管理。&lt;/p&gt;
&lt;p&gt;前文提到&lt;strong&gt;Go内存管理源自TCMalloc，但它比TCMalloc还多了2件东西：逃逸分析和垃圾回收&lt;/strong&gt;，这是2项提高生产力的绝佳武器。&lt;/p&gt;
&lt;p&gt;这一大章节，我们先介绍Go内存管理和Go内存分配，最后涉及一点垃圾回收和内存释放。&lt;/p&gt;
&lt;h3 id=&#34;go内存管理的基本概念&#34;&gt;&lt;strong&gt;Go内存管理的基本概念&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;前面计算机基础知识回顾，是一种自上而下，从宏观到微观的介绍方式，把目光引入到今天的主题。&lt;/p&gt;
&lt;p&gt;Go内存管理的许多概念在TCMalloc中已经有了，含义是相同的，只是名字有一些变化。先给大家上一幅宏观的图，借助图一起来介绍。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;11&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/OwMkYCThsZibqK2.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;Go内存管理&lt;/p&gt;
&lt;h4 id=&#34;page&#34;&gt;&lt;strong&gt;Page&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;与TCMalloc中的Page相同，x64下1个Page的大小是8KB。上图的最下方，1个浅蓝色的长方形代表1个Page。&lt;/p&gt;
&lt;h4 id=&#34;span&#34;&gt;&lt;strong&gt;Span&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;与TCMalloc中的Span相同，&lt;strong&gt;Span是内存管理的基本单位&lt;/strong&gt;，代码中为&lt;code&gt;mspan&lt;/code&gt;，&lt;strong&gt;一组连续的Page组成1个Span&lt;/strong&gt;，所以上图一组连续的浅蓝色长方形代表的是一组Page组成的1个Span，另外，1个淡紫色长方形为1个Span。&lt;/p&gt;
&lt;h4 id=&#34;mcache&#34;&gt;&lt;strong&gt;mcache&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;mcache与TCMalloc中的ThreadCache类似，&lt;strong&gt;mcache保存的是各种大小的Span，并按Span class分类，小对象直接从mcache分配内存，它起到了缓存的作用，并且可以无锁访问&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;但mcache与ThreadCache也有不同点，TCMalloc中是每个线程1个ThreadCache，Go中是&lt;strong&gt;每个P拥有1个mcache&lt;/strong&gt;，因为在Go程序中，当前最多有GOMAXPROCS个线程在运行，所以最多需要GOMAXPROCS个mcache就可以保证各线程对mcache的无锁访问，线程的运行又是与P绑定的，把mcache交给P刚刚好。&lt;/p&gt;
&lt;h4 id=&#34;mcentral&#34;&gt;&lt;strong&gt;mcentral&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;mcentral与TCMalloc中的CentralCache类似，&lt;strong&gt;是所有线程共享的缓存，需要加锁访问&lt;/strong&gt;，它按Span class对Span分类，串联成链表，当mcache的某个级别Span的内存被分配光时，它会向mcentral申请1个当前级别的Span。&lt;/p&gt;
&lt;p&gt;但mcentral与CentralCache也有不同点，CentralCache是每个级别的Span有1个链表，mcache是每个级别的Span有2个链表，这和mcache申请内存有关，稍后我们再解释。&lt;/p&gt;
&lt;h4 id=&#34;mheap&#34;&gt;&lt;strong&gt;mheap&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;mheap与TCMalloc中的PageHeap类似，&lt;strong&gt;它是堆内存的抽象，把从OS申请出的内存页组织成Span，并保存起来&lt;/strong&gt;。当mcentral的Span不够用时会向mheap申请，mheap的Span不够用时会向OS申请，向OS的内存申请是按页来的，然后把申请来的内存页生成Span组织起来，同样也是需要加锁访问的。&lt;/p&gt;
&lt;p&gt;但mheap与PageHeap也有不同点：mheap把Span组织成了树结构，而不是链表，并且还是2棵树，然后把Span分配到heapArena进行管理，它包含地址映射和span是否包含指针等位图，这样做的主要原因是为了更高效的利用内存：分配、回收和再利用。&lt;/p&gt;
&lt;h4 id=&#34;大小转换&#34;&gt;&lt;strong&gt;大小转换&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;除了以上内存块组织概念，还有几个重要的大小概念，一定要拿出来讲一下，不要忽视他们的重要性，他们是内存分配、组织和地址转换的基础。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;12&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/18esSzyTkaB3ZDH.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;Go内存大小转换&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;object size&lt;/strong&gt;：代码里简称&lt;code&gt;size&lt;/code&gt;，指申请内存的对象大小。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;size class&lt;/strong&gt;：代码里简称&lt;code&gt;class&lt;/code&gt;，它是size的级别，相当于把size归类到一定大小的区间段，比如size[1,8]属于size class 1，size(8,16]属于size class 2。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;span class&lt;/strong&gt;：指span的级别，但span class的大小与span的大小并没有正比关系。span class主要用来和size class做对应，1个size class对应2个span class，2个span class的span大小相同，只是功能不同，1个用来存放包含指针的对象，一个用来存放不包含指针的对象，不包含指针对象的Span就无需GC扫描了。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;num of page&lt;/strong&gt;：代码里简称&lt;code&gt;npage&lt;/code&gt;，代表Page的数量，其实就是Span包含的页数，用来分配内存。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在介绍这几个大小之间的换算前，我们得先看下图这个表，这个表决定了映射关系。&lt;/p&gt;
&lt;p&gt;最上面2行是我手动加的，前3列分别是size class，object size和span size，根据这3列做size、size class和num of page之间的转换。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;另外，第4列num of objects代表是当前size class级别的Span可以保存多少对象数量，第5列tail waste是&lt;code&gt;span%obj&lt;/code&gt;计算的结果，因为span的大小并不一定是对象大小的整数倍。最后一列max waste代表最大浪费的内存百分比，计算方法在&lt;code&gt;printComment&lt;/code&gt;函数中，没搞清为何这样计算。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;仔细看一遍这个表，再向下看转换是如何实现的。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;13&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/mxG5OaV3FEIpAXM.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;Go内存分配表&lt;/p&gt;
&lt;p&gt;在Go内存大小转换那幅图中已经标记各大小之间的转换，分别是数组：&lt;code&gt;class_to_size&lt;/code&gt;，&lt;code&gt;size_to_class*&lt;/code&gt;和&lt;code&gt;class_to_allocnpages&lt;/code&gt;，这3个数组内容，就是跟上表的映射关系匹配的。比如&lt;code&gt;class_to_size&lt;/code&gt;，从上表看class 1对应的保存对象大小为8，所以&lt;code&gt;class_to_size[1]=8&lt;/code&gt;，span大小为8192Byte，即8KB，为1页，所以&lt;code&gt;class_to_allocnpages[1]=1&lt;/code&gt;。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;14&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/2HicOJ5VkxACohX.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;Size转换&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为何不使用函数计算各种转换，而是写成数组？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有1个很重要的原因：&lt;strong&gt;空间换时间&lt;/strong&gt;。你如果仔细观察了，上表中的转换，并不能通过简单的公式进行转换，比如size和size class的关系，并不是正比的。这些数据是使用较复杂的公式计算出来的，公式在&lt;code&gt;makesizeclass.go&lt;/code&gt;中，这其中存在指数运算与for循环，造成每次大小转换的时间复杂度为O(N*2^N)。另外，对一个程序而言，内存的申请和管理操作是很多的，如果不能快速完成，就是非常的低效。把以上大小转换写死到数组里，做到了把大小转换的时间复杂度直接降到O(1)。&lt;/p&gt;
&lt;h3 id=&#34;go内存分配&#34;&gt;&lt;strong&gt;Go内存分配&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;涉及的概念已经讲完了，我们看下Go内存分配原理。&lt;/p&gt;
&lt;p&gt;Go中的内存分类并不像TCMalloc那样分成小、中、大对象，但是它的小对象里又细分了一个Tiny对象，Tiny对象指大小在1Byte到16Byte之间并且不包含指针的对象。小对象和大对象只用大小划定，无其他区分。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;15&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/bt2Gj3gFKZDzQTY.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;Go内存对象分类&lt;/p&gt;
&lt;p&gt;小对象是在mcache中分配的，而大对象是直接从mheap分配的，从小对象的内存分配看起。&lt;/p&gt;
&lt;h4 id=&#34;小对象分配&#34;&gt;&lt;strong&gt;小对象分配&lt;/strong&gt;&lt;/h4&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;16&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/OwMkYCThsZibqK2.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;Go内存管理&lt;/p&gt;
&lt;p&gt;大小转换这一小节，我们介绍了转换表，size class从1到66共66个，代码中&lt;code&gt;_NumSizeClasses=67&lt;/code&gt;代表了实际使用的size class数量，即67个，从0到67，size class 0实际并未使用到。&lt;/p&gt;
&lt;p&gt;上文提到1个size class对应2个span class：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;numSpanClasses = _NumSizeClasses * 2 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;numSpanClasses&lt;/code&gt;为span class的数量为134个，所以span class的下标是从0到133，所以上图中mcache标注了的span class是，&lt;code&gt;span class 0&lt;/code&gt;到&lt;code&gt;span class 133&lt;/code&gt;。每1个span class都指向1个span，也就是mcache最多有134个span。&lt;/p&gt;
&lt;h5 id=&#34;为对象寻找span&#34;&gt;&lt;strong&gt;为对象寻找span&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;寻找span的流程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算对象所需内存大小size&lt;/li&gt;
&lt;li&gt;根据size到size class映射，计算出所需的size class&lt;/li&gt;
&lt;li&gt;根据size class和对象是否包含指针计算出span class&lt;/li&gt;
&lt;li&gt;获取该span class指向的span。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以分配一个不包含指针的，大小为24Byte的对象为例。&lt;/p&gt;
&lt;p&gt;根据映射表：&lt;/p&gt;
&lt;p&gt;size class 3，它的对象大小范围是(16,32]Byte，24Byte刚好在此区间，所以此对象的size class为3。&lt;/p&gt;
&lt;p&gt;Size class到span class的计算如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; func makeSpanClass(sizeclass uint8, noscan bool) spanClass {
    return spanClass(sizeclass&amp;lt;&amp;lt;1) | spanClass(bool2int(noscan))
} 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所以，对应的span class为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;span class = 3 &amp;lt;&amp;lt; 1 | 1 = 7 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所以该对象需要的是span class 7指向的span。&lt;/p&gt;
&lt;h5 id=&#34;从span分配对象空间&#34;&gt;&lt;strong&gt;从span分配对象空间&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Span可以按对象大小切成很多份，这些都可以从映射表上计算出来，以size class 3对应的span为例，span大小是8KB，每个对象实际所占空间为32Byte，这个span就被分成了256块，可以根据span的起始地址计算出每个对象块的内存地址。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;17&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/16KiTrgLHGScMYq.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;Span内对象&lt;/p&gt;
&lt;p&gt;随着内存的分配，span中的对象内存块，有些被占用，有些未被占用，比如上图，整体代表1个span，蓝色块代表已被占用内存，绿色块代表未被占用内存。&lt;/p&gt;
&lt;p&gt;当分配内存时，只要快速找到第一个可用的绿色块，并计算出内存地址即可，如果需要还可以对内存块数据清零。&lt;/p&gt;
&lt;h5 id=&#34;span没有空间怎么分配对象&#34;&gt;&lt;strong&gt;span没有空间怎么分配对象&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;span内的所有内存块都被占用时，没有剩余空间继续分配对象，mcache会向mcentral申请1个span，mcache拿到span后继续分配对象。&lt;/p&gt;
&lt;h5 id=&#34;mcentral向mcache提供span&#34;&gt;&lt;strong&gt;mcentral向mcache提供span&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;mcentral和mcache一样，都是0~133这134个span class级别，但每个级别都保存了2个span list，即2个span链表：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;nonempty&lt;/code&gt;：这个链表里的span，所有span都至少有1个空闲的对象空间。这些span是mcache释放span时加入到该链表的。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;empty&lt;/code&gt;：这个链表里的span，所有的span都不确定里面是否有空闲的对象空间。当一个span交给mcache的时候，就会加入到empty链表。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这2个东西名称一直有点绕，建议直接把empty理解为没有对象空间就好了。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;18&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/nhImrCA9QvWBbwy.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;mcentral&lt;/p&gt;
&lt;p&gt;&lt;em&gt;实际代码中每1个span class对应1个mcentral，图里把所有mcentral抽象成1个整体了。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;mcache向mcentral要span时，mcentral会先从&lt;code&gt;nonempty&lt;/code&gt;搜索满足条件的span，如果每找到再从&lt;code&gt;emtpy&lt;/code&gt;搜索满足条件的span，然后把找到的span交给mcache。&lt;/p&gt;
&lt;h5 id=&#34;mheap的span管理&#34;&gt;&lt;strong&gt;mheap的span管理&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;mheap里保存了2棵&lt;strong&gt;二叉排序树&lt;/strong&gt;，按span的page数量进行排序：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;free&lt;/code&gt;：free中保存的span是空闲并且非垃圾回收的span。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scav&lt;/code&gt;：scav中保存的是空闲并且已经垃圾回收的span。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果是垃圾回收导致的span释放，span会被加入到&lt;code&gt;scav&lt;/code&gt;，否则加入到&lt;code&gt;free&lt;/code&gt;，比如刚从OS申请的的内存也组成的Span。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;19&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/PydomukVxDAInhO.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;mheap&lt;/p&gt;
&lt;p&gt;mheap中还有arenas，有一组heapArena组成，每一个heapArena都包含了连续的&lt;code&gt;pagesPerArena&lt;/code&gt;个span，这个主要是为mheap管理span和垃圾回收服务。&lt;/p&gt;
&lt;p&gt;mheap本身是一个全局变量，它其中的数据，也都是从OS直接申请来的内存，并不在mheap所管理的那部分内存内。&lt;/p&gt;
&lt;h5 id=&#34;mcentral向mheap要span&#34;&gt;&lt;strong&gt;mcentral向mheap要span&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;mcentral向mcache提供span时，如果&lt;code&gt;emtpy&lt;/code&gt;里也没有符合条件的span，mcentral会向mheap申请span。&lt;/p&gt;
&lt;p&gt;mcentral需要向mheap提供需要的内存页数和span class级别，然后它优先从&lt;code&gt;free&lt;/code&gt;中搜索可用的span，如果没有找到，会从&lt;code&gt;scav&lt;/code&gt;中搜索可用的span，如果还没有找到，它会向OS申请内存，再重新搜索2棵树，必然能找到span。如果找到的span比需求的span大，则把span进行分割成2个span，其中1个刚好是需求大小，把剩下的span再加入到&lt;code&gt;free&lt;/code&gt;中去，然后设置需求span的基本信息，然后交给mcentral。&lt;/p&gt;
&lt;h5 id=&#34;mheap向os申请内存&#34;&gt;&lt;strong&gt;mheap向OS申请内存&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;当mheap没有足够的内存时，mheap会向OS申请内存，把申请的内存页保存到span，然后把span插入到&lt;code&gt;free&lt;/code&gt;树 。&lt;/p&gt;
&lt;p&gt;在32位系统上，mheap还会预留一部分空间，当mheap没有空间时，先从预留空间申请，如果预留空间内存也没有了，才向OS申请。&lt;/p&gt;
&lt;h4 id=&#34;大对象分配&#34;&gt;&lt;strong&gt;大对象分配&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;大对象的分配比小对象省事多了，99%的流程与mcentral向mheap申请内存的相同，所以不重复介绍了，不同的一点在于mheap会记录一点大对象的统计信息，见&lt;code&gt;mheap.alloc_m()&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&#34;go垃圾回收和内存释放&#34;&gt;&lt;strong&gt;Go垃圾回收和内存释放&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;如果只申请和分配内存，内存终将枯竭，Go使用垃圾回收收集不再使用的span，调用&lt;code&gt;mspan.scavenge()&lt;/code&gt;把span释放给OS（并非真释放，只是告诉OS这片内存的信息无用了，如果你需要的话，收回去好了），然后交给mheap，mheap对span进行span的合并，把合并后的span加入&lt;code&gt;scav&lt;/code&gt;树中，等待再分配内存时，由mheap进行内存再分配，Go垃圾回收也是一个很强的主题，计划后面单独写一篇文章介绍。&lt;/p&gt;
&lt;p&gt;现在我们关注一下，Go程序是怎么把内存释放给操作系统的？&lt;/p&gt;
&lt;p&gt;释放内存的函数是&lt;code&gt;sysUnused&lt;/code&gt;，它会被&lt;code&gt;mspan.scavenge()&lt;/code&gt;调用:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; func sysUnused(v unsafe.Pointer, n uintptr) {
    
    
    madvise(v, n, _MADV_FREE_REUSABLE)
} 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注释说&lt;code&gt;_MADV_FREE_REUSABLE&lt;/code&gt;与&lt;code&gt;MADV_FREE&lt;/code&gt;的功能类似，它的功能是给内核提供一个建议：&lt;strong&gt;这个内存地址区间的内存已经不再使用，可以回收。但内核是否回收，以及什么时候回收，这就是内核的事情了&lt;/strong&gt;。如果内核真把这片内存回收了，当Go程序再使用这个地址时，内核会重新进行虚拟地址到物理地址的映射。所以在内存充足的情况下，内核也没有必要立刻回收内存。&lt;/p&gt;
&lt;h2 id=&#34;4-go栈内存&#34;&gt;&lt;strong&gt;4. Go栈内存&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;最后提一下栈内存。从一个宏观的角度看，内存管理不应当只有堆，也应当有栈。&lt;/p&gt;
&lt;p&gt;每个goroutine都有自己的栈，栈的初始大小是2KB，100万的goroutine会占用2G，但goroutine的栈会在2KB不够用时自动扩容，当扩容为4KB的时候，百万goroutine会占用4GB。&lt;/p&gt;
&lt;p&gt;关于goroutine栈内存管理，有篇很好的文章，饿了么框架技术部的专栏文章：《聊一聊goroutine stack》，把里面的一段内容摘录下，你感受下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;可以看到在rpc调用(&lt;em&gt;grpc invoke&lt;/em&gt;)时，栈会发生扩容(&lt;em&gt;runtime.morestack&lt;/em&gt;)，也就意味着在读写routine内的任何rpc调用都会导致栈扩容， 占用的内存空间会扩大为原来的两倍，4kB的栈会变为8kB，100w的连接的内存占用会从8G扩大为16G（全双工，不考虑其他开销），这简直是噩梦。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;另外，再推荐一篇曹大翻译的一篇汇编入门文章，里面也介绍了扩栈：第一章: Go 汇编入门，顺便&lt;strong&gt;入门&lt;/strong&gt;一下汇编。&lt;/p&gt;
&lt;h2 id=&#34;5-总结&#34;&gt;&lt;strong&gt;5. 总结&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;内存分配原理就不再回顾了，强调2个重要的思想：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;使用缓存提高效率&lt;/strong&gt;。在存储的整个体系中到处可见缓存的思想，Go内存分配和管理也使用了缓存，利用缓存一是减少了系统调用的次数，二是降低了锁的粒度，减少加锁的次数，从这2点提高了内存管理效率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;以空间换时间，提高内存管理效率&lt;/strong&gt;。空间换时间是一种常用的性能优化思想，这种思想其实非常普遍，比如Hash、Map、二叉排序树等数据结构的本质就是空间换时间，在&lt;a href=&#34;https://cloud.tencent.com/solution/database?from_column=20065&amp;amp;from=20065&#34;&gt;数据库&lt;/a&gt;中也很常见，比如数据库索引、索引视图和数据缓存等，再如&lt;a href=&#34;https://cloud.tencent.com/product/crs?from_column=20065&amp;amp;from=20065&#34;&gt;Redis&lt;/a&gt;等缓存数据库也是空间换时间的思想。&lt;/li&gt;
&lt;/ol&gt;
">Go实现原理-内存管理</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-gc/"" data-c="
          &lt;hr&gt;
&lt;h3 id=&#34;什么是gc&#34;&gt;什么是GC？&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;垃圾回收（Garbage Collection，缩写为GC）&lt;/strong&gt;，是一种自动内存管理机制。&lt;/p&gt;
&lt;p&gt;即我们在程序中定义一个变量后，会在内存中开辟相应空间进行存储。当不需要此变量后，需要手动销毁此对象，并释放内存。而这种对不再使用的内存资源进行自动回收的功能即为垃圾回收&lt;/p&gt;
&lt;h3 id=&#34;gc相关术语&#34;&gt;GC相关术语&lt;/h3&gt;
&lt;p&gt;在对GC开始讲解之前，有很多关于GC的行话，先普及一下，不然后文读起来会稍微有点懵。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;赋值器&lt;/strong&gt;:说白了就是你写的程序代码，在程序的执行过程中，可能会改变对象的引用关系，或者创建新的引用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;回收器&lt;/strong&gt;:垃圾回收器的责任就是去干掉那些程序中不再被引用得对象&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;STW&lt;/strong&gt;:全称是stop the word，GC期间某个阶段会停止所有的赋值器，中断你的程序逻辑，以确定引用关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;root对象&lt;/strong&gt;:根对象是指赋值器不需要通过其他对象就可以直接访问到的对象，通过Root对象, 可以追踪到其他存活的对象。常见的root对象有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;全局变量&lt;/strong&gt;：程序在编译期就能确定的那些存在于程序整个生命周期的变量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;执行栈&lt;/strong&gt;：每个 goroutine (包括main函数)都拥有自己的执行栈，这些执行栈上包含&lt;strong&gt;栈上的变量及堆内存指针&lt;/strong&gt;。【堆内存指针即在gorouine中申请或者引用了在堆内存的变量】&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;go的gc发展演变史&#34;&gt;Go的GC发展演变史&lt;/h2&gt;
&lt;h3 id=&#34;v-13-标记清除法&#34;&gt;v 1.3-标记清除法&lt;/h3&gt;
&lt;p&gt;标记清除法主要包含两个步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标记&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;清除&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;示例如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;开启STW，停止程序的运行&lt;/li&gt;
&lt;li&gt;从根节点出发，标记所有可达对象&lt;/li&gt;
&lt;li&gt;停止STW，然后回收所有未被标记的对象&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;标记清除法的&lt;strong&gt;最大弊端就是在整个GC期间需要STW，将整个程序暂停&lt;/strong&gt;。因为如果不进行STW的话，会出现已经被标记的对象A，引用了新的未被标记的对象B，但由于对象A已经标记过了，不会再重新扫描A对B的可达性，从而将B对象当做垃圾回收掉。&lt;/p&gt;
&lt;p&gt;说实话这种全程STW的GC算法真的是如过街老鼠，人见人打....好家伙，让我程序停下来，专门去做垃圾回收这件事，在追求高性能的今天，很难有人可以接受这种性能损耗。&lt;/p&gt;
&lt;p&gt;所以Golang团队这个时期就开始专注于如何能提升GC的性能，这里希望各位道友能明白Golang团队对GC算法优化的方向是什么，或者目标是什么，那就是让GC和用户程序可以互不干扰，并发进行。所以才有了后面的三色标记法。&lt;/p&gt;
&lt;h3 id=&#34;v15-三色标记法&#34;&gt;v1.5 三色标记法&lt;/h3&gt;
&lt;h4 id=&#34;三色标记法&#34;&gt;三色标记法&lt;/h4&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692760551711.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692760389644.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;3&#34;&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692760560259.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;4&#34;&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692760400648.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;5&#34;&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692760417819.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;对于上述的三色标记法来讲,仍然需要依赖STW的. 因为如果不暂停程序, 程序的逻辑改变对象引用关系, 这种动作如果在标记阶段做了修改，会影响标记结果的正确性。&lt;/p&gt;
&lt;p&gt;看到这里，普通的标记清除法，需要STW，好家伙，这一顿操作猛如虎，咋还是需要STW，性能上没有什么优化啊&lt;br&gt;
Golang是如何解决这个STW问题的呢？&lt;br&gt;
其实总结来看，在三色标记法的过程中对象丢失，需要同时满足下面两个条件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;条件一：&lt;strong&gt;白色对象被黑色对象引用&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;条件二：&lt;strong&gt;灰色对象与白色对象之间的可达关系遭到破坏&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;看来只要把上面两个条件破坏掉一个，就可以保证对象不丢失，所以我们的golang团队就提出了两种破坏条件的方式：&lt;strong&gt;强三色不变式&lt;/strong&gt;和&lt;strong&gt;弱三色不变式&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;两种不变式&#34;&gt;两种不变式&lt;/h3&gt;
&lt;h4 id=&#34;强三色不变式&#34;&gt;强三色不变式&lt;/h4&gt;
&lt;p&gt;规则：不允许黑色对象引用白色对象&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;破坏了条件一： &lt;strong&gt;白色对象被黑色对象引用&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;解释：如果一个黑色对象不直接引用白色对象，那么就不会出现白色对象扫描不到，从而被当做垃圾回收掉的尴尬。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;弱三色不变式&#34;&gt;弱三色不变式&lt;/h4&gt;
&lt;p&gt;规则：黑色对象可以引用白色对象，但是白色对象的上游必须存在灰色对象&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;破坏了条件二：灰色对象与白色对象之间的可达关系遭到破坏&lt;/p&gt;
&lt;p&gt;解释： 如果一个白色对象的上游有灰色对象，则这个白色对象一定可以扫描到，从而不被回收&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;屏障机制&#34;&gt;屏障机制&lt;/h3&gt;
&lt;p&gt;Golang团队遵循上述两种不变式提到的原则，分别提出了两种实现机制：&lt;strong&gt;插入写屏障和删除写屏障&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;插入写屏障&#34;&gt;插入写屏障：&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;规则&lt;/strong&gt;：当一个对象引用另外一个对象时，将另外一个对象标记为灰色。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;满足：强三色不变式。不会存在黑色对象引用白色对象&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这里需要注意一点，插入屏障仅会在堆内存中生效，不对栈内存空间生效，这是因为go在并发运行时，大部分的操作都发生在栈上，函数调用会非常频繁。数十万goroutine的栈都进行屏障保护自然会有性能问题。&lt;/p&gt;
&lt;p&gt;可以发现，对象3在插入写屏障机制下，得到了保护，但是由于栈上的对像没有插入写机制，在扫描完成后，仍然可能存在栈上的白色对象被黑色对象引用，所以在最后需要对栈上的空间进行STW，防止对象误删除。&lt;/p&gt;
&lt;p&gt;对于插入写屏障来讲，需记住，插入写屏障最大的弊端就是，在一次正常的三色标记流程结束后，需要对栈上重新进行一次stw，然后再rescan一次。&lt;/p&gt;
&lt;h4 id=&#34;删除写屏障&#34;&gt;&lt;strong&gt;删除写屏障&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;规则&lt;/strong&gt;：在删除引用时，如果被删除引用的对象自身为灰色或者白色，那么被标记为灰色。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;满足弱三色不变式。灰色对象到白色对象的路径不会断&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;解释&lt;/strong&gt;：白色对象始终会被灰色对象保护&lt;/p&gt;
&lt;p&gt;但是引入删除写屏障，有一个弊端，就是一个对象的引用被删除后，即使没有其他存活的对象引用它，它仍然会活到下一轮。如此一来，会产生很多的冗余扫描成本，且降低了回收精度&lt;/p&gt;
&lt;h4 id=&#34;小结&#34;&gt;小结&lt;/h4&gt;
&lt;p&gt;从上面示例来看，插入写屏障机制和删除写屏障机制中任一机制均可保护对象不被丢失。&lt;strong&gt;在V1.5的版本中采用的是插入写机制实现&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;对比插入写屏障和删除写屏障&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;插入写屏障：
&lt;ul&gt;
&lt;li&gt;插入写屏障哪里都好，就是栈上的操作管不到，所以最后需要对栈空间进行stw保护，然后rescan保证引用的白色对象存活。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;删除写屏障：
&lt;ul&gt;
&lt;li&gt;在GC开始时，会扫描记录整个栈做快照，从而在删除操作时，可以拦截操作，将白色对象置为灰色对象。&lt;/li&gt;
&lt;li&gt;回收精度低。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v18-混合写屏障机制&#34;&gt;v1.8 混合写屏障机制&lt;/h2&gt;
&lt;p&gt;讲到这里，如果是你，你会怎么做呢？当然是取其精华，去其糟泊啦....没错，Golang团队，正是结合了这两点，在v1.8版本下引入了&lt;strong&gt;混合写屏障机制&lt;/strong&gt;。下面我们看下混合屏障机制的核心定义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GC刚开始的时候，会将栈上的可达对象全部标记为黑色&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GC期间，任何在栈上新创建的对象，均为黑色&lt;/strong&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;上面两点只有一个目的，将栈上的可达对象全部标黑，最后无需对栈进行STW，就可以保证栈上的对象不会丢失。有人说，一直是黑色的对象，那么不就永远清除不掉了么，这里强调一下，标记为黑色的是可达对象，不可达的对象一直会是白色，直到最后被回收。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;堆上被删除的对象标记为灰色&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;堆上新添加的对象标记为灰色&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面我们看看混合写屏障机制的示例图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692760953759.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;https://muyuge.github.io/post-images/1692760958749.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;https://muyuge.github.io/post-images/1692760851377.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;https://muyuge.github.io/post-images/1692760858836.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;https://muyuge.github.io/post-images/1692760863576.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;https://muyuge.github.io/post-images/1692760868646.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;提一个问题，万一栈上的对象1引用了堆上的对象8，由于不触发混合写屏障机制，那对象8一直是白色的，最后不就被垃圾回收走了么，谁来保护它？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这个情况是不会发生的，因为一个对象之所以可以引用另外一个对象，它的前提是需要另外一个对象可达，图中的8号显然是不可达的，所以不会出现这种情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为什么1号对象可以引用7号对象呢？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这是因为1号对象在引用7号对象的时候，对象7是在对象6的下游，本身是可达。&lt;/li&gt;
&lt;/ul&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;6&#34;&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692760873377.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Golang v1.3之前采用传统采取标记-清除法，需要STW，暂停整个程序的运行。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在v1.5版本中，引入了三色标记法和插入写屏障机制，其中插入写屏障机制只在堆内存中生效。但在标记过程中，最后需要对栈进行STW。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在v1.8版本中结合删除写屏障机制，推出了混合屏障机制，屏障限制只在堆内存中生效。避免了最后节点对栈进行STW的问题，提升了GC效率&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
">Go实现原理-GC</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-channel/"" data-c="
          &lt;h2 id=&#34;channel的底层数据结构&#34;&gt;channel的底层数据结构&lt;/h2&gt;
&lt;p&gt;channel是golang中用来实现多个goroutine通信的管道，它的底层是一个叫做hchan的结构体。在go的runtime包下。&lt;/p&gt;
&lt;h3 id=&#34;数据结构&#34;&gt;数据结构&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type hchan struct {
  //channel分为无缓冲和有缓冲两种。
  //对于有缓冲的channel存储数据，借助的是如下循环数组的结构
	qcount   uint           // 循环数组中的元素数量
	dataqsiz uint           // 循环数组的长度
	buf      unsafe.Pointer // 指向底层循环数组的指针
	elemsize uint16 //能够收发元素的大小
  

	closed   uint32   //channel是否关闭的标志
	elemtype *_type //channel中的元素类型
  
  //有缓冲channel内的缓冲数组会被作为一个“环型”来使用。
  //当下标超过数组容量后会回到第一个位置，所以需要有两个字段记录当前读和写的下标位置
	sendx    uint   // 下一次发送数据的下标位置
	recvx    uint   // 下一次读取数据的下标位置
  
  //当循环数组中没有数据时，收到了接收请求，那么接收数据的变量地址将会写入读等待队列
  //当循环数组中数据已满时，收到了发送请求，那么发送数据的变量地址将写入写等待队列
	recvq    waitq  // 读等待队列
	sendq    waitq  // 写等待队列


	lock mutex //互斥锁，保证读写channel时不存在并发竞争问题
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;对应图解如下：&lt;/strong&gt;&lt;br&gt;
&lt;img src=&#34;https://muyuge.github.io/post-images/1692611815206.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
总结hchan结构体的主要组成部分有四个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用来保存goroutine之间传递数据的循环链表。=====&amp;gt; buf。&lt;/li&gt;
&lt;li&gt;用来记录此循环链表当前发送或接收数据的下标值。=====&amp;gt; sendx和recvx。&lt;/li&gt;
&lt;li&gt;用于保存向该chan发送和从改chan接收数据的goroutine的队列。=====&amp;gt; sendq 和 recvq&lt;/li&gt;
&lt;li&gt;保证channel写入和读取数据时线程安全的锁。 =====&amp;gt; lock&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;举个栗子&#34;&gt;举个栗子&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;//G1
func sendTask(taskList []Task) {
	...
  
	ch:=make(chan Task, 4) // 初始化长度为4的channel
	for _,task:=range taskList {
		ch &amp;lt;- task  //发送任务到channel
	}
  
	...
}

//G2
func handleTask(ch chan Task) {
	for {
		task:= &amp;lt;-ch //接收任务
		process(task) //处理任务
	}
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ch是长度为4的带缓冲的channel，G1是发送者，G2是接收者&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始hchan结构体重的buf为空，sendx和recvx均为0。&lt;/li&gt;
&lt;li&gt;当G1向ch里发送数据时，首先会对buf加锁，然后&lt;strong&gt;将数据copy到buf中&lt;/strong&gt;，然后sendx++，然后释放对buf的锁。&lt;/li&gt;
&lt;li&gt;当G2消费ch的时候，会首先对buf加锁，然后将buf中的&lt;strong&gt;数据copy到task变量对应的内存里&lt;/strong&gt;，然后recvx++,并释放锁。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以发现整个过程，G1和G2没有共享的内存，&lt;strong&gt;底层是通过hchan结构体的buf，并使用copy内存的方式进行通信，最后达到了共享内存的目的&lt;/strong&gt;，这里也体现了Go中的CSP并发模型。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Go中的CSP并发模型即是通过goroutine和channel实现的。&lt;/p&gt;
&lt;p&gt;CSP并发模型：不要以共享内存的方式来通信，相反，要通过通信的方式来共享内存。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;那么当channel中的缓存满了之后会发生什么呢？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;首先简单了解下&lt;strong&gt;GMP的概念&lt;/strong&gt;。相关的数据模型如下图:&lt;br&gt;
&lt;img src=&#34;https://muyuge.github.io/post-images/1692611847063.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
【G】goroutine是Golang实现的用户空间的轻量级的线程&lt;/p&gt;
&lt;p&gt;【M】代表操作系统线程&lt;/p&gt;
&lt;p&gt;【P】处理器, 它包含了待运行goroutine。&lt;/p&gt;
&lt;p&gt;如果线程M想运行goroutine，必须先获取P，从P中获取goroutine执行。&lt;/p&gt;
&lt;p&gt;当G1向buf已经满了的ch发送数据的时候，检测到hchan的buf已经满了，会通知调度器，调度器会将G1的状态设置为waiting, 并移除与线程M的联系，然后从P的runqueue中选择一个goroutine在线程M中执行，此时G1就是阻塞状态，但是不是操作系统的线程阻塞，所以这个时候只用消耗少量的资源。&lt;br&gt;
&lt;img src=&#34;https://muyuge.github.io/post-images/1692611871893.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
那么G1设置为waiting状态后去哪了？怎们去resume呢？我们再回到hchan结构体，注意到hchan有个sendq的成员，其类型是waitq，查看源码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type hchan struct {
	...
  recvq    waitq  // 读等待队列
	sendq    waitq  // 写等待队列
	...
}

type waitq struct {
	first *sudog
	last *sudog
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;实际上，当G1变为waiting状态后，会创建一个代表自己的sudog的结构，然后放到sendq这个list中，sudog结构中保存了channel相关的变量的指针(如果该Goroutine是sender，那么保存的是待发送数据的变量的地址，如果是receiver则为接收数据的变量的地址，之所以是地址，前面我们提到在传输数据的时候使用的是copy的方式)&lt;/p&gt;
&lt;p&gt;当G2从ch中接收一个数据时，会通知调度器，设置G1的状态为runnable，然后将加入P的runqueue里，等待线程执行.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func G2(){
  t := &amp;lt;-ch
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692611969159.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
前面我们是假设G1先运行，如果G2先运行会怎么样呢？&lt;/p&gt;
&lt;p&gt;如果G2先运行，那么G2会从一个empty的channel里取数据，这个时候G2就会阻塞，和前面介绍的G1阻塞一样，G2也会创建一个sudog结构体，保存接收数据的变量的地址，但是该sudog结构体是放到了recvq列表里。&lt;br&gt;
&lt;img src=&#34;https://muyuge.github.io/post-images/1692611996474.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
当G1向ch发送数据的时候，为了提升效率，&lt;strong&gt;runtime并不会对hchan结构体题的buf进行加锁，而是直接将G1里的发送到ch的数据copy到了G2 sudog里对应的elem指向的内存地址！【不通过buf】&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这时候，张三道友抛出了三个问题:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;为什么在第一种情况下，即G1向缓存满的channel中发送数据时被阻塞。在G2后来接收时，不将阻塞的G1发送的数据直接拷贝到G2中呢？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这是因为channel中的数据是队列的，遵循先进先出的原则，当有消费者G2接收数据时，需要先接收缓存中的数据，即buf中的数据，而不是直接消费阻塞的G1中的数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多个goroutine向有缓存的channel接收/发送数据时，可以保证顺序吗？&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main(){
	cache:=make(chan int,3)
	go func() {
		for i:=0;i&amp;lt; 3;i++ {
		cache&amp;lt;-i
	}
	}()
	time.Sleep(time.Second)
  //休眠1秒钟，保证channel中的数据已经写入完整
	go getCache(&amp;quot;gorouine1&amp;quot;,cache)
	go getCache(&amp;quot;gorouine2&amp;quot;,cache)
	go getCache(&amp;quot;gorouine3&amp;quot;,cache)
	time.Sleep(time.Second)
}

func getCache(routine string,cache &amp;lt;-chan int) {
	for  {
		select {
		case i:=&amp;lt;-cache:
			fmt.Printf(&amp;quot;%s:%d\n&amp;quot;,routine,i)
		}
	}
}

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;很多道友在工作中应用channel时遇到上述场景都会默认为是有序的，即认为输出结果应该是：

```bash
gorouine1:0
gorouine2:1
gorouine3:2
```

但实则不然，输出结果如下：

```bash
$go run main.go
gorouine3:1
gorouine2:2
gorouine1:0
```

这里其实主要需要明确两点：

- channel中的数据遵循队列先进先出原则。
- 每一个goroutine抢到处理器的时间点不一致，gorouine的执行本身不能保证顺序。

即代码中先写的gorouine并不能保证先从channel中获取数据，或者发送数据。但是先执行的gorouine与后执行的goroutine在channel中获取的数据肯定是有序的。
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Channel为什么是线程安全的？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在对buf中的数据进行入队和出队操作时，为当前chnnel使用了互斥锁，防止多个线程并发修改数据&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;channel的用法&#34;&gt;channel的用法&lt;/h2&gt;
&lt;h3 id=&#34;使用for-range-读取channel&#34;&gt;使用for range 读取channel&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;for i := range ch{
    fmt.Println(i)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;场景：当需要不断从channel读取数据时&lt;/li&gt;
&lt;li&gt;原理：使用&lt;code&gt;for-range&lt;/code&gt;读取channel，这样既安全又便利，当channel关闭时，for循环会自动退出，无需主动监测channel是否关闭，可以防止读取已经关闭的channel，造成读到数据为通道所存储的数据类型的零值。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;使用_ok判断channel是否关闭&#34;&gt;使用_,ok判断channel是否关闭&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;if v, ok := &amp;lt;- ch; ok {
    fmt.Println(v)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;场景：读channel，但不确定channel是否关闭时&lt;/li&gt;
&lt;li&gt;原理：读已关闭的channel会得到零值，如果不确定channel，需要使用&lt;code&gt;ok&lt;/code&gt;进行检测。ok的结果和含义：
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;true&lt;/code&gt;：读到数据，并且通道没有关闭。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;false&lt;/code&gt;：通道关闭，无数据读到。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;使用select处理多个channel&#34;&gt;使用select处理多个channel&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;for{
  select {
  case &amp;lt;-ch1:
  	process1()
  	return
  case &amp;lt;-ch2:
  	process2()
  	return
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;场景：需要对多个通道进行同时处理，但只处理最先发生的channel时&lt;/li&gt;
&lt;li&gt;原理：&lt;code&gt;select&lt;/code&gt;可以同时监控多个通道的情况，只处理未阻塞的case。
&lt;ul&gt;
&lt;li&gt;当通道为nil时，对应的case永远为阻塞。&lt;/li&gt;
&lt;li&gt;如果channel已经关闭，则这个case是非阻塞的，每次select都可能会被执行到。&lt;/li&gt;
&lt;li&gt;如果多个channel都处于非阻塞态，则select会随机选择一个执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;关闭channel的注意事项&#34;&gt;关闭channel的注意事项&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;一个 channel不能多次关闭，会导致painc&lt;/li&gt;
&lt;li&gt;向一个已经关闭了的 channel发送数据会导致panic&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;面试点总结&#34;&gt;面试点总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Go channel的底层数据结构以及工作原理？&lt;/li&gt;
&lt;li&gt;向缓存满的channel写数据会发生什么？&lt;/li&gt;
&lt;li&gt;向没有数据的channel读数据会发生什么？&lt;/li&gt;
&lt;li&gt;简述channel的日常用法？&lt;/li&gt;
&lt;/ul&gt;
">Go实现原理-Channel</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-mutex/"" data-c="
          &lt;p&gt;在Go中，主要实现了两种锁：sync.Mutex(互斥锁) 以及 sync.RWMutex(读写锁)。&lt;/p&gt;
&lt;p&gt;本篇主要介绍sync.Mutex的使用和实现原理。&lt;/p&gt;
&lt;h2 id=&#34;为什么需要锁&#34;&gt;为什么需要锁&lt;/h2&gt;
&lt;p&gt;在高并发下或多goroutine同时执行下，可能会同时读写同一块内存，比如如下场景：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var count int
var mu sync.Mutex

func func1() {
    for i := 0; i &amp;lt; 1000; i++ {
        go func() {
            count = count + 1
        }()
    }
    time.Sleep(time.Second)
    fmt.Println(count)
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出的值预期是1000，实际是 948，965等，多次运行结果不一致。&lt;/p&gt;
&lt;p&gt;之所以出现这样的现象，是因为对于count=count+1来讲，每个goroutine执行步骤为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;读取当前count值&lt;/li&gt;
&lt;li&gt;count+1&lt;/li&gt;
&lt;li&gt;修改count值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当多个goroutine同时执行修改数值时，后面执行的goroutine会把前面goroutine对count的修改覆盖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;在Go中对于并发程序进行公共资源的访问的限制最常用的就是互斥锁（sync.mutex）的方式&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;sync.mutex的常用方法有两个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mutex.lock()用来获取锁&lt;/li&gt;
&lt;li&gt;Mutex.Unlock()用于释放锁&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 Lock 和 Unlock 方法之间的代码段称为资源的临界区，这一区间的代码是严格被锁保护的，是线程安全的，任何一个时间点最多只能有一个goroutine在执行。&lt;/p&gt;
&lt;p&gt;基于此，上面的示例可以采用sync.mutex来改进：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var count int
var mutex sync.Mutex

func func2() {
    for i := 0; i &amp;lt; 1000; i++ {
        go func() {
            mutex.Lock()
            count = count + 1
            mutex.Unlock()
        }()
    }
    time.Sleep(time.Second)
    fmt.Println(count)
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出结果为1000。&lt;/p&gt;
&lt;p&gt;当某一goroutine执行了mutex.lock()方法后，如果有其他的goroutine来执行上锁操作，会被阻塞，直到当前的goroutine执行mutex.unlock()方法释放锁后其他的goroutine才会继续抢锁执行。&lt;/p&gt;
&lt;h2 id=&#34;实现原理&#34;&gt;实现原理&lt;/h2&gt;
&lt;h3 id=&#34;syncmutex的数据结构&#34;&gt;sync.Mutex的数据结构&lt;/h3&gt;
&lt;p&gt;Go中的sync.Mutex的结构体为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;type Mutex struct {
    state int32
    sema  uint32
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Sync.Mutex由两个字段构成，state用来表示当前互斥锁处于的状态，sema用于控制锁状态的信号量&lt;/strong&gt;。相信各位道友读完这两个字段的描述后，好像懂了，又好像没懂。下面我们详细理解下这两个字段到底都作了哪些事。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;互斥锁state主要记录了如下四种状态：&lt;/strong&gt;&lt;br&gt;
&lt;img src=&#34;https://muyuge.github.io/post-images/1692611606521.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;waiter_num：&lt;/strong&gt; 记录了当前等待抢这个锁的goroutine数量&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;starving：&lt;/strong&gt; 当前锁是否处于&lt;strong&gt;饥饿状态&lt;/strong&gt; (后文会详解锁的饥饿状态) 0: 正常状态 1: 饥饿状态&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;woken：&lt;/strong&gt; 当前锁是否有goroutine已被唤醒。 0：没有goroutine被唤醒； 1: 有goroutine正在加锁过程&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;locked：&lt;/strong&gt; 当前锁是否被goroutine持有。 0: 未被持有 1: 已被持有&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;sema信号量的作用：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当持有锁的gorouine释放锁后，会释放sema信号量，这个信号量会唤醒之前抢锁阻塞的gorouine来获取锁。&lt;/p&gt;
&lt;h3 id=&#34;锁的两种模式&#34;&gt;锁的两种模式&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;互斥锁在设计上主要有两种模式： 正常模式和饥饿模式&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;之所以引入了饥饿模式，是为了保证goroutine获取互斥锁的公平性。所谓公平性，其实就是多个goroutine在获取锁时，goroutine获取锁的顺序，和请求锁的顺序一致，则为公平。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;正常模式&lt;/strong&gt;下，所有阻塞在等待队列中的goroutine会按顺序进行锁获取，当唤醒一个等待队列中的goroutine时，此goroutine并不会直接获取到锁，而是会和新请求锁的goroutine竞争。 通常新请求锁的goroutine更容易获取锁，这是因为新请求锁的goroutine正在占用cpu片执行，大概率可以直接执行到获取到锁的逻辑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;饥饿模式&lt;/strong&gt;下， 新请求锁的goroutine不会进行锁获取，而是加入到队列尾部阻塞等待获取锁。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;饥饿模式的触发条件：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当一个goroutine等待锁的时间超过1ms时，互斥锁会切换到饥饿模式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;饥饿模式的取消条件：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当获取到锁的这个goroutine是等待锁队列中的最后一个goroutine，互斥锁会切换到正常模式&lt;/li&gt;
&lt;li&gt;当获取到锁的这个goroutine的等待时间在1ms之内，互斥锁会切换到正常模式&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;注意事项&#34;&gt;注意事项&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;在一个goroutine中执行Lock()加锁成功后，不要再重复进行加锁，否则会panic。&lt;/li&gt;
&lt;li&gt;在Lock() 之前 执行Unlock()释放锁 会panic&lt;/li&gt;
&lt;li&gt;对于同一把锁，可以在一个goroutine中执行Lock加锁成功后，可以在另外一个gorouine中执行Unlock释放锁。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;面试点总结&#34;&gt;面试点总结&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;sync.Mutex的底层结构？&lt;/li&gt;
&lt;li&gt;锁的饥饿模式？&lt;/li&gt;
&lt;/ol&gt;
">Go实现原理-Mutex</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-gmp-mo-xing/"" data-c="
          &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Go 调度器模型我们通常叫做&lt;strong&gt;G-P-M 模型&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;G、P、M 是 Go 调度器的三个核心组件，是 Go 语言天然支持高并发的内在动力&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;线程&#34;&gt;线程&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;操作系统最小调度单元，通常语义中的线程指的是内核级线程&lt;/li&gt;
&lt;li&gt;创建、销毁、调度交由内核完成，CPU 需完成用户态与内核态间的切换&lt;/li&gt;
&lt;li&gt;可以更好地使用多道程序并发执行，提高资源利用率和系统吞吐量&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;协程&#34;&gt;协程&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;定义上是一种用户态的轻量级线程&lt;/li&gt;
&lt;li&gt;调度完全由用户控制，协程间切换只需要保存任务的上下文，没有内核的开销&lt;/li&gt;
&lt;li&gt;与线程存在映射关系，为 M:1&lt;/li&gt;
&lt;li&gt;因为线程是最小调度单元，在内核视角下根本不知道有协程这个概念，所以协程是无法进行真正意义上的并行的。且一个协程由于某些原因发生了阻塞，有可能上升至线程阻塞，因为内核只能看到线程发生问题，而看不到协程。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;goroutine&#34;&gt;Goroutine&lt;/h2&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/cqkxmvbRVFoXuGa.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;在 Go 语言中进行优化后的特殊“协程”&lt;/li&gt;
&lt;li&gt;与线程存在映射关系，为 M:N&lt;/li&gt;
&lt;li&gt;创建、销毁、调度在用户态实现，对内核透明，足够轻巧&lt;/li&gt;
&lt;li&gt;可利用多个线程实现并行&lt;/li&gt;
&lt;li&gt;通过调度器的擀旋，实现和线程间的动态绑定和灵活调度&lt;/li&gt;
&lt;li&gt;栈空间大小可动态扩缩，因地制宜&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;对比&#34;&gt;对比&lt;/h2&gt;
&lt;p&gt;​ 来一波&lt;sub&gt;雷军比较法&lt;/sub&gt;：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;th&gt;弱依赖内核&lt;/th&gt;
&lt;th&gt;可并行&lt;/th&gt;
&lt;th&gt;可应对阻塞&lt;/th&gt;
&lt;th&gt;栈可动态扩缩&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;线程&lt;/td&gt;
&lt;td&gt;❎&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;❎&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;协程&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;❎&lt;/td&gt;
&lt;td&gt;❎&lt;/td&gt;
&lt;td&gt;❎&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Goroutine&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;​ 有主角光环的 Goroutine 毫无悬念的胜出了，下面一起来了解一下 Golang 调度 Goroutine 时的经典模型：GPM 模型。&lt;/p&gt;
&lt;p&gt;​ Go 调度器模型我们通常叫做 GPM 模型~~（记忆窍门：Mai Pi Gu）~~，也有叫 GMP 模型的，讲的是同一个东西。它由 &lt;strong&gt;Goroutine&lt;/strong&gt; 、&lt;strong&gt;Processor&lt;/strong&gt; 、&lt;strong&gt;Machine&lt;/strong&gt; 、sched 组成，sched 就是 Go 的调度器，它维护有存储 M 和 G 的队列以及调度器的一些状态信息等。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/Pm1MOrtwYUx4jaB.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h2 id=&#34;g&#34;&gt;G&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;G 即 Goroutine ，Golang 中对协程的抽象&lt;/li&gt;
&lt;li&gt;G 有自己的运行栈、状态、执行的任务函数，用户通过关键字 Go 启动一个 goroutine&lt;/li&gt;
&lt;li&gt;G 需要绑定 P 才能执行，在 G 的视角中，P 就是它的 CPU&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;p&#34;&gt;P&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;P 即 Processor ，只是一个抽象的概念，并不是真正的物理 CPU&lt;/li&gt;
&lt;li&gt;是 GPM 的中枢，借由 P 承上启下，实现 G 和 M 之间的动态结合&lt;/li&gt;
&lt;li&gt;对 G 而言，P 是它的 CPU ，G 只有被 P 调度才能执行&lt;/li&gt;
&lt;li&gt;对 M 而言，P 是其执行代理，为其提供必要信息的同时（可执行的 G 、内存分配情况等）还隐藏了繁杂的调度细节&lt;/li&gt;
&lt;li&gt;P 的数量决定了 G 最大并行数量，可由用户通过 GOMAXPROCS 进行设定（超过 CPU 核数时无意义）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;m&#34;&gt;M&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;M 即 Machine ，是 Golang 中对协程的抽象&lt;/li&gt;
&lt;li&gt;M 不直接执行 G ，而是先和 P 绑定，由其实现代理&lt;/li&gt;
&lt;li&gt;借由 P 的存在，M 无需和 G 绑死，也无需记录 G 的状态信息，因此 G 在全生命周期中可以实现跨 M 执行&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;任务窃取&#34;&gt;任务窃取&lt;/h2&gt;
&lt;p&gt;​ 在实际生产中，每个 P 中的 G 执行速度有快有慢，且 G 的个数有多有少。为了充分提高 Go 的并行处理能力，当每个 P 之间的 G 任务不均衡时，调度器允许 P 从全局队列或其它 P 的本地队列中获取 G 执行。&lt;/p&gt;
&lt;h2 id=&#34;减少阻塞&#34;&gt;减少阻塞&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;由于原子、互斥量或通道操作调用导致 G 阻塞，调度器将把当前阻塞的 G 切换出去，重新调度本地队列上的其他 G&lt;/li&gt;
&lt;li&gt;由于网络请求和 IO 操作导致 G 阻塞，Go 提供了网络轮询器，通过 ipoll 实现 IO 多路复用来解决这一问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;go-func-调度流程&#34;&gt;go func() 调度流程&lt;/h2&gt;
&lt;p&gt;即：如果处理器没有任务可处理，它会按以下规则来执行，直到满足某一条规则：&lt;/p&gt;
&lt;p&gt;从本地队列获取任务 从全局队列获取任务 从网络轮询器获取任务 从其它的处理器的本地队列窃取任务&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;3&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/A1VLhEQYwapDzTv.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h2 id=&#34;调度器的生命周期&#34;&gt;调度器的生命周期&lt;/h2&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;4&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/s5X1FuBJIEUq2LK.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;M0&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;M0 是启动程序后的编号为 0 的主线程，这个 M 对应的实例会在全局变量 &lt;code&gt;runtime.m0&lt;/code&gt; 中，不需要在栈上分配，M0 负责执行初始化操作和启动第一个 G ， 在之后 M0 就和其他的 M 一样了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;G0&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;G0 是每次启动一个 M 都会第一个创建的 gourtine ，G0 仅用于负责调度的 G ，G0 不指向任何可执行的函数，每个 M 都会有一个自己的 G0 。在调度或系统调用时会使用 G0 的栈空间，全局变量的 G0 是 M0 的 G0 。&lt;/p&gt;
">Go实现原理-GMP模型</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/go-shi-xian-yuan-li-map/"" data-c="
          &lt;h2 id=&#34;map的数据结构&#34;&gt;map的数据结构&lt;/h2&gt;
&lt;p&gt;首先先列出源码结构关键字段，实现在 src/runtime/map.go：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type hmap struct {
	count     int    // 元素的个数
	B         uint8  // buckets 数组的长度就是 2^B 个
	overflow uint16 // 溢出桶的数量

	buckets    unsafe.Pointer // 2^B个桶对应的数组指针
	oldbuckets unsafe.Pointer  // 发生扩容时，记录扩容前的buckets数组指针

	extra *mapextra //用于保存溢出桶的地址
}

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;type mapextra struct {
	overflow    *[]*bmap
	oldoverflow *[]*bmap

	nextOverflow *bmap
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;type bmap struct {
	tophash [bucketCnt]uint8
}

//在编译期间会产生新的结构体
type bmap struct {
    tophash [8]uint8 //存储哈希值的高8位
    data    byte[1]  //key value数据:key/key/key/.../value/value/value...
    overflow *bmap   //溢出bucket的地址
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;便于理解源码的结构，提炼关键字段并转换为图形模式：&lt;br&gt;
&lt;img src=&#34;https://muyuge.github.io/post-images/1692610223168.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
在go的map实现中，它的底层结构体是hmap，hmap里维护着若干个bucket数组 (即桶数组)。&lt;br&gt;
Bucket数组中每个元素都是bmap结构，也即每个bucket（桶）都是bmap结构，【ps：后文为了语义一致，和方便理解，就不再提bmap了，统一叫作桶】 每个桶中保存了8个kv对，如果8个满了，又来了一个key落在了这个桶里，会使用overflow连接下一个桶(溢出桶)。&lt;/p&gt;
&lt;h2 id=&#34;map中数据操作&#34;&gt;map中数据操作&lt;/h2&gt;
&lt;p&gt;了解了map的数据结构后，下面让我们学习一下在map中存取数据的过程：&lt;/p&gt;
&lt;h3 id=&#34;get获取数据&#34;&gt;GET获取数据&lt;/h3&gt;
&lt;p&gt;假设当前 B=4  即桶数量为2^B=16个，要从map中获取k4对应的value&lt;br&gt;
&lt;img src=&#34;https://muyuge.github.io/post-images/1692610302859.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
参考上图，k4的get流程可以归纳为如下几步：&lt;br&gt;
①计算k4的hash值。[由于当前主流机都是64位操作系统，所以计算结果有64个比特位]&lt;br&gt;
②通过最后的“B”位来确定在哪号桶，此时B为4，所以取k4对应哈希值的后4位，也就是0101，0101用十进制表示为5，所以在5号桶）&lt;br&gt;
③根据k4对应的hash值前8位快速确定是在这个桶的哪个位置（额外说明一下，在bmap中存放了每个key对应的tophash，是key的哈希值前8位),一旦发现前8位一致，则会执行下一步&lt;br&gt;
④对比key完整的hash是否匹配，如果匹配则获取对应value&lt;br&gt;
⑤如果都没有找到，就去连接的下一个溢出桶中找&lt;br&gt;
有很多同学会问这里为什么要多维护一个tophash，即hash前8位？&lt;br&gt;
这是因为tophash可以快速确定key是否正确，也可以把它理解成一种缓存措施，如果前8位都不对了，后面就没有必要比较了。&lt;/p&gt;
&lt;h3 id=&#34;put存放数据&#34;&gt;PUT存放数据&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692610343139.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
map的赋值流程可总结位如下几步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;通过key的hash值后“B”位确定是哪一个桶&lt;/strong&gt;，图中示例为4号桶。&lt;/li&gt;
&lt;li&gt;遍历当前桶，通过key的tophash和hash值，防止key重复，然后找到第一个可以插入的位置，即空位置处存储数据。&lt;/li&gt;
&lt;li&gt;如果&lt;strong&gt;当前桶元素已满，会通过overflow链接创建一个新的桶&lt;/strong&gt;，来存储数据。&lt;br&gt;
&lt;strong&gt;关于hash冲突&lt;/strong&gt;：当两个不同的 key 落在同一个桶中，就是发生了哈希冲突。冲突的解决手段是采用链表法：在 桶 中，从前往后找到第一个空位进行插入。如果8个kv满了，那么当前桶就会连接到下一个溢出桶（bmap）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;扩容&#34;&gt;扩容&lt;/h2&gt;
&lt;h3 id=&#34;扩容的方式&#34;&gt;扩容的方式&lt;/h3&gt;
&lt;h4 id=&#34;相同容量扩容&#34;&gt;相同容量扩容&lt;/h4&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692610377777.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;由于map中不断的put和delete key，桶中可能会出现很多断断续续的空位，这些空位会导致连接的bmap溢出桶很长，导致扫描时间边长。这种扩容实际上是一种整理，把后置位的数据整理到前面。这种情况下，元素会发生重排，但不会换桶。&lt;/p&gt;
&lt;h4 id=&#34;2倍容量扩容&#34;&gt;2倍容量扩容&lt;/h4&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://muyuge.github.io/post-images/1692610406024.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;这种2倍扩容是由于当前桶数组确实不够用了，发生这种扩容时，元素会重排，可能会发生桶迁移。&lt;br&gt;
如图中所示，扩容前B=2,扩容后B=3，假设一元素key的hash值后三位为101，那么由上文的介绍可知，在扩容前，由hash值的后两位来决定几号桶，即 01 所以元素在1号桶。 在扩容发生后，由hash值得后三位来决定几号桶，即101所以元素会迁移到5号桶。&lt;/p&gt;
&lt;h3 id=&#34;扩容的条件&#34;&gt;扩容的条件&lt;/h3&gt;
&lt;p&gt;首先我们了解下**装载因子(loadFactor)**的概念&lt;br&gt;
loadFactor:=count / (2^B)  即 装载因子 = map中元素的个数 / map中当前桶的个数&lt;br&gt;
通过计算公式我们可以得知，装载因子是指当前map中，每个桶中的平均元素个数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;扩容条件1：&lt;strong&gt;装载因子 &amp;gt; 6.5&lt;/strong&gt;(源码中定义的)&lt;br&gt;
这个也非常容易理解，正常情况下，如果没有溢出桶，那么一个桶中最多有8个元素，当平均每个桶中的数据超过了6.5个，那就意味着当前容量要不足了，发生扩容。&lt;/li&gt;
&lt;li&gt;扩容条件2:  溢出桶的数量过多&lt;br&gt;
当 B &amp;lt; 15 时，如果overflow的bucket数量超过 2^B。&lt;br&gt;
当 B &amp;gt;= 15 时，overflow的bucket数量超过 2^15。&lt;br&gt;
简单来讲，新加入key的hash值后B位都一样，使得个别桶一直在插入新数据，进而导致它的溢出桶链条越来越长。如此一来，当map在操作数据时，扫描速度就会变得很慢。及时的扩容，可以对这些元素进行重排，使元素在桶的位置更平均一些。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;扩容时的细节&#34;&gt;扩容时的细节&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;在我们的hmap结构中有一个oldbuckets吗，扩容刚发生时，会先将老数据存到这个里面。&lt;/li&gt;
&lt;li&gt;每次对map进行删改操作时，会触发从oldbucket中迁移到bucket的操作【非一次性，分多次】&lt;/li&gt;
&lt;li&gt;在扩容没有完全迁移完成之前，每次get或者put遍历数据时，都会先遍历oldbuckets，然后再遍历buckets。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;注意事项&#34;&gt;注意事项&lt;/h2&gt;
&lt;h3 id=&#34;对map数据进行操作时不可取地址&#34;&gt;对map数据进行操作时不可取地址。&lt;/h3&gt;
&lt;p&gt;常见的错误用例如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;type Student struct {
	Name string
	Age  int
}

func f1() {
	m := map[int]Student{
		1: Student{Age: 15, Name: &amp;quot;jack&amp;quot;},
		2: Student{Age: 16, Name: &amp;quot;danny&amp;quot;},
		3: Student{Age: 17, Name: &amp;quot;andy&amp;quot;},
	}
	m[1].Name = &amp;quot;JACK&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这种情况会发生编译错误，因为map元素是无法取址的，也就是说，你可以得到m[1]，但是无法对它的值作出任何修改。如果你想修改value，可以使用带指针的value，如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;func f2() {
	m := map[int]*Student{
		1: &amp;amp;Student{Age: 15, Name: &amp;quot;jack&amp;quot;},
		2: &amp;amp;Student{Age: 16, Name: &amp;quot;danny&amp;quot;},
		3: &amp;amp;Student{Age: 17, Name: &amp;quot;andy&amp;quot;},
	}
	m[1].Name = &amp;quot;JACK&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;不知道各位道友有没有疑惑，为什么go中要禁止对map的元素进行取址呢？这是因为map 会随着元素数量的增长而重新分配更大的内存空间，会导致之前的地址无效。&lt;/p&gt;
&lt;h3 id=&#34;map是线程不安全的&#34;&gt;map是线程不安全的&lt;/h3&gt;
&lt;p&gt;在同一时间点，两个 goroutine 对同一个map进行读写操作是不安全的。举个栗子：&lt;br&gt;
某map桶数量为4，即B=2。此时  goroutine1来插入key1， goroutine2来读取 key2. 可能会发生如下过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;goroutine2 计算key2的hash值,B=2，并确定桶号为1。&lt;/li&gt;
&lt;li&gt;goroutine1添加key1，触发扩容条件。&lt;/li&gt;
&lt;li&gt;B=B+1=3, buckets数据迁移到oldbuckets。&lt;/li&gt;
&lt;li&gt;goroutine2从桶1中遍历，获取数据失败。&lt;br&gt;
在工作中，当我们涉及到对一个map进行并发读写时，一般采用的做法是采用golang中自带的mutex锁&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;type Resource struct {
	sync.RWMutex
	m map[string]int
}

func main() {
	r := Resource{m: make(map[string]int)}

	go func() { //开一个goroutine写map
		for j := 0; j &amp;lt; 100; j++ {
			r.Lock()
			r.m[fmt.Sprintf(&amp;quot;resource_%d&amp;quot;, j)] = j
			r.Unlock()
		}
	}()
	go func() { //开一个goroutine读map
		for j := 0; j &amp;lt; 100; j++ {
			r.RLock()
			fmt.Println(r.m[fmt.Sprintf(&amp;quot;resource_%d&amp;quot;, j)])
			r.RUnlock()
		}
	}()
}
&lt;/code&gt;&lt;/pre&gt;
">Go实现原理-Map</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/cpu-huan-cun-yi-zhi-xing/"" data-c="
          &lt;hr&gt;
&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://gitee.com/fxiang930/muyuge/raw/master/20220312193703.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;正文&#34;&gt;正文&lt;/h2&gt;
&lt;h3 id=&#34;cpu-cache-的数据写入&#34;&gt;CPU Cache 的数据写入&lt;/h3&gt;
&lt;p&gt;随着时间的推移，CPU 和内存的访问性能相差越来越大，于是就在 CPU 内部嵌入了 CPU Cache（高速缓存），CPU Cache 离 CPU 核心相当近，因此它的访问速度是很快的，于是它充当了 CPU 与内存之间的缓存角色。&lt;/p&gt;
&lt;p&gt;CPU Cache 通常分为三级缓存：L1 Cache、L2 Cache、L3 Cache，级别越低的离 CPU 核心越近，访问速度也快，但是存储容量相对就会越小。其中，在多核心的 CPU 里，每个核心都有各自的 L1/L2 Cache，而 L3 Cache 是所有核心共享使用的。&lt;br&gt;
&lt;img src=&#34;https://gitee.com/fxiang930/muyuge/raw/master/20220312193704.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;我们先简单了解下 CPU Cache 的结构，CPU Cache 是由很多个 Cache Line 组成的，CPU Line 是 CPU 从内存读取数据的基本单位，而 CPU Line 是由各种标志（Tag）+ 数据块（Data Block）组成，你可以在下图清晰的看到：&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://gitee.com/fxiang930/muyuge/raw/master/20220312193706.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;我们当然期望 CPU 读取数据的时候，都是尽可能地从 CPU Cache 中读取，而不是每一次都要从内存中获取数据。所以，身为程序员，我们要尽可能写出缓存命中率高的代码，这样就有效提高程序的性能。&lt;/p&gt;
&lt;p&gt;事实上，数据不光是只有读操作，还有写操作，那么如果数据写入 Cache 之后，内存与 Cache 相对应的数据将会不同，这种情况下 Cache 和内存数据都不一致了，于是我们肯定是要把 Cache 中的数据同步到内存里的。&lt;/p&gt;
&lt;p&gt;问题来了，那在什么时机才把 Cache 中的数据写回到内存呢？为了应对这个问题，下面介绍两种针对写入数据的方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;写直达（&lt;em&gt;Write Through&lt;/em&gt;）&lt;/li&gt;
&lt;li&gt;写回（&lt;em&gt;Write Back&lt;/em&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;写直达&#34;&gt;写直达&lt;/h4&gt;
&lt;p&gt;保持内存与 Cache 一致性最简单的方式是，&lt;strong&gt;把数据同时写入内存和 Cache 中&lt;/strong&gt;，这种方法称为&lt;strong&gt;写直达（&lt;em&gt;Write Through&lt;/em&gt;）&lt;/strong&gt;。&lt;br&gt;
&lt;img src=&#34;https://gitee.com/fxiang930/muyuge/raw/master/20220312193700.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;在这个方法里，写入前会先判断数据是否已经在 CPU Cache 里面了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果数据已经在 Cache 里面，先将数据更新到 Cache 里面，再写入到内存里面；&lt;/li&gt;
&lt;li&gt;如果数据没有在 Cache 里面，就直接把数据更新到内存里面。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;写直达法很直观，也很简单，但是问题明显，无论数据在不在 Cache 里面，每次写操作都会写回到内存，这样写操作将会花费大量的时间，无疑性能会受到很大的影响。&lt;/p&gt;
&lt;h4 id=&#34;写回&#34;&gt;写回&lt;/h4&gt;
&lt;p&gt;既然写直达由于每次写操作都会把数据写回到内存，而导致影响性能，于是为了要减少数据写回内存的频率，就出现了&lt;strong&gt;写回（&lt;em&gt;Write Back&lt;/em&gt;）的方法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在写回机制中，&lt;strong&gt;当发生写操作时，新的数据仅仅被写入 Cache Block 里，只有当修改过的 Cache Block「被替换」时才需要写到内存中&lt;/strong&gt;，减少了数据写回内存的频率，这样便可以提高系统的性能。&lt;br&gt;
&lt;img src=&#34;https://gitee.com/fxiang930/muyuge/raw/master/20220312193658.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;那具体如何做到的呢？下面来详细说一下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果当发生写操作时，数据已经在 CPU Cache 里的话，则把数据更新到 CPU Cache 里，同时标记 CPU Cache 里的这个 Cache Block 为脏（Dirty）的，这个脏的标记代表这个时候，我们 CPU Cache 里面的这个 Cache Block 的数据和内存是不一致的，这种情况是不用把数据写到内存里的；&lt;/li&gt;
&lt;li&gt;如果当发生写操作时，数据所对应的 Cache Block 里存放的是「别的内存地址的数据」的话，就要检查这个 Cache Block 里的数据有没有被标记为脏的，如果是脏的话，我们就要把这个 Cache Block 里的数据写回到内存，然后再把当前要写入的数据，写入到这个 Cache Block 里，同时也把它标记为脏的；如果 Cache Block 里面的数据没有被标记为脏，则就直接将数据写入到这个 Cache Block 里，然后再把这个 Cache Block 标记为脏的就好了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以发现写回这个方法，在把数据写入到 Cache 的时候，只有在缓存不命中，同时数据对应的 Cache 中的 Cache Block 为脏标记的情况下，才会将数据写到内存中，而在缓存命中的情况下，则在写入后 Cache 后，只需把该数据对应的 Cache Block 标记为脏即可，而不用写到内存里。&lt;/p&gt;
&lt;p&gt;这样的好处是，如果我们大量的操作都能够命中缓存，那么大部分时间里 CPU 都不需要读写内存，自然性能相比写直达会高很多。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;缓存一致性问题&#34;&gt;缓存一致性问题&lt;/h3&gt;
&lt;p&gt;现在 CPU 都是多核的，由于 L1/L2 Cache 是多个核心各自独有的，那么会带来多核心的&lt;strong&gt;缓存一致性（&lt;em&gt;Cache Coherence&lt;/em&gt;）&lt;/strong&gt; 的问题，如果不能保证缓存一致性的问题，就可能造成结果错误。&lt;/p&gt;
&lt;p&gt;那缓存一致性的问题具体是怎么发生的呢？我们以一个含有两个核心的 CPU 作为例子看一看。&lt;/p&gt;
&lt;p&gt;假设 A 号核心和 B 号核心同时运行两个线程，都操作共同的变量 i（初始值为 0 ）。&lt;br&gt;
&lt;img src=&#34;https://gitee.com/fxiang930/muyuge/raw/master/20220312193701.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;这时如果 A 号核心执行了 &lt;code&gt;i++&lt;/code&gt; 语句的时候，为了考虑性能，使用了我们前面所说的写回策略，先把值为 &lt;code&gt;1&lt;/code&gt; 的执行结果写入到 L1/L2 Cache 中，然后把 L1/L2 Cache 中对应的 Block 标记为脏的，这个时候数据其实没有被同步到内存中的，因为写回策略，只有在 A 号核心中的这个 Cache Block 要被替换的时候，数据才会写入到内存里。&lt;/p&gt;
&lt;p&gt;如果这时旁边的 B 号核心尝试从内存读取 i 变量的值，则读到的将会是错误的值，因为刚才 A 号核心更新 i 值还没写入到内存中，内存中的值还依然是 0。&lt;strong&gt;这个就是所谓的缓存一致性问题，A 号核心和 B 号核心的缓存，在这个时候是不一致，从而会导致执行结果的错误。&lt;/strong&gt;&lt;br&gt;
&lt;img src=&#34;https://gitee.com/fxiang930/muyuge/raw/master/20220312193705.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;那么，要解决这一问题，就需要一种机制，来同步两个不同核心里面的缓存数据。要实现的这个机制的话，要保证做到下面这 2 点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一点，某个 CPU 核心里的 Cache 数据更新时，必须要传播到其他核心的 Cache，这个称为&lt;strong&gt;写传播（&lt;em&gt;Wreite Propagation&lt;/em&gt;）&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;第二点，某个 CPU 核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为&lt;strong&gt;事务的串形化（&lt;em&gt;Transaction Serialization&lt;/em&gt;）&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第一点写传播很容易就理解，当某个核心在 Cache 更新了数据，就需要同步到其他核心的 Cache 里。而对于第二点事务事的串形化，我们举个例子来理解它。&lt;/p&gt;
&lt;p&gt;假设我们有一个含有 4 个核心的 CPU，这 4 个核心都操作共同的变量 i（初始值为 0 ）。A 号核心先把 i 值变为 100，而此时同一时间，B 号核心先把 i 值变为 200，这里两个修改，都会「传播」到 C 和 D 号核心。&lt;br&gt;
&lt;img src=&#34;https://gitee.com/fxiang930/muyuge/raw/master/20220312193659.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;那么问题就来了，C 号核心先收到了 A 号核心更新数据的事件，再收到 B 号核心更新数据的事件，因此 C 号核心看到的变量 i 是先变成 100，后变成 200。&lt;/p&gt;
&lt;p&gt;而如果 D 号核心收到的事件是反过来的，则 D 号核心看到的是变量 i 先变成 200，再变成 100，虽然是做到了写传播，但是各个 Cache 里面的数据还是不一致的。&lt;/p&gt;
&lt;p&gt;所以，我们要保证 C 号核心和 D 号核心都能看到&lt;strong&gt;相同顺序的数据变化&lt;/strong&gt;，比如变量 i 都是先变成 100，再变成 200，这样的过程就是事务的串形化。&lt;/p&gt;
&lt;p&gt;要实现事务串形化，要做到 2 点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU 核心对于 Cache 中数据的操作，需要同步给其他 CPU 核心；&lt;/li&gt;
&lt;li&gt;要引入「锁」的概念，如果两个 CPU 核心里有相同数据的 Cache，那么对于这个 Cache 数据的更新，只有拿到了「锁」，才能进行对应的数据更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那接下来我们看看，写传播和事务串形化具体是用什么技术实现的。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;总线嗅探&#34;&gt;总线嗅探&lt;/h3&gt;
&lt;p&gt;写传播的原则就是当某个 CPU 核心更新了 Cache 中的数据，要把该事件广播通知到其他核心。最常见实现的方式是&lt;strong&gt;总线嗅探（&lt;em&gt;Bus Snooping&lt;/em&gt;）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;我还是以前面的 i 变量例子来说明总线嗅探的工作机制，当 A 号 CPU 核心修改了 L1 Cache 中 i 变量的值，通过总线把这个事件广播通知给其他所有的核心，然后每个 CPU 核心都会监听总线上的广播事件，并检查是否有相同的数据在自己的 L1 Cache 里面，如果 B 号 CPU 核心的 L1 Cache 中有该数据，那么也需要把该数据更新到自己的 L1 Cache。&lt;/p&gt;
&lt;p&gt;可以发现，总线嗅探方法很简单， CPU 需要每时每刻监听总线上的一切活动，但是不管别的核心的 Cache 是否缓存相同的数据，都需要发出一个广播事件，这无疑会加重总线的负载。&lt;/p&gt;
&lt;p&gt;另外，总线嗅探只是保证了某个 CPU 核心的 Cache 更新数据这个事件能被其他 CPU 核心知道，但是并不能保证事务串形化。&lt;/p&gt;
&lt;p&gt;于是，有一个协议基于总线嗅探机制实现了事务串形化，也用状态机机制降低了总线带宽压力，这个协议就是 MESI 协议，这个协议就做到了 CPU 缓存一致性。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;mesi-协议&#34;&gt;MESI 协议&lt;/h3&gt;
&lt;p&gt;MESI 协议其实是 4 个状态单词的开头字母缩写，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Modified&lt;/em&gt;，已修改&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Exclusive&lt;/em&gt;，独占&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Shared&lt;/em&gt;，共享&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Invalidated&lt;/em&gt;，已失效&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这四个状态来标记 Cache Line 四个不同的状态。&lt;/p&gt;
&lt;p&gt;「已修改」状态就是我们前面提到的脏标记，代表该 Cache Block 上的数据已经被更新过，但是还没有写到内存里。而「已失效」状态，表示的是这个 Cache Block 里的数据已经失效了，不可以读取该状态的数据。&lt;/p&gt;
&lt;p&gt;「独占」和「共享」状态都代表 Cache Block 里的数据是干净的，也就是说，这个时候 Cache Block 里的数据和内存里面的数据是一致性的。&lt;/p&gt;
&lt;p&gt;「独占」和「共享」的差别在于，独占状态的时候，数据只存储在一个 CPU 核心的 Cache 里，而其他 CPU 核心的 Cache 没有该数据。这个时候，如果要向独占的 Cache 写数据，就可以直接自由地写入，而不需要通知其他 CPU 核心，因为只有你这有这个数据，就不存在缓存一致性的问题了，于是就可以随便操作该数据。&lt;/p&gt;
&lt;p&gt;另外，在「独占」状态下的数据，如果有其他核心从内存读取了相同的数据到各自的 Cache ，那么这个时候，独占状态下的数据就会变成共享状态。&lt;/p&gt;
&lt;p&gt;那么，「共享」状态代表着相同的数据在多个 CPU 核心的 Cache 里都有，所以当我们要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后再更新当前 Cache 里面的数据。&lt;/p&gt;
&lt;p&gt;我们举个具体的例子来看看这四个状态的转换：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;当 A 号 CPU 核心从内存读取变量 i 的值，数据被缓存在 A 号 CPU 核心自己的 Cache 里面，此时其他 CPU 核心的 Cache 没有缓存该数据，于是标记 Cache Line 状态为「独占」，此时其 Cache 中的数据与内存是一致的；&lt;/li&gt;
&lt;li&gt;然后 B 号 CPU 核心也从内存读取了变量 i 的值，此时会发送消息给其他 CPU 核心，由于 A 号 CPU 核心已经缓存了该数据，所以会把数据返回给 B 号 CPU 核心。在这个时候， A 和 B 核心缓存了相同的数据，Cache Line 的状态就会变成「共享」，并且其 Cache 中的数据与内存也是一致的；&lt;/li&gt;
&lt;li&gt;当 A 号 CPU 核心要修改 Cache 中 i 变量的值，发现数据对应的 Cache Line 的状态是共享状态，则要向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后 A 号 CPU 核心才更新 Cache 里面的数据，同时标记 Cache Line 为「已修改」状态，此时 Cache 中的数据就与内存不一致了。&lt;/li&gt;
&lt;li&gt;如果 A 号 CPU 核心「继续」修改 Cache 中 i 变量的值，由于此时的 Cache Line 是「已修改」状态，因此不需要给其他 CPU 核心发送消息，直接更新数据即可。&lt;/li&gt;
&lt;li&gt;如果 A 号 CPU 核心的 Cache 里的 i 变量对应的 Cache Line 要被「替换」，发现 Cache Line 状态是「已修改」状态，就会在替换前先把数据同步到内存。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以，可以发现当 Cache Line 状态是「已修改」或者「独占」状态时，修改更新其数据不需要发送广播给其他 CPU 核心，这在一定程度上减少了总线带宽压力。&lt;/p&gt;
&lt;p&gt;事实上，整个 MESI 的状态可以用一个有限状态机来表示它的状态流转。还有一点，对于不同状态触发的事件操作，可能是来自本地 CPU 核心发出的广播事件，也可以是来自其他 CPU 核心通过总线发出的广播事件。下图即是 MESI 协议的状态图：&lt;br&gt;
&lt;img src=&#34;https://gitee.com/fxiang930/muyuge/raw/master/20220312193707.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;MESI 协议的四种状态之间的流转过程，我汇总成了下面的表格，你可以更详细的看到每个状态转换的原因：&lt;br&gt;
&lt;img src=&#34;https://gitee.com/fxiang930/muyuge/raw/master/20220312193702.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;
&lt;p&gt;CPU 在读写数据的时候，都是在 CPU Cache 读写数据的，原因是 Cache 离 CPU 很近，读写性能相比内存高出很多。对于 Cache 里没有缓存 CPU 所需要读取的数据的这种情况，CPU 则会从内存读取数据，并将数据缓存到 Cache 里面，最后 CPU 再从 Cache 读取数据。&lt;/p&gt;
&lt;p&gt;而对于数据的写入，CPU 都会先写入到 Cache 里面，然后再在找个合适的时机写入到内存，那就有「写直达」和「写回」这两种策略来保证 Cache 与内存的数据一致性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;写直达，只要有数据写入，都会直接把数据写入到内存里面，这种方式简单直观，但是性能就会受限于内存的访问速度；&lt;/li&gt;
&lt;li&gt;写回，对于已经缓存在 Cache 的数据的写入，只需要更新其数据就可以，不用写入到内存，只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到内存里，这种方式在缓存命中率高的情况，性能会更好；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当今 CPU 都是多核的，每个核心都有各自独立的 L1/L2 Cache，只有 L3 Cache 是多个核心之间共享的。所以，我们要确保多核缓存是一致性的，否则会出现错误的结果。&lt;/p&gt;
&lt;p&gt;要想实现缓存一致性，关键是要满足 2 点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一点是写传播，也就是当某个 CPU 核心发生写入操作时，需要把该事件广播通知给其他核心；&lt;/li&gt;
&lt;li&gt;第二点是事物的串行化，这个很重要，只有保证了这个，次啊能保障我们的数据是真正一致的，我们的程序在各个不同的核心上运行的结果也是一致的；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基于总线嗅探机制的 MESI 协议，就满足上面了这两点，因此它是保障缓存一致性的协议。&lt;/p&gt;
&lt;p&gt;MESI 协议，是已修改、独占、共享、已实现这四个状态的英文缩写的组合。整个 MSI 状态的变更，则是根据来自本地 CPU 核心的请求，或者来自其他 CPU 核心通过总线传输过来的请求，从而构成一个流动的状态机。另外，对于在「已修改」或者「独占」状态的 Cache Line，修改更新其数据不需要发送广播给其他 CPU 核心。&lt;/p&gt;
">CPU 缓存一致性</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/go-gc/"" data-c="
          &lt;h3 id=&#34;一-go-gc-要点&#34;&gt;一. Go GC 要点&lt;/h3&gt;
&lt;p&gt;先来回顾一下GC的几个重要的阶段:&lt;/p&gt;
&lt;h4 id=&#34;mark-prepare-stw&#34;&gt;Mark Prepare - STW&lt;/h4&gt;
&lt;p&gt;做标记阶段的准备工作，需要停止所有正在运行的goroutine(即STW)，标记根对象，启用内存屏障，内存屏障有点像内存读写钩子，它用于在后续并发标记的过程中，维护三色标记的完备性(三色不变性)，这个过程通常很快，大概在10-30微秒。&lt;/p&gt;
&lt;h4 id=&#34;marking-concurrent&#34;&gt;Marking - Concurrent&lt;/h4&gt;
&lt;p&gt;标记阶段会将大概25%(gcBackgroundUtilization)的P用于标记对象，逐个扫描所有G的堆栈，执行三色标记，在这个过程中，所有新分配的对象都是黑色，被扫描的G会被暂停，扫描完成后恢复，这部分工作叫后台标记(&lt;a href=&#34;https://github.com/golang/go/blob/dev.boringcrypto.go1.13/src/runtime/mgc.go#L1817&#34;&gt;gcBgMarkWorker&lt;/a&gt;)。这会降低系统大概25%的吞吐量，比如&lt;code&gt;MAXPROCS=6&lt;/code&gt;，那么GC P期望使用率为&lt;code&gt;6*0.25=1.5&lt;/code&gt;，这150%P会通过专职(Dedicated)/兼职(Fractional)/懒散(Idle)三种工作模式的Worker共同来完成。&lt;/p&gt;
&lt;p&gt;这还没完，为了保证在Marking过程中，其它G分配堆内存太快，导致Mark跟不上Allocate的速度，还需要其它G配合做一部分标记的工作，这部分工作叫辅助标记(mutator assists)。在Marking期间，每次G分配内存都会更新它的”负债指数”(gcAssistBytes)，分配得越快，gcAssistBytes越大，这个指数乘以全局的”负载汇率”(assistWorkPerByte)，就得到这个G需要帮忙Marking的内存大小(这个计算过程叫&lt;a href=&#34;https://github.com/golang/go/blob/dev.boringcrypto.go1.13/src/runtime/mgc.go#L484&#34;&gt;revise&lt;/a&gt;)，也就是它在本次分配的mutator assists工作量(&lt;a href=&#34;https://github.com/golang/go/blob/dev.boringcrypto.go1.13/src/runtime/mgcmark.go#L363&#34;&gt;gcAssistAlloc&lt;/a&gt;)。&lt;/p&gt;
&lt;h4 id=&#34;mark-termination-stw&#34;&gt;Mark Termination - STW&lt;/h4&gt;
&lt;p&gt;标记阶段的最后工作是Mark Termination，关闭内存屏障，停止后台标记以及辅助标记，做一些清理工作，整个过程也需要STW，大概需要60-90微秒。在此之后，所有的P都能继续为应用程序G服务了。&lt;/p&gt;
&lt;h4 id=&#34;sweeping-concurrent&#34;&gt;Sweeping - Concurrent&lt;/h4&gt;
&lt;p&gt;在标记工作完成之后，剩下的就是清理过程了，清理过程的本质是将没有被使用的内存块整理回收给上一个内存管理层级(mcache -&amp;gt; mcentral -&amp;gt; mheap -&amp;gt; OS)，清理回收的开销被平摊到应用程序的每次内存分配操作中，直到所有内存都Sweeping完成。当然每个层级不会全部将待清理内存都归还给上一级，避免下次分配再申请的开销，比如Go1.12对mheap归还OS内存做了&lt;a href=&#34;https://ms2008.github.io/2019/06/30/golang-madvfree/&#34;&gt;优化&lt;/a&gt;，使用&lt;a href=&#34;https://go-review.googlesource.com/c/go/+/135395/&#34;&gt;NADV_FREE&lt;/a&gt;延迟归还内存。&lt;/p&gt;
&lt;h4 id=&#34;stw&#34;&gt;STW&lt;/h4&gt;
&lt;p&gt;在&lt;a href=&#34;https://wudaijun.com/2018/01/go-scheduler/&#34;&gt;Go调度模型&lt;/a&gt;中我们已经提到，Go没有真正的实时抢占机制，而是一套协作式抢占(cooperative preemption)，即给G(groutine)打个标记，等待G在调用函数时检查这个标记，以此作为一个安全的抢占点(GC safe-point)。但如果其它P上的G都停了，某个G还在执行如下代码:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func add(numbers []int) int {
     var v int
     for _, n := range numbers {
         v += n
     }
     return v
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;add函数的运行时间取决于切片的长度，并且在函数内部是没有调用其它函数的，也就是没有抢占点。就会导致整个运行时都在等待这个G调用函数(以实现抢占，开始处理GC)，其它P也被挂起。这就是Go GC最大的诟病: GC STW时间会受到G调用函数的时机的影响并被延长，甚至如果某个G在执行无法抢占的死循环(即循环内部没有发生函数调用的死循环)，那么整个Go的runtime都会挂起，CPU 100%，节点无法响应任何消息，连正常停服都做不到。pprof这类调试工具也用不了，只能通过gdb，delve等外部调试工具来找到死循环的goroutine正在执行的堆栈。如此后果比没有被defer的panic更严重，因为那个时候的节点内部状态是无法预期的。&lt;/p&gt;
&lt;p&gt;因此有Gopher开始倡议Go使用非协作式抢占(non-cooperative preemption)，通过堆栈和寄存器来保存抢占上下文，避免对抢占不友好的函数导致GC STW延长(毕竟第三方库代码的质量也是参差不齐的)。相关的Issue在&lt;a href=&#34;https://github.com/golang/go/issues/24543&#34;&gt;这里&lt;/a&gt;。好消息是，&lt;strong&gt;&lt;a href=&#34;https://tip.golang.org/doc/go1.14&#34;&gt;Go1.14&lt;/a&gt;(目前还是Beta1版本，还未正式发布)已经支持异步抢占&lt;/strong&gt;，也就是说:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main() {
  go func() {
    for {
    }
  }()

  time.Sleep(time.Millisecond)
  runtime.GC()
  println(&amp;quot;OK&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这段代码在Go1.14中终于能输出&lt;code&gt;OK&lt;/code&gt;了。这个提了近五年的Issue: &lt;a href=&#34;https://github.com/golang/go/issues/10958&#34;&gt;runtime: tight loops should be preemptible #10958&lt;/a&gt;前几天终于关闭了。不得不说，这是Go Runtime的一大进步，它不止避免了单个goroutine死循环导致整个runtime卡死的问题，更重要的是，它为STW提供了最坏预期，避免了GC STW造成了性能抖动隐患。&lt;/p&gt;
&lt;h3 id=&#34;二-go-gc-度量&#34;&gt;二. Go GC 度量&lt;/h3&gt;
&lt;h4 id=&#34;1-go-tool-prof&#34;&gt;1. go tool prof&lt;/h4&gt;
&lt;p&gt;Go 基础性能分析工具，pprof的用法和启动方式参考&lt;a href=&#34;https://wudaijun.com/2018/04/go-pprof/&#34;&gt;go pprof性能分析&lt;/a&gt;，其中的heap即为内存分配分析，go tool默认是查看正在使用的内存(&lt;code&gt;inuse_heap&lt;/code&gt;)，如果要看其它数据，使用&lt;code&gt;go tool pprof --alloc_space|inuse_objects|alloc_objects&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;需要注意的是，go pprof本质是数据采样分析，其中的值并不是精确值，适用于性能热点优化，而非真实数据统计。&lt;/p&gt;
&lt;h4 id=&#34;2-go-tool-trace&#34;&gt;2. go tool trace&lt;/h4&gt;
&lt;p&gt;go tool trace可以将GC统计信息以可视化的方式展现出来。要使用go tool trace，可以通过以下方式生成采样数据:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;API: &lt;code&gt;trace.Start&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;go test: &lt;code&gt;go test -trace=trace.out pkg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;net/http/pprof: &lt;code&gt;curl http://127.0.0.1:6060/debug/pprof/trace?seconds=20&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;得到采样数据后，之后即可以通过 &lt;code&gt;go tool trace trace.out&lt;/code&gt; 启动一个HTTP Server，在浏览器中查看可视化trace数据:&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://wudaijun.com/assets/image/202001/trace-index.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;里面提供了各种trace和prof的可视化入口，点击第一个View trace可以看到追踪总览:&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://wudaijun.com/assets/image/202001/trace-view.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;包含的信息量比较广，横轴为时间线，各行为各种维度的度量，通过A/D左右移动，W/S放大放小。以下是各行的意义:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Goroutines: 包含GCWaiting，Runnable，Running三种状态的Goroutine数量统计&lt;/li&gt;
&lt;li&gt;Heap: 包含当前堆使用量(Allocated)和下次GC阈值(NextGC)统计&lt;/li&gt;
&lt;li&gt;Threads: 包含正在运行和正在执行系统调用的Threads数量&lt;/li&gt;
&lt;li&gt;GC: 哪个时间段在执行GC&lt;/li&gt;
&lt;li&gt;ProcN: 各个P上面的goroutine调度情况&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除了&lt;strong&gt;View trace&lt;/strong&gt;之外，trace目录的第二个&lt;strong&gt;Goroutine analysis&lt;/strong&gt;也比较有用，它能够直观统计Goroutine的数量和执行状态:&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;3&#34;&gt;&lt;img src=&#34;https://wudaijun.com/assets/image/202001/trace-goroutines.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;4&#34;&gt;&lt;img src=&#34;https://wudaijun.com/assets/image/202001/trace-goroutines2.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;通过它可以对各个goroutine进行健康诊断，各种network,syscall的采样数据下载下来之后可以直接通过&lt;code&gt;go tool pprof&lt;/code&gt;分析，因此，实际上pprof和trace两套工具是相辅相成的。&lt;/p&gt;
&lt;h4 id=&#34;&#34;&gt;&lt;a href=&#34;#3-GC-Trace&#34; title=&#34;3. GC Trace&#34;&gt;&lt;/a&gt;3. GC Trace&lt;/h4&gt;
&lt;p&gt;GC Trace是Golang提供的非侵入式查看GC信息的方案，用法很简单，设置&lt;code&gt;GCDEBUG=gctrace=1&lt;/code&gt;环境变量即可:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;GODEBUG=gctrace=1 bin/game
gc 1 @0.039s 3%: 0.027+4.5+0.015 ms clock, 0.11+2.3/4.0/5.5+0.063 ms cpu, 4-&amp;gt;4-&amp;gt;2 MB, 5 MB goal, 4 P
gc 2 @0.147s 1%: 0.007+1.2+0.008 ms clock, 0.029+0.15/1.1/2.0+0.035 ms cpu, 5-&amp;gt;5-&amp;gt;3 MB, 6 MB goal, 4 P
gc 3 @0.295s 0%: 0.010+2.3+0.013 ms clock, 0.040+0.14/2.1/4.3+0.053 ms cpu, 7-&amp;gt;7-&amp;gt;4 MB, 8 MB goal, 4 P
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面是各项指标的解释:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;gc 1 @0.039s 3%: 0.027+4.5+0.015 ms clock, 0.11+2.3/4.0/5.5+0.063 ms cpu, 4-&amp;gt;4-&amp;gt;2 MB, 5 MB goal, 4 P

// 通用参数
gc 2: 程序运行后的第2次GC
@0.147s: 到目前为止程序运行的时间
3%: 到目前为止程序花在GC上的CPU%

// Wall-Clock 流逝的系统时钟
0.027ms+4.5ms+0.015 ms   : 分别是 STW Mark Prepare，Concurrent Marking，STW Mark Termination 的时钟时间

// CPU Time 消耗的CPU时间
0.11+2.3/4.0/5.5+0.063 ms : 以+分隔的阶段同上，不过将Concurrent Marking细分为Mutator Assists Time, Background GC Time(包括Dedicated和Fractional Worker), Idle GC Time三种。其中0.11=0.027*4，0.063=0.015*4。

// 内存相关统计
4-&amp;gt;4-&amp;gt;2 MB: 分别是开始标记时，标记结束后的堆占用大小，以及标记结束后真正存活的(有效的)堆内存大小
5 MB goal: 下次GC Mark Termination后的目标堆占用大小，该值受GC Percentage影响，并且会影响mutator assist工作量(每次堆大小变更时都动态评估，如果快超出goal了，就需要其它goroutine帮忙干活了, https://github.com/golang/go/blob/dev.boringcrypto.go1.13/src/runtime/mgc.go#L484)

// Processors
4 P : P的数量，也就是GOMAXPROCS大小，可通过runtime.GoMaxProcs设置

// 其它
GC forced: 如果两分钟内没有执行GC，则会强制执行一次GC，此时会换行打印 GC forced
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-memstats&#34;&gt;4. MemStats&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/golang/go/blob/dev.boringcrypto.go1.13/src/runtime/mstats.go#L147&#34;&gt;runtime.MemStats&lt;/a&gt;记录了内存分配的一些统计信息，通过&lt;code&gt;runtime.ReadMemStats(&amp;amp;ms)&lt;/code&gt;获取，它是&lt;a href=&#34;https://github.com/golang/go/blob/dev.boringcrypto.go1.13/src/runtime/mstats.go#L24&#34;&gt;runtime.mstats&lt;/a&gt;的对外版(再次可见Go单一访问控制的弊端)，MemStats字段比较多，其中比较重要的有:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type MemStats struct {
    TotalAlloc uint64
    Sys uint64
    Mallocs uint64
    Frees   uint64
    HeapAlloc uint64
    HeapSys uint64
    HeapInuse uint64
    HeapIdle uint64
    HeapReleased uint64
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;程序可以通过定期调用&lt;code&gt;runtime.ReadMemStats&lt;/code&gt;API来获取内存分配信息发往时序数据库进行监控。另外，该API是会STW的，但是很短，Google内部也在用，用他们的话说:”STW不可怕，长时间STW才可怕”，该API通常一分钟调用一次即可。&lt;/p&gt;
&lt;h4 id=&#34;-2&#34;&gt;&lt;a href=&#34;#5-ReadGCStats&#34; title=&#34;5. ReadGCStats&#34;&gt;&lt;/a&gt;5. ReadGCStats&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;debug.ReadGCStats&lt;/code&gt;用于获取最近的GC统计信息，主要是GC造成的延迟信息:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type GCStats struct {
	LastGC         time.Time       
	NumGC          int64           
	PauseTotal     time.Duration   
	Pause          []time.Duration 
	...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;和ReadMemStats一样，ReadGCStats也可以定时收集，发送给时序数据库做监控统计。&lt;/p&gt;
&lt;h3 id=&#34;-3&#34;&gt;&lt;a href=&#34;#%E4%B8%89-Go-GC-%E8%B0%83%E4%BC%98&#34; title=&#34;三. Go GC 调优&#34;&gt;&lt;/a&gt;三. Go GC 调优&lt;/h3&gt;
&lt;p&gt;Go GC相关的参数少得可怜，一如既往地精简:&lt;/p&gt;
&lt;h4 id=&#34;-4&#34;&gt;&lt;a href=&#34;#1-debug-SetGCPercent&#34; title=&#34;1. debug.SetGCPercent&#34;&gt;&lt;/a&gt;1. debug.SetGCPercent&lt;/h4&gt;
&lt;p&gt;一个百分比数值，决定即本次GC后，下次触发GC的阈值，比如本次GC Sweeping完成后的内存占用为200M，GC Percentage为100(默认值)，那么下次触发GC的内存阈值就是400M。这个值通常不建议修改，因为优化GC开销的方法通常是避免不必要的分配或者内存复用，而非通过调整GC Percent延迟GC触发时机(Go GC本身也会根据当前分配速率来决定是否需要提前开启新一轮GC)。另外，debug.SetGCPercent传入&amp;lt;0的值将关闭GC。&lt;/p&gt;
&lt;h4 id=&#34;2-runtimegc&#34;&gt;2. runtime.GC&lt;/h4&gt;
&lt;p&gt;强制执行一次GC，如果当前正在执行GC，则帮助当前GC执行完成后，再执行一轮完整的GC。该函数阻塞直到GC完成。&lt;/p&gt;
&lt;h4 id=&#34;3-debugfreeosmemory&#34;&gt;3. debug.FreeOSMemory&lt;/h4&gt;
&lt;p&gt;强制执行一次GC，并且尽可能多地将不再使用的内存归还给OS。&lt;/p&gt;
&lt;p&gt;严格意义上说，以上几个API预期说调优，不如说是补救，它们都只是把Go GC本身就会做的事情提前或者延后了，通常是治标不治本的方法。真正的GC调优主要还是在应用层面。我在&lt;a href=&#34;https://wudaijun.com/2019/09/go-performance-optimization/&#34;&gt;这篇文章&lt;/a&gt;聊了一些Go应用层面的内存优化。&lt;/p&gt;
&lt;p&gt;以上主要从偏应用的角度介绍了Golang GC的几个重要阶段，STW，GC度量/调试，以及相关API等。这些理论和方法能在在必要的时候派上用场，帮助更深入地了解应用程序并定位问题。&lt;/p&gt;
">Go GC</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/redis-yu-mysq-yi-zhi-xing/"" data-c="
          &lt;h2 id=&#34;谈谈一致性&#34;&gt;谈谈一致性&lt;/h2&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/EHw3d4NjKetZmak.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;一致性就是数据保持一致，在分布式系统中，可以理解为多个节点中数据的值是一致的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;强一致性&lt;/strong&gt;：这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;弱一致性&lt;/strong&gt;：这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最终一致性&lt;/strong&gt;：最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三个经典的缓存模式&#34;&gt;三个经典的缓存模式&lt;/h2&gt;
&lt;p&gt;缓存可以提升性能、缓解数据库压力，但是使用缓存也会导致数据&lt;strong&gt;不一致性&lt;/strong&gt;的问题。一般我们是如何使用缓存呢？有三种经典的缓存模式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cache-Aside Pattern&lt;/li&gt;
&lt;li&gt;Read-Through/Write through&lt;/li&gt;
&lt;li&gt;Write behind&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cache-aside-pattern&#34;&gt;Cache-Aside Pattern&lt;/h3&gt;
&lt;p&gt;Cache-Aside Pattern，即&lt;strong&gt;旁路缓存模式&lt;/strong&gt;，它的提出是为了尽可能地解决缓存与数据库的数据不一致问题。&lt;/p&gt;
&lt;h4 id=&#34;cache-aside读流程&#34;&gt;Cache-Aside读流程&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Cache-Aside Pattern&lt;/strong&gt;的读请求流程如下：&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/DS2OaGPC3igUJwZ.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;读的时候，先读缓存，缓存命中的话，直接返回数据&lt;/li&gt;
&lt;li&gt;缓存没有命中的话，就去读数据库，从数据库取出数据，放入缓存后，同时返回响应。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;cache-aside-写流程&#34;&gt;Cache-Aside 写流程&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Cache-Aside Pattern&lt;/strong&gt;的写请求流程如下：&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;3&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/bGYKM9FyIlTzZcj.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;更新的时候，先&lt;strong&gt;更新数据库，然后再删除缓存&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;read-throughwrite-through读写穿透&#34;&gt;Read-Through/Write-Through（读写穿透）&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Read/Write Through&lt;/strong&gt;模式中，服务端把缓存作为主要数据存储。应用程序跟数据库缓存交互，都是通过&lt;strong&gt;抽象缓存层&lt;/strong&gt;完成的。&lt;/p&gt;
&lt;h4 id=&#34;read-through&#34;&gt;Read-Through&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Read-Through&lt;/strong&gt;的简要流程如下&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;4&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/RON436PtminUo7b.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;从缓存读取数据，读到直接返回&lt;/li&gt;
&lt;li&gt;如果读取不到的话，从数据库加载，写入缓存后，再返回响应。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这个简要流程是不是跟&lt;strong&gt;Cache-Aside&lt;/strong&gt;很像呢？其实&lt;strong&gt;Read-Through&lt;/strong&gt;就是多了一层&lt;strong&gt;Cache-Provider&lt;/strong&gt;，流程如下：&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;5&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/bnWG4mK3R752Ozf.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;Read-Through实际只是在&lt;strong&gt;Cache-Aside&lt;/strong&gt;之上进行了一层封装，它会让程序代码变得更简洁，同时也减少数据源上的负载。&lt;/p&gt;
&lt;h4 id=&#34;write-through&#34;&gt;Write-Through&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Write-Through&lt;/strong&gt;模式下，当发生写请求时，也是由&lt;strong&gt;缓存抽象层&lt;/strong&gt;完成数据源和缓存数据的更新,流程如下： &lt;img src=&#34;https://s2.loli.net/2023/08/23/qAl34Sjud5bJ6rC.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;write-behind-异步缓存写入&#34;&gt;Write behind （异步缓存写入）&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Write behind&lt;/strong&gt;跟&lt;strong&gt;Read-Through/Write-Through&lt;/strong&gt;有相似的地方，都是由&lt;code&gt;Cache Provider&lt;/code&gt;来负责缓存和数据库的读写。它两又有个很大的不同：&lt;strong&gt;Read/Write Through&lt;/strong&gt;是同步更新缓存和数据的，&lt;strong&gt;Write Behind&lt;/strong&gt;则是只更新缓存，不直接更新数据库，通过&lt;strong&gt;批量异步&lt;/strong&gt;的方式来更新数据库。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;6&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/Oj7H1oKeyRYi9IP.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;这种方式下，缓存和数据库的一致性不强，&lt;strong&gt;对一致性要求高的系统要谨慎使用&lt;/strong&gt;。但是它适合频繁写的场景，MySQL的&lt;strong&gt;InnoDB Buffer Pool机制&lt;/strong&gt;就使用到这种模式。&lt;/p&gt;
&lt;h2 id=&#34;操作缓存的时候删除缓存呢还是更新缓存&#34;&gt;操作缓存的时候，删除缓存呢，还是更新缓存？&lt;/h2&gt;
&lt;p&gt;一般业务场景，我们使用的就是&lt;strong&gt;Cache-Aside&lt;/strong&gt;模式。 有些小伙伴可能会问， &lt;strong&gt;Cache-Aside&lt;/strong&gt;在写入请求的时候，为什么是&lt;strong&gt;删除缓存而不是更新缓存&lt;/strong&gt;呢？&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;7&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/uxQ2MhL9ejyfr6v.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;我们在操作缓存的时候，到底应该删除缓存还是更新缓存呢？我们先来看个例子：&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;8&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/HUSmu7v6hjqicfF.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;线程A先发起一个写操作，第一步先更新数据库&lt;/li&gt;
&lt;li&gt;线程B再发起一个写操作，第二步更新了数据库&lt;/li&gt;
&lt;li&gt;由于网络等原因，线程B先更新了缓存&lt;/li&gt;
&lt;li&gt;线程A更新缓存。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这时候，缓存保存的是A的数据（老数据），数据库保存的是B的数据（新数据），数据&lt;strong&gt;不一致&lt;/strong&gt;了，脏数据出现啦。如果是&lt;strong&gt;删除缓存取代更新缓存&lt;/strong&gt;则不会出现这个脏数据问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;更新缓存相对于删除缓存&lt;/strong&gt;，还有两点劣势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果你写入的缓存值，是经过复杂计算才得到的话。更新缓存频率高的话，就浪费性能啦。&lt;/li&gt;
&lt;li&gt;在写数据库场景多，读数据场景少的情况下，数据很多时候还没被读取到，又被更新了，这也浪费了性能呢(实际上，写多的场景，用缓存也不是很划算了)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;双写的情况下先操作数据库还是先操作缓存&#34;&gt;双写的情况下，先操作数据库还是先操作缓存？&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Cache-Aside&lt;/code&gt;缓存模式中，有些小伙伴还是有疑问，在写入请求的时候，为什么是&lt;strong&gt;先操作数据库呢&lt;/strong&gt;？为什么&lt;strong&gt;不先操作缓存&lt;/strong&gt;呢？&lt;/p&gt;
&lt;p&gt;假设有A、B两个请求，请求A做更新操作，请求B做查询读取操作。 &lt;img src=&#34;https://s2.loli.net/2023/08/23/wLv1lSV2xrPHFhd.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;线程A发起一个写操作，第一步del cache&lt;/li&gt;
&lt;li&gt;此时线程B发起一个读操作，cache miss&lt;/li&gt;
&lt;li&gt;线程B继续读DB，读出来一个老数据&lt;/li&gt;
&lt;li&gt;然后线程B把老数据设置入cache&lt;/li&gt;
&lt;li&gt;线程A写入DB最新的数据&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;酱紫就有问题啦，&lt;strong&gt;缓存和数据库的数据不一致了。缓存保存的是老数据，数据库保存的是新数据&lt;/strong&gt;。因此，&lt;code&gt;Cache-Aside&lt;/code&gt;缓存模式，选择了先操作数据库而不是先操作缓存。&lt;/p&gt;
&lt;h3 id=&#34;缓存延时双删&#34;&gt;缓存延时双删&lt;/h3&gt;
&lt;p&gt;有些小伙伴可能会说，不一定要先操作数据库呀，采用&lt;strong&gt;缓存延时双删&lt;/strong&gt;策略就好啦？什么是延时双删呢？&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;9&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/uCo1pcWDEqIM96S.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;先删除缓存&lt;/li&gt;
&lt;li&gt;再更新数据库&lt;/li&gt;
&lt;li&gt;休眠一会（比如1秒），再次删除缓存。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这个休眠一会，一般多久呢？都是1秒？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这个休眠时间 = 读业务逻辑数据的耗时 + 几百毫秒。 为了确保读请求结束，写请求可以删除读请求可能带来的缓存脏数据。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;删除缓存重试机制&#34;&gt;删除缓存重试机制&lt;/h3&gt;
&lt;p&gt;不管是&lt;strong&gt;延时双删&lt;/strong&gt;还是&lt;strong&gt;Cache-Aside的先操作数据库再删除缓存&lt;/strong&gt;，如果第二步的删除缓存失败呢，删除失败会导致脏数据哦~&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;删除失败就多删除几次呀,保证删除缓存成功呀~ 所以可以引入&lt;strong&gt;删除缓存重试机制&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;10&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/Pl5rNcu9Uh2KYQy.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;写请求更新数据库&lt;/li&gt;
&lt;li&gt;缓存因为某些原因，删除失败&lt;/li&gt;
&lt;li&gt;把删除失败的key放到消息队列&lt;/li&gt;
&lt;li&gt;消费消息队列的消息，获取要删除的key&lt;/li&gt;
&lt;li&gt;重试删除缓存操作&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;读取biglog异步删除缓存&#34;&gt;读取biglog异步删除缓存&lt;/h3&gt;
&lt;p&gt;重试删除缓存机制还可以，就是会造成好多业务代码入侵。其实，还可以通过&lt;strong&gt;数据库的binlog来异步淘汰key&lt;/strong&gt;。&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;11&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/08/23/RjJMSb1NzFCo8VG.webp&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;以mysql为例 可以使用阿里的canal将binlog日志采集发送到MQ队列里面，然后通过ACK机制确认处理这条更新消息，删除缓存，保证数据缓存一致性&lt;/p&gt;
">Redis与MySQ一致性</a>
      </div>
      
      <div class="item">
        <a class="result-title" style="opacity: 0;" href="https://muyuge.github.io/post/mysql-zhong-jie-1/"" data-c="
          &lt;h1 id=&#34;结构一览&#34;&gt;结构一览&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;参考资料 &lt;a href=&#34;http://c.biancheng.net/view/7939.html&#34;&gt;MySQL体系结构详解&lt;/a&gt;&lt;br&gt;
参考资料 &lt;a href=&#34;https://cloud.tencent.com/developer/article/1575547&#34;&gt;MySQL学习笔记（一）MySQL体系结构&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;如何使用 MySQL 固然重要，但是就像学习 Java那样，不了解 JVM 很多东西都寸步难行，所以这里把 MySQL 的基本结构做个复盘&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://images.alsritter.icu/images/2021/02/10/20210210152130.png&#34;&gt;&lt;img src=&#34;http://images.alsritter.icu/images/2021/02/10/20210210152130.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;从上图可以发现，MySQL由以下几部分组成：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;连接池组件&lt;/strong&gt;：身份认证和权限相关（登录 MySQL 的时候）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;管理服务和工具组件&lt;/strong&gt;：系统管理和控制工具，例如备份恢复、MySQL复制、集群等 ；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SQL接口组件（就是一个接口）&lt;/strong&gt;：接受用户的SQL命令，并且返回用户需要查询的结果；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;查询缓存（Cache）&lt;/strong&gt;: 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分析器&lt;/strong&gt;: 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查 SQL 语句语法是否正确。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;优化器组件&lt;/strong&gt;：按照 MySQL 认为最优的方案去执行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;插件式存储引擎&lt;/strong&gt;：存储引擎层，也是与其他数据库的主要区别&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;物理文件&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;自上而下可以分为网络连接层，服务层（核心层），存储引擎层，物理文件层&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://images.alsritter.icu/images/2021/07/17/20210717221422.png&#34;&gt;&lt;img src=&#34;https://images.alsritter.icu/images/2021/07/17/20210717221422.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;什么是-sql&#34;&gt;什么是 SQL？&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;结构化查询语言（Structured Query Language）简称 SQL，是一种数据库查询语言。&lt;/p&gt;
&lt;p&gt;作用：用于存取数据、查询、更新和管理关系数据库系统。&lt;/p&gt;
&lt;h1 id=&#34;sql-语句是如何执行的&#34;&gt;SQL 语句是如何执行的&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;参考资料 &lt;a href=&#34;https://github.com/Snailclimb/JavaGuide/blob/master/docs/database/%E4%B8%80%E6%9D%A1sql%E8%AF%AD%E5%8F%A5%E5%9C%A8mysql%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84.md&#34;&gt;一条sql语句在mysql中如何执行的&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://images.alsritter.icu/images/2021/02/11/20210211193142.png&#34;&gt;&lt;img src=&#34;http://images.alsritter.icu/images/2021/02/11/20210211193142.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;简单来说 MySQL 主要分为 Server 层和存储引擎层：&lt;/p&gt;
&lt;p&gt;Server 层：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binlog 日志模块。&lt;/p&gt;
&lt;p&gt;存储引擎： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持 InnoDB、MyISAM、Memory 等多个存储引擎，其中 InnoDB 引擎有自有的日志模块 redolog 模块。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始就被当做默认存储引擎了。&lt;/p&gt;
&lt;h1 id=&#34;网络连接层&#34;&gt;网络连接层&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;负责 MySQL 和其他应用程序的连接交互，包括连接管理、授权认证、安全等。由客户端程序连接和 MySQL服务器上的连接池组成。&lt;/p&gt;
&lt;p&gt;每个客户端连接都对应着服务器上的一个线程，MySQL服务器上维护了一个线程池，避免为每个连接都创建销毁一个线程。当客户端连接到 MySQL服务器时，服务器对其进行认证。登录认证后，还要验证客户端是否有执行某个查询的操作权限。这一层并不是 MySQL所特有的技术。&lt;/p&gt;
&lt;p&gt;使用连接池的必要性：&lt;/p&gt;
&lt;p&gt;每个连接对应一个线程，实际业务中通常有许多个连接访问数据库服务器，如果每次连接都要创建一个新的线程，连接释放则销毁线程，对于系统损耗是非常大的。通过连接池维护和缓存一定的连接，由应用程序动态地对池中的连接进行申请、使用和释放，减少了内存损耗，提升了资源利用率。&lt;/p&gt;
&lt;h1 id=&#34;服务层核心层&#34;&gt;服务层（核心层）&lt;/h1&gt;
&lt;hr&gt;
&lt;h3 id=&#34;连接器&#34;&gt;连接器&lt;/h3&gt;
&lt;p&gt;连接器主要和身份认证和权限相关的功能相关，就好比一个级别很高的门卫一样。&lt;/p&gt;
&lt;p&gt;主要负责用户登录数据库，进行用户的身份认证，包括校验账户密码，权限等操作，如果用户账户密码已通过，连接器会到权限表中查询该用户的所有权限，之后在这个连接里的权限逻辑判断都是会依赖此时读取到的权限数据，也就是说，后续只要这个连接不断开，即时管理员修改了该用户的权限，该用户也是不受影响的。&lt;/p&gt;
&lt;h3 id=&#34;查询缓存&#34;&gt;查询缓存&lt;/h3&gt;
&lt;p&gt;查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。&lt;/p&gt;
&lt;p&gt;查询缓存主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集。&lt;/p&gt;
&lt;p&gt;连接建立后，执行查询语句的时候，会先查询缓存，MySQL 会先校验这个 sql 是否执行过，以 Key-Value 的形式缓存在内存中，Key 是查询预计，Value 是结果集。如果缓存 key 被命中，就会直接返回给客户端，如果没有命中，就会执行后续的操作，完成后也会把结果缓存起来，方便下一次调用。当然在真正执行缓存查询的时候还是会校验用户的权限，是否有该表的查询条件。&lt;/p&gt;
&lt;p&gt;MySQL 查询不建议使用缓存，因为查询缓存失效在实际业务场景中可能会非常频繁，假如你对一个表更新的话，这个表上的所有的查询缓存都会被清空。对于不经常更新的数据来说，使用缓存还是可以的。&lt;/p&gt;
&lt;p&gt;所以，一般在大多数情况下我们都是不推荐去使用查询缓存的。&lt;/p&gt;
&lt;p&gt;MySQL 8.0 版本后删除了缓存的功能，官方也是认为该功能在实际的应用场景比较少，所以干脆直接删掉了。&lt;/p&gt;
&lt;h3 id=&#34;分析器&#34;&gt;分析器&lt;/h3&gt;
&lt;p&gt;MySQL 没有命中缓存，那么就会进入分析器，分析器主要是用来分析 SQL 语句是来干嘛的，分析器也会分为几步：&lt;/p&gt;
&lt;p&gt;第一步，词法分析，一条 SQL 语句有多个字符串组成，首先要提取关键字，比如 select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步。&lt;/p&gt;
&lt;p&gt;第二步，语法分析，主要就是判断你输入的 sql 是否正确，是否符合 MySQL 的语法。&lt;/p&gt;
&lt;p&gt;完成这 2 步之后，MySQL 就准备开始执行了，但是如何执行，怎么执行是最好的结果呢？这个时候就需要优化器上场了。&lt;/p&gt;
&lt;h3 id=&#34;优化器&#34;&gt;优化器&lt;/h3&gt;
&lt;p&gt;优化器的作用就是它认为的最优的执行方案去执行，比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。&lt;/p&gt;
&lt;p&gt;可以说，经过了优化器之后可以说这个语句具体该如何执行就已经定下来。&lt;/p&gt;
&lt;h3 id=&#34;执行器&#34;&gt;执行器&lt;/h3&gt;
&lt;p&gt;当选择了执行方案后，MySQL 就准备开始执行了，首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果。&lt;/p&gt;
&lt;h1 id=&#34;语句分析&#34;&gt;语句分析&lt;/h1&gt;
&lt;hr&gt;
&lt;h3 id=&#34;查询语句&#34;&gt;查询语句&lt;/h3&gt;
&lt;p&gt;说了以上这么多，那么究竟一条 sql 语句是如何执行的呢？其实我们的 sql 可以分为两种，一种是查询，一种是更新（增加，更新，删除）。我们先分析下查询语句，语句如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select * from tb_student  A where A.age=&#39;18&#39; and A.name=&#39; 张三&#39;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;分析下这个语句的执行流程：&lt;/p&gt;
&lt;p&gt;1、先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在 MySQL8.0 版本以前，会先查询缓存，以这条 sql 语句为 key 在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。&lt;/p&gt;
&lt;p&gt;2、通过分析器进行词法分析，提取 sql 语句的关键元素，比如提取上面这个语句是查询 select，提取需要查询的表名为 tb_student，需要查询所有的列，查询条件是这个表的 &lt;code&gt;id=&#39;1&#39;&lt;/code&gt;。然后判断这个 sql 语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。&lt;/p&gt;
&lt;p&gt;3、接下来就是优化器进行确定执行方案，上面的 sql 语句，可以有两种执行方案：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;A：先查询学生表中姓名为 “张三” 的学生，然后判断是否年龄是 18。

B：先找出学生中年龄 18 岁的学生，然后再查询姓名为 “张三” 的学生。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了。&lt;/p&gt;
&lt;p&gt;4、进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;

如果命中缓存

权限校验

查询缓存

分析器

优化器

再一次权限校验

执行器

引擎


&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;更新语句&#34;&gt;更新语句&lt;/h3&gt;
&lt;p&gt;更新语句如何执行的呢？sql 语句如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;update tb_student A set A.age=&#39;19&#39; where A.name=&#39;张三&#39;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;来给张三修改下年龄，其实条语句也基本上会沿着上一个查询的流程走，只不过执行更新的时候要记录日志，这就会引入日志模块了，MySQL 自带的日志模块式 binlog（归档日志） ，所有的存储引擎都可以使用，常用的 InnoDB 引擎还自带了一个日志模块 redo log（重做日志），就以 InnoDB 模式下来探讨这个语句的执行流程。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;流程如下：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;1、先查询到张三这一条数据，如果有缓存，也是会用到缓存。&lt;/p&gt;
&lt;p&gt;2、然后拿到查询的语句，把 age 改为 19，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。&lt;/p&gt;
&lt;p&gt;3、执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。&lt;/p&gt;
&lt;p&gt;4、更新完成。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;

分析器

权限校验

执行器

引擎

redoLog

redoLog的prepare状态

binlog

redoLog的commit状态


&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;为什么要用两个日志模块，用一个日志模块不行吗?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这是因为最开始 MySQL 并没与 InnoDB 引擎（InnoDB 引擎是其他公司以插件形式插入 MySQL 的） ，MySQL 自带的引擎是 MyISAM，但是 redo log 是 InnoDB 引擎特有的，其他存储引擎都没有，这就导致会没有 crash-safe 的能力（crash-safe 的能力即使数据库发生异常重启，之前提交的记录都不会丢失），binlog 日志只能用来归档。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;那么，为什么 redo log 要引入 prepare 预提交状态？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;因为如果先写 redo log 直接提交，然后写 binlog，假设写完 redo log 后，机器挂了，binlog 日志没有被写入，那么机器重启后，这台机器会通过 redo log 恢复数据，但是这个时候 bingog 并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。&lt;/p&gt;
&lt;p&gt;如果先写 binlog，然后写 redo log，假设写完了 binlog，机器异常重启了，由于没有 redo log，本机是无法恢复这一条记录的，但是 binlog 又有记录，那么和上面同样的道理，就会产生数据不一致的情况。&lt;/p&gt;
&lt;p&gt;而采用 redo log 两阶段提交的方式就不一样了，写完 binlog 后，然后再提交 redo log 就会防止出现上述的问题，从而保证了数据的一致性。那么问题来了，有没有一个极端的情况呢？假设 redo log 处于预提交状态，binlog 也已经写完了，这个时候发生了异常重启会怎么样呢？ 这个就要依赖于 MySQL 的处理机制了，MySQL 的处理过程如下：&lt;/p&gt;
&lt;p&gt;判断 redo log 是否完整，如果判断是完整的，就立即提交。&lt;br&gt;
如果 redo log 只是预提交但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 redo log, 不完整就回滚事务。&lt;/p&gt;
&lt;p&gt;这样就解决了数据一致性的问题。&lt;/p&gt;
&lt;h1 id=&#34;存储引擎层&#34;&gt;存储引擎层&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;参考资料 &lt;a href=&#34;https://segmentfault.com/a/1190000012588602&#34;&gt;MySQL - 常见的三种存储引擎&lt;/a&gt;&lt;br&gt;
参考资料 &lt;a href=&#34;https://www.cnblogs.com/54chensongxia/p/11439956.html&#34;&gt;MySQL 存储引擎介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;MySQL 数据库区别于其他数据库的最重要的一个特点就是其插件式的表存储引擎，从上图中也可以看到，MySql支持很多种存储引擎。需要特别注意的是，&lt;strong&gt;存储引擎是基于表的&lt;/strong&gt;，而不是数据库。&lt;/p&gt;
&lt;p&gt;所以每个表都可以设置为不同的引擎，以满足业务需求&lt;/p&gt;
&lt;p&gt;插件式存储引擎的好处是：能够根据具体的应用的特点选择不同的存储引擎。下面是几种 MySQL 常用的存储引擎。&lt;/p&gt;
&lt;p&gt;查看当支持的全部存储引擎&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SHOW ENGINES
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;http://images.alsritter.icu/images/2021/02/10/20210210153847.png&#34;&gt;&lt;img src=&#34;http://images.alsritter.icu/images/2021/02/10/20210210153847.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在 MySQL 中，不需要在整个服务器中使用同一种存储引擎，针对具体的要求，可以对每一个表使用不同的存储引擎。Support列的值表示某种引擎是否能使用：YES表示可以使用、NO表示不能使用、DEFAULT表示该引擎为当前默认的存储引擎。&lt;/p&gt;
&lt;p&gt;如果要想查看数据库默认使用哪个引擎，可以通过使用命令：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;
SHOW VARIABLES LIKE &#39;default_storage_engine&#39;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;http://images.alsritter.icu/images/2021/02/10/20210210154101.png&#34;&gt;&lt;img src=&#34;http://images.alsritter.icu/images/2021/02/10/20210210154101.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;innodb存储引擎&#34;&gt;InnoDB存储引擎&lt;/h3&gt;
&lt;p&gt;InnoDB是事务型数据库的首选引擎，支持事务安全表（ACID），支持行锁定和外键，上图也看到了，InnoDB是默认的MySQL引擎。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;InnoDB 主要特性&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;InnoDB 为 MySQL 提供了具有提交、回滚和崩溃恢复能力的 &lt;strong&gt;事务安全（ACID兼容）存储引擎&lt;/strong&gt;。InnoDB 锁定在行级并且也在 SELECT 语句中提供一个类似 Oracle 的非锁定读。这些功能增加了多用户部署和性能。在 SQL 查询中，可以自由地将 InnoDB 类型的表和其他 MySQL 的表类型混合起来，甚至在同一个查询中也可以混合&lt;/p&gt;
&lt;p&gt;InnoDB 存储引擎为在 &lt;strong&gt;主内存中缓存数据和索引而维持它自己的缓冲池&lt;/strong&gt;。InnoDB 将它的表和索引在一个逻辑表空间中，表空间可以包含数个文件（或原始磁盘文件）。这与 MyISAM 表不同，比如在 MyISAM 表中每个表被存放在分离的文件中。InnoDB 表可以是任何尺寸，即使在文件尺寸被限制为2GB的操作系统上&lt;/p&gt;
&lt;p&gt;InnoDB 支持 &lt;strong&gt;外键完整性约束&lt;/strong&gt;，存储表中的数据时，每张表的存储都按主键顺序存放，如果没有显示在表定义时指定主键，InnoDB 会为每一行生成一个6字节的 ROWID（行 Id），并以此作为主键&lt;/p&gt;
&lt;p&gt;使用 InnoDB 存储引擎 MySQL 将在数据目录下创建一个名为 ibdata1 的 10MB 大小的 &lt;strong&gt;自动扩展数据文件&lt;/strong&gt;，以及两个名为 ib_logfile0 和 ib_logfile1 的 5MB 大小的日志文件&lt;/p&gt;
&lt;h3 id=&#34;myisam存储引擎&#34;&gt;MyISAM存储引擎&lt;/h3&gt;
&lt;p&gt;MyISAM 基于 ISAM 存储引擎，并对其进行扩展。它是在 Web、数据仓储和其他应用环境下最常使用的存储引擎之一。&lt;strong&gt;MyISAM 拥有较高的插入、查询速度，但不支持事务&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;MyISAM 存储引擎不支持事务、表锁设计，支持全文索引，主要面向一些 OLAP 数据库应用。此外，MyISAM 存储引擎的另一个与众不同的地方是它的缓冲池 &lt;strong&gt;只缓存（cache）索引文件，而不缓冲数据文件&lt;/strong&gt;，这点和大多数的数据库都非常不同。从MySQL 5.0版本开始，MyISAM 默认支持256TB 的单表数据，这足够满足一般应用需求。&lt;/p&gt;
&lt;p&gt;使用 MyISAM 引擎创建数据库，将产生3个文件。文件的名字以表名字开始，扩展名之处文件类型：&lt;code&gt;frm&lt;/code&gt; 文件存储表定义、数据文件的扩展名为 &lt;code&gt;.MYD&lt;/code&gt;（MYData）、索引文件的扩展名时 &lt;code&gt;.MYI&lt;/code&gt;（MYIndex）&lt;/p&gt;
&lt;h3 id=&#34;memory存储引擎&#34;&gt;MEMORY存储引擎&lt;/h3&gt;
&lt;p&gt;Memory存储引擎（之前称HEAP存储引擎）&lt;strong&gt;将表中的数据存放在内存中&lt;/strong&gt;，如果数据库重启或发生崩溃，表中的数据都将消失。它非常适合用于存储临时数据的临时表，以及数据仓库中的纬度表。Memory存储引擎默认使用哈希索引，而不是我们熟悉的B+树索引。&lt;/p&gt;
&lt;p&gt;虽然 Memory存储引擎速度非常快，但在使用上还是有一定的限制。比如，只支持表锁，并发性能较差，并且不支持 TEXT 和 BLOB列类型。最重要的是，存储变长字段（varchar）时是按照定常字段（char）的方式进行的，因此会浪费内存。&lt;/p&gt;
&lt;p&gt;此外有一点容易被忽视，MySQL 数据库使用 Memory 存储引擎作为临时表来存放查询的中间结果集（intermediate result）。如果中间结果集大于 Memory 存储引擎表的容量设置，又或者中间结果含有 TEXT或 BLOB列类型字段，则 MySQL数据库会把其转换到 MyISAM存储引擎表而存放到磁盘中。之前提到 MyISAM不缓存数据文件，因此这时产生的临时表的性能对于查询会有损失。&lt;/p&gt;
&lt;h3 id=&#34;存储引擎的选择&#34;&gt;存储引擎的选择&lt;/h3&gt;
&lt;p&gt;种存储引擎都有自己的优缺点，不能笼统地说谁比谁好。但通常建议选择使用 InnoDB&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://images.alsritter.icu/images/2021/02/10/20210210160612.png&#34;&gt;&lt;img src=&#34;http://images.alsritter.icu/images/2021/02/10/20210210160612.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;InnoDB：&lt;/strong&gt;  支持事务处理，支持外键，支持崩溃修复能力和并发控制。如果需要对事务的完整性要求比较高（比如银行），要求实现并发控制（比如售票），那选择InnoDB有很大的优势。如果需要频繁的更新、删除操作的数据库，也可以选择 InnoDB，因为支持事务的提交（commit）和回滚（rollback）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MyISAM：&lt;/strong&gt;  插入数据快，空间和内存使用比较低。如果表主要是用于插入新记录和读出记录，那么选择 MyISAM 能实现处理高效率。如果应用的完整性、并发性要求比较低，也可以使用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MEMORY：&lt;/strong&gt;  所有的数据都在内存中，数据的处理速度快，但是安全性不高。如果需要很快的读写速度，对数据的安全性要求较低，可以选择 MEMORY。它对表的大小有要求，不能建立太大的表。所以，这类数据库只使用在相对较小的数据库表。&lt;/p&gt;
&lt;p&gt;值得一提的是，MySQL 数据库使用 Memory 存储引擎作为临时表来存放查询的中间结果集（intermediate result）。 如果中间结果集大于 Memory 存储引擎表的容量设置，又或者中间结果含有 TEXT 或 BLOB 列类型字段，则 MySQL 数据库会把其转换到 MyISAM 存储引擎表而存放到磁盘。但是因为 MyISAM 不缓存数据文件，因此这时产生的临时表的性能对于查询会有损失。&lt;/p&gt;
&lt;h1 id=&#34;物理文件层&#34;&gt;物理文件层&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;参考资料 &lt;a href=&#34;http://c.biancheng.net/view/7911.html&#34;&gt;MySQL物理文件（数据目录）体系结构详解&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在 MySQL 中，物理文件存放在数据目录中。数据目录与安装目录不同，安装目录用来存储控制服务器和客户端程序的命令，数据目录用来存储 MySQL 服务器在运行过程中产生的数据。本节主要介绍 MySQL 数据目录的物理结构和作用。&lt;/p&gt;
&lt;p&gt;该层主要将数据库的数据（表，索引等）存储在文件系统上，并完成与存储引擎的交互。存储数据包括日志文件，数据文件，配置文件等。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注意：MySQL 中任何一项逻辑性或者物理性文件都具有可配置性，另外每个版本都有一些改进，所以在学习本节内容时要灵活掌握，不能生搬硬套。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;可以通过下面这个命令查看 mysql 的文件路径&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SHOW VARIABLES LIKE &#39;datadir&#39;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;http://images.alsritter.icu/images/2021/02/10/20210210163650.png&#34;&gt;&lt;img src=&#34;http://images.alsritter.icu/images/2021/02/10/20210210163650.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;数据目录&#34;&gt;数据目录&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://images.alsritter.icu/images/2021/02/10/20210210164031.png&#34;&gt;&lt;img src=&#34;http://images.alsritter.icu/images/2021/02/10/20210210164031.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data 目录用来存放数据库相关的数据信息，包括数据库信息，表信息等。&lt;br&gt;
&lt;code&gt;my.ini&lt;/code&gt; 文件是 MySQL 服务端和客户端主要的配置文件，包括编码集、默认引擎、最大连接数等设置。MySQL 服务器启动时会默认加载此文件。&lt;/p&gt;
&lt;h3 id=&#34;data目录&#34;&gt;Data目录&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://images.alsritter.icu/images/2021/02/10/20210210164343.png&#34;&gt;&lt;img src=&#34;http://images.alsritter.icu/images/2021/02/10/20210210164343.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;由图中可以看出，系统数据库和用户自定义数据库的存放路径相同。数据库目录中主要存放相应的数据库对象&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对 Data 目录中的文件说明如下：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;mysql、performance_schema、sys 和 world 是系统数据库，information_schema 数据库比较特殊，这里没有相应的数据库目录。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;auto.cnf&lt;/code&gt;：MySQL 服务器的选项文件，用于存储 server-uuid 的值。server-uuid 与 server-id 一样，用于标识 MySQL 实例在集群中的唯一性。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ib_logfile0、ib_logfile1&lt;/code&gt; 是支持事务性引擎的 redo 日志文件&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ibdata1&lt;/code&gt; 为共享表空间（系统表空间）。如果采用 InnoDB 引擎，默认大小为 10M&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;数据目录里可能还有：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;MySQL 服务器的进程 ID（PID）文件。&lt;br&gt;
MySQL 服务器所生成的状态和日志文件。&lt;br&gt;
DES 密钥文件或服务器的 SSL 证书。&lt;/p&gt;
&lt;h3 id=&#34;数据库目录&#34;&gt;数据库目录&lt;/h3&gt;
&lt;p&gt;数据库实际是一个目录，每个目录都保存着相应数据库中的表以及表数据。下面以 test 数据库（自定义的一个数据库）为例讲解目录中存放的文件。&lt;/p&gt;
&lt;p&gt;test 数据库中有如下几张数据表：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;+-------------------+
| Tables_in_test    |
+-------------------+
| tb_student        |
| tb_student_course |
| tb_students_info  |
| tb_usertest       |
+-------------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对 test 数据库目录中的文件说明如下：&lt;/p&gt;
&lt;p&gt;1、&lt;code&gt;db.opt&lt;/code&gt;：用来保存数据库的配置信息，比如该库的默认字符集编码和字符集排序规则。如果你创建数据库时指定了字符集和排序规则，后续创建的表没有指定字符集和排序规则，那么该表将采用 &lt;code&gt;db.opt&lt;/code&gt; 文件中指定的属性。&lt;/p&gt;
&lt;p&gt;对于 InnoDB 表，如果是独立的表空间，数据库中的表结构以及数据都存储在数据库的路径下（而不是在共享表空间 ibdata1 文件中）。但是数据中的其他对象，包括数据被修改之后，事务提交之间的版本信息，仍然存储在共享表空间的 ibdata1 文件中。&lt;/p&gt;
&lt;p&gt;2、&lt;code&gt;.frm&lt;/code&gt;：在 MySQL 中建立任何一张数据表，其对应的数据库目录下都会有该表的 &lt;code&gt;.frm&lt;/code&gt; 文件。&lt;code&gt;.frm&lt;/code&gt; 文件用来保存每个数据表的元数据（meta）和表结构等信息。数据库崩溃时，可以用 &lt;code&gt;.frm&lt;/code&gt; 文件恢复表结构。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.frm&lt;/code&gt; 文件跟存储引擎无关，任何存储引擎的数据表都有 &lt;code&gt;.frm&lt;/code&gt; 文件，命名方式为 &lt;code&gt;表名.frm&lt;/code&gt;，如 &lt;code&gt;users.frm&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;MySQL 8.0 版本开始，frm 文件被取消，MySQL 把文件中的数据都写到了系统表空间。通过利用 InnoDB 存储引擎来实现表 DDL 语句操作的原子性（在之前版本中是无法实现表 DDL 语句操作的原子性的，如 TRUNCATE 无法回滚）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;3、&lt;code&gt;.MYD&lt;/code&gt; 和 &lt;code&gt;.MYI&lt;/code&gt; 文件&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.MYD&lt;/code&gt; 理解为 My Data，用于存放 MyISAM 表的数据。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.MYI&lt;/code&gt; 理解为 My Index，主要存放 MyISAM 表的索引及相关信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;5、&lt;code&gt;.ibd&lt;/code&gt; 和 &lt;code&gt;.ibdata&lt;/code&gt; 文件&lt;/p&gt;
&lt;p&gt;对于 InnoDB 存储引擎的数据表，一个表对应两个文件，一个是 &lt;code&gt;*.frm&lt;/code&gt;，存储表结构信息；一个是 &lt;code&gt;*.ibd&lt;/code&gt;，存储表中数据。&lt;code&gt;.ibd&lt;/code&gt; 和 &lt;code&gt;.ibdata&lt;/code&gt; 都是专属于 InnoDB 存储引擎的数据库文件。当采用共享表空间时，所有 InnoDB 表的数据均存放在 &lt;code&gt;.ibdata&lt;/code&gt; 中。所以当表越来越多时，这个文件会变得很大。相对应的 &lt;code&gt;.ibd&lt;/code&gt; 就是采用独享表空间时 InnoDB 表的数据文件。&lt;/p&gt;
&lt;p&gt;当然，就算开启了独享表空间，ibdata 文件也会越来越大，因为这个文件里还存储了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;变更缓冲区&lt;/li&gt;
&lt;li&gt;双写缓冲区&lt;/li&gt;
&lt;li&gt;撤销日志&lt;/li&gt;
&lt;/ul&gt;
">MySQL 总结</a>
      </div>
      
    </div>
    <div class="page">
      <div id="page_ul"></div>
    </div>
  </div>
</div>
<script>
  !function () {
    let searchMask = document.querySelector('#search_mask');
    let result = document.querySelector('#result');
    let items = document.querySelectorAll('.item');
    let searchBox = document.querySelector('#search');
    let statCount = document.querySelector('#stat_count');
    let statTimes = document.querySelector('#stat_times');
    let pageUl = document.querySelector('#page_ul');
    let close = document.querySelector('#close');
    
    close.addEventListener('click', function() {
      searchMask.style = 'display: none;'
    })

    let finds = [];
    let contents = [];
    let pageSize = 10;
    items.forEach(item => {
      let a = item.querySelector('a');
      contents.push({
        title: a.innerText,
        details: a.dataset.c,
        link: a.href
      })
      item.remove();
    })

    function insertStr(soure, start, count) {
      let newStr = soure.substr(start, count);
      return soure.slice(0, start) + '<em>' + newStr + '</em>' + soure.slice(start + count);
    }

    pageUl.addEventListener('click', function(event) {
      let target = event.target;
      if (target.__proto__ === HTMLSpanElement.prototype) {
        appendResults(parseInt(target.dataset.i));
      }
    })

    function appendResults(index) {
      let htmlResult = '';
      let start = index || 0;
      let end = Math.min(start + pageSize, finds.length);
      for (let i = start; i < end; i++) {
        const current = finds[i];
        let html = current.title;
        let sum = 0;
        let positions = current.positions;
        positions.forEach(position => {
          html = insertStr(html, position.start + sum, position.count);
          sum += 9;
        })
        htmlResult += `<div class="item"><a class="result-title" href="${current.link}">${html}</a></div>`;
      }
      result.innerHTML = htmlResult;
      pageUl.innerHTML = '';
      let count = finds.length / pageSize;
      let lis = '';
      if (start !== 0) {
        lis += `<span class="fa fa-angle-left" data-i='${start - 1}'></span>`;
      }
      for (let i = 0; i < count; i++) {
        lis += `<span class='${i === start?'current':''}' data-i='${i}'>${i+1}</span>`;     
      }
      if (start+1 < count) {
        lis += `<span class="fa fa-angle-right" data-i='${start+1}'></span>`;  
      }
      pageUl.innerHTML = lis;
    }

    function search(delay) {
      let timer = null
      return function () {
        clearTimeout(timer)
        timer = setTimeout(() => {
          let start = Date.now();
          let segments = searchBox.value.split(' ').filter(c => c != '');
          if (segments.length <= 0) {
            return;
          }
          finds = [];
          let htmlResult = '';
          contents.forEach(content => {
            let title = content.title;
            let positions = [];
            let find = false;
            segments.forEach((segment) => {
              if (content.title.includes(segment)) {
                find = true;
                positions.push({
                  start: content.title.indexOf(segment),
                  count: segment.length
                })
              } else if (content.details.includes(segment)) {
                find = true;
              }
            });
            if (find) {
              finds.push({
                title: content.title,
                link: content.link,
                positions
              });
            }
          })
          appendResults(0);
          statCount.textContent = finds.length;
          statTimes.textContent = Date.now() - start;
        }, delay)
      }
    }
    searchBox.addEventListener('input', search(200));
  }()
</script>

<input hidden id="copy" />
<script>
  !function () {
    let times = document.querySelectorAll('.publish-time');
    for (let i = 0; i < times.length; i++) {
      let date = times[i].dataset.t;
      let time = Math.floor((new Date().getTime() - new Date(date).getTime()) / 1000);
      if (time < 60) {
        str = time + '秒之前';
      } else if (time < 3600) {
        str = Math.floor(time / 60) + '分钟之前';
      } else if (time >= 3600 && time < 86400) {
        str = Math.floor(time / 3600) + '小时之前';
      } else if (time >= 86400 && time < 259200) {
        str = Math.floor(time / 86400) + '天之前';
      } else {
        str = times[i].textContent;
      }
      times[i].textContent = str;
    }
  }();
</script>

<script>
  let language = '';
  if (language !== '') {
    let map = new Map();
    if (language === 'en') {
      map.set('search', 'Search');
      map.set('category', 'Categories');
      map.set('article', 'Articles');
      map.set('tag', 'Tags');
      map.set('top', 'Top');
      map.set('publish', 'published');
      map.set('minute', ' minutes');
      map.set('read-more', 'Read More');
      map.set('view', 'View');
      map.set('words', ' words');
      map.set('category-in', 'category in');
      map.set('preview', 'Meta');
      map.set('index', 'Toc');
      map.set('no-archives', "You haven't created yet");
      map.set('archives', " articles in total");
      map.set('cloud-tags', " tags in total");
      map.set('copyright', "Copyright: ");
      map.set('author', "Author: ");
      map.set('link', "Link: ");
      map.set('leave-message', "Leave a message");
      map.set('format', "Links Format");
      map.set('site-name', "Name: ");
      map.set('site-link', "Link: ");
      map.set('site-desc', "Desc: ");
      map.set('stat', " related results, taking ");
      map.set('stat-time', " ms");
      map.set('site-img', "Image: ");
    }

    if (map.size > 0) {
      let lanElems = document.querySelectorAll('.language');
      lanElems.forEach(elem => {
        let lan = elem.dataset.lan, text = map.get(lan);
        if (elem.__proto__ === HTMLInputElement.prototype) {
          elem.placeholder = text
        } else {
          if (elem.dataset.count) {
            text = elem.dataset.count + text;
          }
          elem.textContent = text;
        }
      })
    }
  }

  window.Clipboard = (function (window, document, navigator) {
    var textArea,
      copy;

    // 判断是不是ios端
    function isOS() {
      return navigator.userAgent.match(/ipad|iphone/i);
    }
    //创建文本元素
    function createTextArea(text) {
      textArea = document.createElement('textArea');
      textArea.value = text;
      textArea.style.width = 0;
      textArea.style.height = 0;
      textArea.clientHeight = 0;
      textArea.clientWidth = 0;
      document.body.appendChild(textArea);
    }
    //选择内容
    function selectText() {
      var range,
        selection;

      if (isOS()) {
        range = document.createRange();
        range.selectNodeContents(textArea);
        selection = window.getSelection();
        selection.removeAllRanges();
        selection.addRange(range);
        textArea.setSelectionRange(0, 999999);
      } else {
        textArea.select();
      }
    }

    //复制到剪贴板
    function copyToClipboard() {
      try {
        document.execCommand("Copy")
      } catch (err) {
        alert("复制错误！请手动复制！")
      }
      document.body.removeChild(textArea);
    }

    copy = function (text) {
      createTextArea(text);
      selectText();
      copyToClipboard();
    };

    return {
      copy: copy
    };
  })(window, document, navigator);

  function copyCode(e) {
    if (e.srcElement.tagName === 'SPAN' && e.srcElement.classList.contains('copy-code')) {
      let code = e.currentTarget.querySelector('code');
      var text = code.innerText;
      if (e.srcElement.textContent === '复制成功') {
        return;
      }
      e.srcElement.textContent = '复制成功';
      (function (elem) {
        setTimeout(() => {
          if (elem.textContent === '复制成功') {
            elem.textContent = '复制代码'
          }
        }, 1000);
      })(e.srcElement)
      Clipboard.copy(text);
    }
  }

  let pres = document.querySelectorAll('pre');
  pres.forEach(pre => {
    let code = pre.querySelector('code');
    let copyElem = document.createElement('span');
    copyElem.classList.add('copy-code');
    copyElem.textContent = '复制代码';
    pre.appendChild(copyElem);
    pre.onclick = copyCode
  })

</script>
<script src="/media/js/motion.js"></script>


  <script
    src="https://cdn.jsdelivr.net/gh/cferdinandi/smooth-scroll/dist/smooth-scroll.polyfills.min.js"></script>
  <script>
    var scroll = new SmoothScroll('a[href*="#"]', {
      speed: 200
    });
  </script>



<script src="/media/js/mouse/love.js"></script>


  <script src="/media/js/cool.js"></script>




</html>